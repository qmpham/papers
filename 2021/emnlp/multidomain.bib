% -*- mode:bibtex -*-
% encoding:utf-8

@PhdThesis{Saunders21domain,
  author = 	 {Danielle Saunders},
  title = 	 {Domain Adaptation for Neural Machine Translation},
  school = 	 {Department of Engineering, University of Cambridge},
  year = 	 2021}

@article{Kumar21learning,
  title={Learning Policies for Multilingual Training of Neural Machine Translation Systems},
  author={Kumar, Gaurav and Koehn, Philipp and Khudanpur, Sanjeev},
  journal={arXiv e-prints},
  pages={arXiv--2103},
  year={2021}
}

@inproceedings{shen21source,
    title = "The Source-Target Domain Mismatch Problem in Machine Translation",
    author = "Shen, Jiajun  and
      Chen, Peng-Jen  and
      Le, Matthew  and
      He, Junxian  and
      Gu, Jiatao  and
      Ott, Myle  and
      Auli, Michael  and
      Ranzato, Marc{'}Aurelio",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2021.eacl-main.130",
    pages = "1519--1533",
    abstract = "While we live in an increasingly interconnected world, different places still exhibit strikingly different cultures and many events we experience in our every day life pertain only to the specific place we live in. As a result, people often talk about different things in different parts of the world. In this work we study the effect of local context in machine translation and postulate that this causes the domains of the source and target language to greatly mismatch. We first formalize the concept of source-target domain mismatch, propose a metric to quantify it, and provide empirical evidence for its existence. We conclude with an empirical study of how source-target domain mismatch affects training of machine translation systems on low resource languages. While this may severely affect back-translation, the degradation can be alleviated by combining back-translation with self-training and by increasing the amount of target side monolingual data.",
}

@inproceedings{Zhou20uncertainty,
    title = "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
    author = "Zhou, Yikai and Yang, Baosong and Wong, Derek F. and Wan, Yu and Chao, Lidia S.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.620",
    doi = "10.18653/v1/2020.acl-main.620",
    pages = "6934--6944",
    abstract = "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
}

@InProceedings{Graves17automated,
  title =        {Automated Curriculum Learning for Neural Networks},
  author =       {Alex Graves and Marc G. Bellemare and Jacob Menick and R{\'e}mi Munos and Koray Kavukcuoglu},
  booktitle =    {Proceedings of the 34th International Conference on Machine Learning},
  pages =        {1311--1320},
  year =         {2017},
  editor =       {Precup, Doina and Teh, Yee Whye},
  volume =       {70},
  series =       {Proceedings of Machine Learning Research},
  month =        {06--11 Aug},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v70/graves17a/graves17a.pdf},
  url =          {http://proceedings.mlr.press/v70/graves17a.html},
  abstract =     {We introduce a meth%on a concatenation of all corpora (\texttt{Mixed}). We develop two versions\footnote{In fact three: to enable a fair comparison with WDCMT, a RNN-based variant is also trained and evaluated. \revision{This system appears as \system{Mixed-Nat-RNN} in Table~\ref{tab:performance}}.} of this system, one where the domain unbalance reflects the distribution of our training data \revision{given in %on a concatenation of all corpora (\texttt{Mixed}). We develop two versions\footnote{In fact three: to enable a fair comparison with WDCMT, a RNN-based variant is also trained and evaluated. \revision{This system appears as \system{Mixed-Nat-RNN} in Table~\ref{tab:performance}}.} of this system, one where the domain unbalance reflects the distribution of our training data \revision{given in %on a concatenation of all corpora (\texttt{Mixed}). We develop two versions\footnote{In fact three: to enable a fair comparison with WDCMT, a RNN-based variant is also trained and evaluated. \revision{This system appears as \system{Mixed-Nat-RNN} in Table~\ref{tab:performance}}.} of this system, one where the domain unbalance reflects the distribution of our training data \revision{given in Table~\ref{tab:Corpora}} (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}). The former is the best option when the train mixture $\mathcal{D}^s$ is also expected in testing; the latter should be used when the test distribution is uniform across domains. Accordingly, we report two aggregate scores: a weighted average reflecting the training distribution, and an unweighted average, meaning that test domains are equally important.Table~\ref{tab:Corpora}} (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}). The former is the best option when the train mixture $\mathcal{D}^s$ is also expected in testing; the latter should be used when the test distribution is uniform across domains. Accordingly, we report two aggregate scores: a weighted average reflecting the training distribution, and an unweighted average, meaning that test domains are equally important.Table~\ref{tab:Corpora}} (\system{Mixed-Nat}) and one where all domains are equally represented in training (\system{Mixed-Bal}). The former is the best option when the train mixture $\mathcal{D}^s$ is also expected in testing; the latter should be used when the test distribution is uniform across domains. Accordingly, we report two aggregate scores: a weighted average reflecting the training distribution, and an unweighted average, meaning that test domains are equally important.od for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.}
}

@inproceedings{Bengio09curriculum,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum Learning},
year = {2009},
isbn = {9781605585161},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{Vanderwees17dynamic,
    title = "Dynamic Data Selection for Neural Machine Translation",
    author = "van der Wees, Marlies and Bisazza, Arianna and Monz, Christof",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1147",
    doi = "10.18653/v1/D17-1147",
    pages = "1400--1410",
    abstract = "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce {`}dynamic data selection{'} for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call {`}gradual fine-tuning{'}, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.",
}


@inproceedings{Zhang19curriculum,
    title = "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
    author = "Zhang, Xuan and Shapiro, Pamela and Kumar, Gaurav  and McNamee, Paul  and Carpuat, Marine  and Duh, Kevin",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1189",
    doi = "10.18653/v1/N19-1189",
    pages = "1903--1915",
    abstract = "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
}


@InProceedings{Pham20astudy,
  author    = {Pham, Minh Quang  and  Crego, Josep Maria  and  Yvon, François  and  Senellart, Jean},
  title     = {A Study of Residual Adapters for Multi-Domain Neural Machine Translation},
  booktitle      = {Proceedings of the Fifth Conference on Machine Translation},
  month          = {November},
  year           = {2020},
  address        = {Online},
  publisher      = {Association for Computational Linguistics},
  pages     = {615--626},
  abstract  = {Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.},
  url       = {https://www.aclweb.org/anthology/2020.wmt-1.72}
}

@article{Williams92simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Machine learning},
  volume={8},
  number={3-4},
  pages={229--256},
  year={1992},
  publisher={Springer}
}

@article{Pham21revisiting,
      author = {Minh Quang Pham and Josep Crego and François Yvon},
      title = {Revisiting Multi-Domain Machine Translation},
      journal = {Transactions of the Association for Computational Linguistics},
      volume = {9},
      number = {0},
      year = {2021},
      keywords = {},
      abstract = {When building machine translation systems, one often needs to make the best out of heterogeneous sets of parallel data in training, and to robustly handle inputs from unexpected domains in testing. This multi-domain scenario has attracted a lot of recent work, that fall under the general umbrella of transfer learning. In this study, we revisit multi-domain machine translation, with the aim to formulate the motivations for developing such systems and the associated expectations with respect to performance. Our experiments with a large sample of multi-domain systems show that most of these expectations are hardly met and suggest that further work is needed to better analyze the current behaviour of multi-domain systems and to make them fully hold their promises.},
     issn = {2307-387X},	pages = {17--35},
     url = {https://transacl.org/index.php/tacl/article/view/2327}
}

@inproceedings{Wang18dynamic,
    title = "Dynamic Sentence Sampling for Efficient Training of Neural Machine Translation",
    author = "Wang, Rui  and
      Utiyama, Masao  and
      Sumita, Eiichiro",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-2048",
    doi = "10.18653/v1/P18-2048",
    pages = "298--304",
    abstract = "Traditional Neural machine translation (NMT) involves a fixed training procedure where each sentence is sampled once during each epoch. In reality, some sentences are well-learned during the initial few epochs; however, using this approach, the well-learned sentences would continue to be trained along with those sentences that were not well learned for 10-30 epochs, which results in a wastage of time. Here, we propose an efficient method to dynamically sample the sentences in order to accelerate the NMT training. In this approach, a weight is assigned to each sentence based on the measured difference between the training costs of two iterations. Further, in each epoch, a certain percentage of sentences are dynamically sampled according to their weights. Empirical results based on the NIST Chinese-to-English and the WMT English-to-German tasks show that the proposed method can significantly accelerate the NMT training and improve the NMT performance.",
}

@InProceedings{Wang20optimizing,
  title = 	 {Optimizing Data Usage via Differentiable Rewards},
  author =       {Wang, Xinyi and Pham, Hieu and Michel, Paul and Anastasopoulos, Antonios and Carbonell, Jaime and Neubig, Graham},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9983--9995},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wang20p/wang20p.pdf},
  url = 	 {
http://proceedings.mlr.press/v119/wang20p.html
},
  abstract = 	 {To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that “adapts” to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.}
}

@inproceedings{Wang20balancing,
    title = "Balancing Training for Multilingual Neural Machine Translation",
    author = "Wang, Xinyi and Tsvetkov, Yulia and Neubig, Graham",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.754",
    doi = "10.18653/v1/2020.acl-main.754",
    pages = "8526--8537",
    abstract = "When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",
}

@inproceedings{Wang20learning-multi,
    title = "Learning a Multi-Domain Curriculum for Neural Machine Translation",
    author = "Wang, Wei and Tian, Ye and Ngiam, Jiquan and Yang, Yinfei and Caswell, Isaac and Parekh, Zarana",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.689",
    doi = "10.18653/v1/2020.acl-main.689",
    pages = "7711--7723",
    abstract = "Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",
}

@inproceedings{Kumar19reinforcement,
    title = "Reinforcement Learning based Curriculum Optimization for Neural Machine Translation",
    author = "Kumar, Gaurav and Foster, George and Cherry, Colin and Krikun, Maxim",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1208",
    doi = "10.18653/v1/N19-1208",
    pages = "2054--2061",
    abstract = "We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.",
}

@inproceedings{Li18onesentence,
    title = "One Sentence One Model for Neural Machine Translation",
    author = "Li, Xiaoqing  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1146",
}

@inproceedings{Sharaf20metalearning,
    title = "Meta-Learning for Few-Shot {NMT} Adaptation",
    author = "Sharaf, Amr and Hassan, Hany  and Daum{\'e} III, Hal",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.ngt-1.5",
    doi = "10.18653/v1/2020.ngt-1.5",
    pages = "43--53",
    abstract = "We present META-MT, a meta-learning approach to adapt Neural Machine Translation (NMT) systems in a few-shot setting. META-MT provides a new approach to make NMT models easily adaptable to many target do- mains with the minimal amount of in-domain data. We frame the adaptation of NMT systems as a meta-learning problem, where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks. We evaluate the proposed meta-learning strategy on ten domains with general large scale NMT systems. We show that META-MT significantly outperforms classical domain adaptation when very few in- domain examples are available. Our experiments shows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU points after seeing only 4, 000 translated words (300 parallel sentences).",
}

@inproceedings{Miceli-barone17regularization,
    title = "Regularization techniques for fine-tuning in neural machine translation",
    author = "Miceli Barone, Antonio Valerio and Haddow, Barry and Germann, Ulrich and Sennrich, Rico",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1156",
    doi = "10.18653/v1/D17-1156",
    pages = "1489--1494",
    abstract = "We investigate techniques for supervised domain adaptation for neural machine translation where an existing model trained on a large out-of-domain dataset is adapted to a small in-domain dataset. In this scenario, overfitting is a major challenge. We investigate a number of techniques to reduce overfitting and improve transfer learning, including regularization techniques such as dropout and L2-regularization towards an out-of-domain prior. In addition, we introduce tuneout, a novel regularization technique inspired by dropout. We apply these techniques, alone and in combination, to neural machine translation, obtaining improvements on IWSLT datasets for Englishâ†’German and Englishâ†’Russian. We also investigate the amounts of in-domain training data needed for domain adaptation in NMT, and find a logarithmic relationship between the amount of training data and gain in BLEU score.",
}


@Book{Quinonero08dataset,
  editor = 	 {Joaquin Quiñonero-Candela, Masashi Sugiyama, Anton Schwaighofer and Neil D. Lawrence},
  title = 	 {Dataset Shift in Machine Learning},
  publisher = 	 {MIT Press},
  year = 	 2008,
  series = 	 {Neural Information Processing series},
  note = 	 "ISBN : 9780262170055"}

@inproceedings{Hasler14dynamic-topic,
    title = "Dynamic Topic Adaptation for Phrase-based {MT}",
    author = "Hasler, Eva and Blunsom, Phil and Koehn, Philipp and Haddow, Barry",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E14-1035",
    doi = "10.3115/v1/E14-1035",
    pages = "328--337",
}

@inproceedings{Eidelman12topic,
    title = "Topic Models for Dynamic Translation Model Adaptation",
    author = "Eidelman, Vladimir and Boyd-Graber, Jordan and Resnik, Philip",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-2023",
    pages = "115--119",
}

@article{Arivazhagan19massively,
  author    = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and
               Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and
               George Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  journal   = {arXiv e-prints},
  volume    = {abs/1907.05019},
  year      = {2019},
  primaryClass = {cs.CL},
  url       = {http://arxiv.org/abs/1907.05019}
}

@inproceedings{Artetxe18unsupervised,
  title={Unsupervised Neural Machine Translation},
  author={Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
  booktitle={Proceedings of the International Conference on Learning Representations},
  year={2018},
  address = {Vancouver, Canada},
  url={https://openreview.net/forum?id=Sy2ogebAW},
}

@inproceedings{Axelrod11domain,
	Author = {Axelrod, Amittai and He, Xiaodong and Gao, Jianfeng},
	Booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
	Isbn = {978-1-937284-11-4},
	Address = {Edinburgh, United Kingdom},
	Numpages = {8},
	Pages = {355--362},
	Series = {EMNLP '11},
	Title = {Domain Adaptation via Pseudo In-domain Data Selection},
	Url = {http://dl.acm.org/citation.cfm?id=2145432.2145474},
	Year = {2011},
}

@inproceedings{Bahdanau15learning,
	Author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	Title = {Neural machine translation by jointly learning to align and translate},
	Year = {2015},
	url = {https://arxiv.org/pdf/1409.0473.pdf},
} 

@InProceedings{Banerjee10combining,
  author = 	 {Banerjee, Pratyush and Du, Jinhua and Li, Baoli and Kumar Naskar, Sudip and Way, Andy and van Genabith, Josef},
  title = 	 {Combining multi-domain statistical machine translation models using automatic classifiers},
  booktitle = {Proceedings of the  9th Conference of the Association for Machine Translation in the Americas},
  year = 	 2010,
  series = 	 {AMTA 2010},
  address = 	 {Denver, CO, USA}
}

@inproceedings{Bapna19simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur and Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing",
    series = "EMNLP-IJCNLP",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}

@article{Ben-David09atheory,
	Author = {Shai Ben-David and John Blitzer and Koby Crammer and Alex Kulesza and Fernando Pereira and Jenn Wortman},
	Issue = {Special Issue on Learning from Multiple Sources},
	Volume = {79},
	pages = {151-175},
	Journal = {Machine Learning},
	Title = {A Theory of Learning from Different Domains},
	Number = {1},
        url = {https://doi.org/10.1007/s10994-009-5152-4},
	Year = {2010}}

@conference{Mansour09multiple,
title = "Multiple source adaptation and the R{\'e}nyi divergence",
abstract = "This paper presents a novel theoretical study of the general problem of multiple source adaptation using the notion of R{\'e}nyi divergence. Our results build on our previous work [12], but significantly broaden the scope of that work in several directions. We extend previous multiple source loss guarantees based on distribution weighted combinations to arbitrary target distributions P, not necessarily mixtures of the source distributions, analyze both known and unknown target distribution cases, and prove a lower bound. We further extend our bounds to deal with the case where the learner receives an approximate distribution for each source instead of the exact one, and show that similar loss guarantees can be achieved depending on the divergence between the approximate and true distributions. We also analyze the case where the labeling functions of the source domains are somewhat different. Finally, we report the results of experiments with both an artificial data set and a sentiment analysis task, showing the performance benefits of the distribution weighted combinations and the quality of our bounds based on the R{\'e}nyi divergence.",
author = "Yishay Mansour and Mehryar Mohri and Afshin Rostamizadeh",
year = "2009",
month = dec,
pages = "367--374",
booktitle = "Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence",
series = "UAI 2009",
}

@inproceedings{Bertoldi09domain,
    title = "Domain Adaptation for Statistical Machine Translation with Monolingual Resources",
    author = "Bertoldi, Nicola and Federico, Marcello",
    booktitle = "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    month = mar,
    year = "2009",
    address = "Athens, Greece",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W09-0432",
    pages = "182--189",
}

@article{Biao2017CARENMT,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA}
}

@phdthesis{Blitzer07domain,
  title={Domain adaptation of natural language processing systems},
  author={Blitzer, John},
  school={School of Computer Science, University of Pennsylvania},
  year={2007}
}

@InProceedings{Bojar14findings,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\v{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@InProceedings{Britz17mixing,
  author = 	"Britz, Denny and Le, Quoc and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  address = 	"Copenhagen, Denmark",
  url = 	"http://aclweb.org/anthology/W17-4712"
}


@article{Caruana97multitask,
 author = {Caruana, Rich},
 title = {Multitask Learning},
 journal = {Machine Learning},
 issue_date = {July 1997},
 volume = {28},
 number = {1},
 month = jul,
 year = {1997},
 issn = {0885-6125},
 pages = {41--75},
 numpages = {35},
 url = {https://doi.org/10.1023/A:1007379606734},
 doi = {10.1023/A:1007379606734},
 acmid = {262872},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {backpropagation, generalization, inductive transfer, k-nearest neighbor, kernel regression, multitask learning, parallel transfer, supervised learning}
}

@inproceedings{Cettolo12wit,
        Address = {Trento, Italy},
        Author = {Mauro Cettolo and Christian Girardi and Marcello Federico},
        Booktitle = {Proceedings of the 16$^{th}$ Conference of the European Association for Machine Translation (EAMT)},
        Date = {28-30},
        Month = {May},
        Pages = {261--268},
        Title = {WIT$^3$: Web Inventory of Transcribed and Translated Talks},
        Year = {2012}}

@inproceedings{Chang10necessity,
	Address = {Cambridge, MA},
	Author = {Chang, Ming-Wei and Connor, Michael and Roth, Dan},
	Booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
	Month = oct,
	Pages = {767--777},
	Title = {The Necessity of Combining Adaptation Methods},
	Url = {https://www.aclweb.org/anthology/D10-1075},
	Year = {2010},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D10-1075}}


@inproceedings{Chen16guided,
  title={Guided Alignment Training for Topic-Aware Neural Machine Translation},
  author={Wenhu Chen and Evgeny Matusov and Shahram Khadivi and Jan-Thorsten Peter},
  address = {Austin, Texas},
  year={2016},
  Booktitle = {Proceedings of the Twelth Biennial Conference of the Association for Machine Translation in the Americas},
  Series = {AMTA 2012}
}

@inproceedings{Chen17costweighting,
	Address = {Vancouver},
	Author = {Chen, Boxing and Cherry, Colin and Foster, George and Larkin, Samuel},
	Booktitle = {Proceedings of the First Workshop on Neural Machine Translation},
	Date-Added = {2019-05-19 16:12:54 +0200},
	Date-Modified = {2019-05-19 16:12:54 +0200},
	Doi = {10.18653/v1/W17-3205},
	Pages = {40--46},
	Publisher = {Association for Computational Linguistics},
	Title = {Cost Weighting for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/W17-3205},
	Year = {2017},
}

@inproceedings{Chen98topic,
  title={Topic adaptation for language modeling using unnormalized exponential models},
  author={Chen, Stanley F and Seymore, Kristie and Rosenfeld, Ronald},
  booktitle={Proceedings of the 1998 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP'98 (Cat. No. 98CH36181)},
  volume={2},
  pages={681--684},
  year={1998},
  organization={IEEE}
}


@inproceedings{Cho14properties,
	Address = {Doha, Qatar},
	Author = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	Booktitle = {Proceedings of the Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation},
	Series = {SSST-8},
	Month = {October},
	Pages = {103--111},
	Title = {On the Properties of Neural Machine Translation: Encoder--Decoder Approaches},
	Url = {http://www.aclweb.org/anthology/W14-4012},
	Year = {2014},
}


@InProceedings{Chu18asurvey,
  author = 	"Chu, Chenhui and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  series = "COLING 2018",
  year = 	"2018",
  pages = 	"1304--1319",
  address = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}

@InProceedings{Chu18multilingual,
  author = 	 {Chenhui Chu and Raj Dabre},
  title = 	 {Multilingual and multi-domain adaptation for neural machine translation},
  booktitle = {Proceedings of the 24st Annual Meeting of the Association for Natural Language Processing},
  series = {NLP 2018},
  year = 	 2018,
  pages = 	 {909-–912},
  url = {https://arxiv.org/pdf/1906.07978v2.pdf},
  address = 	 {Okayama, Japan}
}

@InProceedings{Chu2017comparison,
  author = 	"Chu, Chenhui and Dabre, Raj and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine Translation",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2017",
  series = 	"ACL 2017",
  pages = 	"385--391",
  address = 	"Vancouver, Canada",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@inproceedings{Clark12onesystem,
  title={One system, many domains: Open-domain statistical machine translation via feature augmentation},
  author={Clark, Jonathan H. and Lavie, Alon and Dyer, Chris},
  year={2012},
  Address = {San Diego, CA},
  Booktitle = {Proceedings of the Tenth Biennial Conference of the Association for Machine Translation in the Americas},
  Series = {(AMTA 2012)}
}

@article{Cortes19AdaptationBO,
  title={Adaptation Based on Generalized Discrepancy},
  author={Corinna Cortes and Mehryar Mohri and Andr{\'e}s Mu{\~n}oz Medina},
  journal={J. Mach. Learn. Res.},
  year={2019},
  volume={20},
  pages={1:1-1:30}
}


@article{Daume06domain,
  title={Domain adaptation for statistical classifiers},
  author={Daum\'e {III}, Hal and Marcu, Daniel},
  journal={Journal of Artificial Intelligence Research (JAIR)},
  volume={26},
  pages={101--126},
  year={2006}
}

@inproceedings{Daume07frustratingly,
	Address = {Prague, Czech Republic},
	Author = {Daum\'e III, Hal},
	Booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
	Series = {ACL 2007},
	Pages = {256--263},
	Title = {Frustratingly Easy Domain Adaptation},
	Url = {http://aclweb.org/anthology/P07-1033},
	Year = {2007},
	Bdsk-Url-1 = {http://aclweb.org/anthology/P07-1033}}

@InProceedings{Dredze07frustratingly,
  author =	 {Dredze, Mark and Blitzer, John and Pratim Talukdar, Partha and Ganchev, Kuzman and Graca, Jo\~ao and
                  Pereira, Fernando},
  title =	 {Frustratingly Hard Domain Adaptation for Dependency Parsing},
  booktitle =	 {Proceedings of the CoNLL Shared Task Session of
                  EMNLP-CoNLL 2007},
  month =	 {June},
  year =	 {2007},
  address =	 {Prague, Czech Republic},
  publisher =	 {Association for Computational Linguistics},
  pages =	 {1051--1055},
  url =		 {http://www.aclweb.org/anthology/D/D07/D07-1112}
}

@inproceedings{Dredze08online,
	Address = {Honolulu, Hawaii},
	Author = {Dredze, Mark and Crammer, Koby},
	Booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
	Month = oct,
	Pages = {689--697},
	series = {EMNLP},
	Title = {Online Methods for Multi-Domain Learning and Adaptation},
	Url = {https://www.aclweb.org/anthology/D08-1072},
	Year = {2008},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/D08-1072}}

@article{Dredze09multidomain,
Abstract = {State-of-the-art statistical NLP systems for a variety of tasks learn from labeled training data that is often domain specific. However, there may be multiple domains or sources of interest on which the system must perform. For example, a spam filtering system must give high quality predictions for many users, each of whom receives emails from different sources and may make slightly different decisions about what is or is not spam. Rather than learning separate models for each domain, we explore systems that learn across multiple domains. We develop a new multi-domain online learning framework based on parameter combination from multiple classifiers. Our algorithms draw from multi-task learning and domain adaptation to adapt multiple source domain classifiers to a new target domain, learn across multiple similar domains, and learn across a large number of disparate domains. We evaluate our algorithms on two popular NLP domain adaptation tasks: sentiment classification and spam filtering.},
Author = {Mark Dredze and Alex Kulesza and Koby Crammer},
File = {http://www.springerlink.com/content/a78049767680386l/},
Journal = {Machine Learning},
Number = {1-2},
Pages = {123-149},
Title = {Multi-Domain Learning by Confidence-Weighted Parameter Combination},
Volume = {79},
Year = {2010}
}

@inproceedings{Duan09domain,
  title={Domain adaptation from multiple sources via auxiliary classifiers},
  author={Duan, Lixin and Tsang, Ivor W and Xu, Dong and Chua, Tat-Seng},
  booktitle={Proceedings of the 26th Annual International Conference on Machine Learning},
  pages={289--296},
  year={2009}
}

@article{Duan12domain,
  title={Domain adaptation from multiple sources: A domain-dependent regularization approach},
  author={Duan, Lixin and Xu, Dong and Tsang, Ivor Wai-Hung},
  journal={IEEE Transactions on neural networks and learning systems},
  volume={23},
  number={3},
  pages={504--518},
  year={2012},
  publisher={IEEE}
}

@InProceedings{Duh13selection,
  author = 	"Duh, Kevin and Neubig, Graham and Sudoh, Katsuhito and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  address = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@inproceedings{Farajian17multidomain,
	Address = {Copenhagen, Denmark},
	Author = {Farajian, M. Amin and Turchi, Marco and Negri, Matteo and Federico, Marcello},
	Booktitle = {Proceedings of the Second Conference on Machine Translation},
	Doi = {10.18653/v1/W17-4713},
	Month = sep,
	Pages = {127--137},
	Title = {Multi-Domain Neural Machine Translation through Unsupervised Adaptation},
	Url = {https://www.aclweb.org/anthology/W17-4713},
	Year = {2017},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W17-4713},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/W17-4713}}


@inproceedings{Farajian17neural,
    title = "Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario",
    author = "Farajian, M. Amin  and Turchi, Marco and Negri, Matteo and Bertoldi, Nicola and Federico, Marcello",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2045",
    pages = "280--284",
    abstract = "State-of-the-art neural machine translation (NMT) systems are generally trained on specific domains by carefully selecting the training sets and applying proper domain adaptation techniques. In this paper we consider the real world scenario in which the target domain is not predefined, hence the system should be able to translate text from multiple domains. We compare the performance of a generic NMT system and phrase-based statistical machine translation (PBMT) system by training them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings?",
}

@inproceedings{Finkel09hierarchical,
	Address = {Boulder, Colorado},
	Author = {Finkel, Jenny Rose and Manning, Christopher D.},
	Booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics},
	Month = jun,
	Pages = {602--610},
	Title = {Hierarchical {B}ayesian Domain Adaptation},
	Url = {https://www.aclweb.org/anthology/N09-1068},
	Year = {2009},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/N09-1068}}

@inproceedings{Firat16multiway,
	Author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	Doi = {10.18653/v1/N16-1101},
	Location = {San Diego, California},
	Pages = {866--875},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism},
	Url = {http://www.aclweb.org/anthology/N16-1101},
	Year = {2016},
}

@inproceedings{Foster07mixture,
	Address = {Prague, Czech Republic},
	Author = {Foster, George and Kuhn, Roland},
	Booktitle = {Proceedings of the Second Workshop on Statistical Machine Translation},
	Pages = {128--135},
	Title = {Mixture-Model Adaptation for {SMT}},
	Url = {http://www.aclweb.org/anthology/W/W07/W07-0717},
	Year = {2007},
	Bdsk-Url-1 = {http://www.aclweb.org/anthology/W/W07/W07-0717}}

@article{Freitag16fast,
  title={Fast Domain Adaptation for Neural Machine Translation},
  author={Markus Freitag and Yaser Al-Onaizan},
  journal={CoRR},
  year={2016},
  url= {http://arxiv.org/abs/1612.06897},
  volume={abs/1612.06897}
}

@InProceedings{Ghering17convolutional,
  title = 	 {Convolutional Sequence to Sequence Learning},
  author = 	 {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1243--1252},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  url = 	 {http://proceedings.mlr.press/v70/gehring17a.html},
  abstract = 	 {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT’14 English-German and WMT’14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.}
}

@article{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@article{Gulcehre17onintegrating,
	Acmid = {3103741},
	Address = {London, UK, UK},
	Author = {Gulcehre, Caglar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Bengio, Yoshua},
	Doi = {10.1016/j.csl.2017.01.014},
	Issn = {0885-2308},
	Issue_Date = {September 2017},
	Journal = {Comput. Speech Lang.},
	Month = sep,
	Number = {C},
	Numpages = {12},
	Pages = {137--148},
	Publisher = {Academic Press Ltd.},
	Title = {On Integrating a Language Model into Neural Machine Translation},
	Url = {https://doi.org/10.1016/j.csl.2017.01.014},
	Volume = {45},
	Year = {2017},
	Bdsk-Url-1 = {https://doi.org/10.1016/j.csl.2017.01.014}}

@inproceedings{Ha16towards,
	Address = {Vancouver, Canada},
	Author = {Ha, Thanh-He and Niehues, Jan and Waibel, Alex},
	Booktitle = {Proceedings of the 13th International Workshop on Spoken Language Translation},
	Series = {IWSLT 2016},
	Title = {Toward Multilingual Neural Machine Translation with Universal Encoder and Decoder},
	Year = {2016}}

@InProceedings{Hoang14latent,
  author = 	"Hoang, Cuong
		and Sima'an, Khalil",
  title = 	"Latent Domain Translation Models in Mix-of-Domains Haystack",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2014",
  publisher = 	"Dublin City University and Association for Computational Linguistics",
  pages = 	"1928--1939",
  address = 	"Dublin, Ireland",
  url = 	"http://aclweb.org/anthology/C14-1182"
}

@incollection{Hoffman18algorithms,
title = {Algorithms and Theory for Multiple-Source Adaptation},
author = {Hoffman, Judy and Mohri, Mehryar and Zhang, Ningshan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8246--8256},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation.pdf}
}

@InProceedings{Huck15mixeddomain,
  author = 	 {Matthias Huck and Alexandra Birch and Barry Haddow},
  title = 	 {Mixed domain vs. multi-domain statistical machine translation},
  year = 	 2015,
  booktitle = 	 {Procedings of the Machine Translation Summit},
  series = 	 {MT Summit XV},
  address = 	 {Miami Florida},
  url = 	 {http://www.mt-archive.info/15/MTS-2015-Huck.pdf},
  pages = 	 {240--255}
}

@article{Irvine13measuring,
author = {Irvine, Ann and Morgan, John and Carpuat, Marine and Daumé, Hal and Munteanu, Dragos},
title = {Measuring Machine Translation Errors in New Domains},
journal = {Transactions of the Association for Computational Linguistics},
volume = {1},
number = {},
pages = {429-440},
year = {2013},
doi = {10.1162/tacl\_a\_00239},
URL = {https://doi.org/10.1162/tacl_a_00239},
abstract = { We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation. }
}

@inproceedings{Jiang07instance,
    title = "Instance Weighting for Domain Adaptation in {NLP}",
    author = "Jiang, Jing and Zhai, ChengXiang",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P07-1034",
    pages = "264--271",
}
@article{Jiang19multidomain,
    title={Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing},
    author={Haoming Jiang and Chen Liang and Chong Wang and Tuo Zhao},
    year={2019},
    journal={CoRR},
    volume = {abs/1911.02692},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url= {http://arxiv.org/abs/1911.02692},
}
@article{Johnson17google,
	Author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernand a and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	Issn = {2307-387X},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {339--351},
	Title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	Url = {https://transacl.org/ojs/index.php/tacl/article/view/1081},
	Volume = {5},
	Year = {2017},
	Bdsk-Url-1 = {https://transacl.org/ojs/index.php/tacl/article/view/1081}}
@inproceedings{Joshi12multidomain,
        Abstract = {We present a systematic analysis of existing multi-domain learning approaches with respect to two questions. First, many multi-domain learning algorithms resemble ensemble learning algorithms. (1) Are multi-domain learning improvements the result of ensemble learning effects? Second, these algorithms are traditionally evaluated in a balanced label setting, although in practice many multi-domain settings have domain-specific label biases. When multi-domain learning is applied to these settings, (2) are multi-domain methods improving because they capture domain-specific class biases? An understanding of these two issues presents a clearer idea about where the field has had success in multi-domain learning, and it suggests some important open questions for improving beyond the current state of the art.},
        Author = {Mahesh Joshi and Mark Dredze and William W Cohen and Carolyn P Rose},
        Booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
	url = {https://www.aclweb.org/anthology/D12-1119},
        Pages = {1302-1312},
        Title = {Multi-Domain Learning: When Do Domains Matter?},
        Year = {2012}
}
@inproceedings{Joshi13what,
Abstract = {Multi-Domain learning assumes that a single metadata attribute is used in order to divide the data into so-called domains. However, real-world datasets often have multiple metadata attributes that can divide the data into domains. It is not always apparent which single attribute will lead to the best domains, and more than one attribute might impact classification. We propose extensions to two multi-domain learning techniques for our multi-attribute setting, enabling them to simultaneously learn from several metadata attributes. Experimentally, they outperform the multi-domain learning baseline, even when it selects the single "best" attribute.},
Author = {Mahesh Joshi and Mark Dredze and William W. Cohen and Carolyn P. Rose},
Booktitle = {North American Chapter of the Association for Computational Linguistics (NAACL) (short paper)},
Date-Added = {2013-02-13 12:53:55 -0500},
Date-Modified = {2017-08-09 19:22:23 +0000},
File = {http://aclweb.org/anthology/N/N13/N13-1080.pdf},
Pages = {685-690},
Title = {What's in a Domain? Multi-Domain Learning for Multi-Attribute Data},
Year = {2013}
}

@InProceedings{Kalchbrenner13recurrent,
  author = 	"Kalchbrenner, Nal and Blunsom, Phil",
  title = 	"Recurrent Continuous Translation Models",
  booktitle = 	"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1700--1709",
  address = 	"Seattle, Washington, USA",
  url = 	"http://aclweb.org/anthology/D13-1176"
}

@InProceedings{Khayrallah2017lattice,
  author = 	"Khayrallah, Huda and Kumar, Gaurav and Duh, Kevin and Post, Matt and Koehn, Philipp",
  title = 	"Neural Lattice Search for Domain Adaptation in Machine Translation",
  booktitle = 	"Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
  year = 	"2017",
  publisher = 	"Asian Federation of Natural Language Processing",
  pages = 	"20--25",
  address = 	"Taipei, Taiwan",
  url = 	"http://aclweb.org/anthology/I17-2004"
}

@inproceedings{Khosla12undoing,
  title={Undoing the damage of dataset bias},
  author={Khosla, Aditya and Zhou, Tinghui and Malisiewicz, Tomasz and Efros, Alexei A and Torralba, Antonio},
  booktitle={European Conference on Computer Vision},
  pages={158--171},
  year={2012},
  organization={Springer}
} 

@misc{Khresmoi17test,
 title = {Khresmoi Summary Translation Test Data 2.0},
 author = {Du{\v s}ek, Ond{\v r}ej and Haji{\v c}, Jan and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Libovick{\'y}, Jind{\v r}ich and Pecina, Pavel and Tamchyna, Ale{\v s} and Ure{\v s}ov{\'a}, Zde{\v n}ka},
 url = {http://hdl.handle.net/11234/1-2122},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Creative Commons - Attribution-{NonCommercial} 4.0 International ({CC} {BY}-{NC} 4.0)},
 year = {2017} }

@InProceedings{Klein17OpenNMT,
  author = 	"Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander",
  title = 	"{OpenNMT}: Open-Source Toolkit for Neural Machine Translation",
  booktitle = 	"Proceedings of ACL 2017, System Demonstrations",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"67--72",
  address = 	"Vancouver, Canada",
  url = 	"http://aclweb.org/anthology/P17-4012"
}
	
@InProceedings{Kobus17domaincontrol,
  author = 	"Kobus, Catherine and Crego, Josep and Senellart, Jean",
  title = 	"Domain Control for Neural Machine Translation",
  booktitle = 	"Proceedings of the International Conference Recent Advances in Natural Language Processing",
  series = "RANLP 2017",
  year = 	"2017",
  pages = 	"372--378",
  address = 	"Varna, Bulgaria",
  doi = 	"10.26615/978-954-452-049-6_049",
  url = 	"https://doi.org/10.26615/978-954-452-049-6_049"
}

@inproceedings{Koehn04statistical,
    title = "Statistical Significance Tests for Machine Translation Evaluation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-3250",
    pages = "388--395",
}

@inproceedings{Koehn05europarl,
	Address = {Phuket, Thailand},
	Author = {Philipp Koehn},
	Booktitle = {2nd Workshop on EBMT at the MT-Summit X},
	Pages = {79--86},
	Title = {Europarl: A Parallel Corpus for {Statistical Machine Translation}},
	Year = 2005}

@inproceedings{Bousmalis16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NeurIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 address = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://papers.nips.cc/paper/6254-domain-separation-networks.pdf},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
}

@InProceedings{Lambert11investigations,
  author = 	"Lambert, Patrik and Schwenk, Holger and Servan, Christophe and Abdul-Rauf, Sadaf",
  title = 	"Investigations on Translation Model Adaptation Using Monolingual Data",
  booktitle = 	"Proceedings of the Sixth Workshop on Statistical Machine Translation",
  year = 	"2011",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"284--293",
  address = 	"Edinburgh, Scotland",
  url = 	"http://aclweb.org/anthology/W11-2132"
}

@inproceedings{Lample18unsupervised,
  title={Unsupervised Machine Translation Using Monolingual Corpora Only},
  author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
  booktitle={International Conference on Learning Representations},
  year={2018},
  address = {Vancouver, Canada},
  url={https://openreview.net/forum?id=rkYTTf-AZ},
}

@inproceedings{Luong15stanford,
  Address = {Da Nang, Vietnam},
  Author = {Luong, Minh-Thang  and Manning, Christopher D.},
  Booktitle = {Proceedings of the International Workshop on Spoken Language Translation},
  Series = {IWSLT},
  Title = {Stanford Neural Machine Translation Systems for Spoken Language Domain},
  Year = {2015}}


@incollection{Mansour09domainadaptation,
title = {Domain Adaptation with Multiple Sources},
author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1041--1048},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf}
}


@article{McCloskey89catastrophic,
  title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
  author = "Michael McCloskey and Cohen, {Neal J.}",
  year = "1989",
  doi = "10.1016/S0079-7421(08)60536-8",
  language = "English (US)",
  volume = "24",
  pages = "109--165",
  journal = "Psychology of Learning and Motivation - Advances in Research and Theory",
  issn = "0079-7421",
  publisher = "Academic Press Inc.",
  number = "C",
}

@InProceedings{Miceli17regularize,
  author = 	"Miceli Barone, Antonio Valerio and Haddow, Barry and Germann, Ulrich and Sennrich, Rico",
  title = 	"Regularization techniques for fine-tuning in neural machine translation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1489--1494",
  address = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1156",
  url = 	"http://aclweb.org/anthology/D17-1156"
}

@InProceedings{Michel2018extreme,
  author = 	"Michel, Paul and Neubig, Graham",
  title = 	"Extreme Adaptation for Personalized Neural Machine Translation",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"312--318",
  address = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-2050"
}

@InProceedings{Moore10selection,
  author = 	"Moore, Robert C. and Lewis, William",
  title = 	"Intelligent Selection of Language Model Training Data",
  booktitle = 	"Proceedings of the ACL 2010 Conference Short Papers",
  year = 	"2010",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"220--224",
  address = 	"Uppsala, Sweden",
  url = 	"http://aclweb.org/anthology/P10-2041"
}

@article{Neubig19compare-mt,
  author    = {Graham Neubig and Zi{-}Yi Dou and Junjie Hu and Paul Michel and Danish Pruthi and Xinyi Wang and John Wieting},
  title     = {compare-mt: {A} Tool for Holistic Comparison of Language Generation Systems},
  journal   = {CoRR},
  volume    = {abs/1903.07926},
  year      = {2019},
  url       = {http://arxiv.org/abs/1903.07926},
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{Oren19distributionally,
    title = "Distributionally Robust Language Modeling",
    author = "Oren, Yonatan and Sagawa, Shiori and Hashimoto, Tatsunori and Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1432",
    doi = "10.18653/v1/D19-1432",
    pages = "4227--4237",
    abstract = "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
}

@Article{Pan10asurvey,
author={Sinno Jialin {Pan} and Qiang {Yang}},
journal={IEEE Transactions on Knowledge and Data Engineering},
title={A Survey on Transfer Learning},
doi = {doi: 10.1109/TKDE.2009.191},
year={2010},
volume={22},
number={10},
pages={1345-1359},
}

@inproceedings{Papineni02bleu,
	Address = {Stroudsburg, PA, USA},
	Author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	Booktitle = {Proceedings of the 40th Annual Meeting on Association for Computational Linguistics},
	Doi = {10.3115/1073083.1073135},
	Location = {Philadelphia, Pennsylvania},
	Numpages = {8},
	Pages = {311--318},
	Series = {ACL '02},
	Title = {{BLEU:} a method for automatic evaluation of machine translation},
	Year = {2002}}

@inproceedings{Paul10overview,
  title={Overview of the {IWSLT} 2010 evaluation campaign},
  author={Paul, Michael and Federico, Marcello and St{\"u}ker, Sebastian},
  booktitle={International Workshop on Spoken Language Translation (IWSLT) 2010},
  series = {IWSLT},
  address = {Paris, France},
  pages     = {3--27},
  year      = {2010},
  url = {https://www.isca-speech.org/archive/iwslt_10/papers/slta_003.pdf},
}

@inproceedings{Pei2018MADA,
  author    = {Zhongyi Pei and Zhangjie Cao and Mingsheng Long and Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},
  crossref  = {DBLP:conf/aaai/2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Peng17multitask,
  author = 	 {Nanyun Peng and Mark Dredze},
  title = 	 {Multi-task domain adaptation for sequence tagging},
  pages     = {91--100},
  year      = 2017,
  url       = {https://aclanthology.info/papers/W17-2612/w17-2612},
  booktitle = {Proceedings of the 2nd Workshop on Representation Learning for NLP},
  series = 	 {REP4NLP@ACL},
  address = 	 {Vancouver, Canada}}

@inproceedings{Pham19generic,
	Address = {Hong-Kong, CN},
	Author = {Pham, Minh Quang AND Crego, Josep-Maria AND Senellart, Jean AND Yvon, Fran\c{c}ois},
	Booktitle = {Proceedings of the 16th {International Workshop on Spoken Language Translation}},
	Series = {IWSLT},
	Keywords = {Machine Translation; Domain Adaptation},
	Pages = {9p},
	Title = {{Generic and Specialized Word Embeddings for Multi-Domain Machine Translation}},
	Url = {https://zenodo.org/record/3524979},
	Year = {2019}}

@inproceedings{Platanios18contextual,
	Address = {Brussels, Belgium},
	Author = {Platanios, Emmanouil Antonios and Sachan, Mrinmaya and Neubig, Graham and Mitchell, Tom},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {425--435},
	Publisher = {Association for Computational Linguistics},
	Title = {Contextual Parameter Generation for Universal Neural Machine Translation},
	Url = {http://aclweb.org/anthology/D18-1039},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1039}}

@incollection{Rebuffi17learning,
 title = {Learning multiple visual domains with residual adapters},
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems 30},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {506--516},
 year = {2017},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/6654-learning-multiple-visual-domains-with-  residual-adapters.pdf}
}

@inproceedings{Sajjad17neural,
  title={Neural Machine Translation Training in a Multi-Domain Scenario},
  author={Hassan Sajjad and Nadir Durrani and Fahim Dalvi and Yonatan Belinkov and Stephan Vogel},
  booktitle={Proceedings of the 14th International Workshop on Spoken Language Translation},
  series = {IWSLT 2017},
  address = {Tokyo, Japan},
  year={2017},
  url ={http://arxiv.org/abs/1708.08712}
}

@inproceedings{Saunders19ucam,
    title = "{UCAM} Biomedical Translation at {WMT}19: Transfer Learning Multi-domain Ensembles",
    author = "Saunders, Danielle and Stahlberg, Felix and Byrne, Bill",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-5421",
    pages = "169--174",
    abstract = "The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.",
}

@inproceedings{Sennrich13multidomain,
	Address = {Sofia, Bulgaria},
	Author = {Sennrich, Rico and Schwenk, Holger and Aransa, Walid},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Date-Added = {2019-05-14 22:42:31 +0200},
	Date-Modified = {2019-05-14 22:42:31 +0200},
	Month = aug,
	Pages = {832--840},
	Publisher = {Association for Computational Linguistics},
	Title = {A Multi-Domain Translation Model Framework for Statistical Machine Translation},
	Url = {https://www.aclweb.org/anthology/P13-1082},
	Year = {2013},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P13-1082}}

@inproceedings{Sennrich16BPE,
	Address = {Berlin, Germany},
	Author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Doi = {10.18653/v1/P16-1162},
	Month = aug,
	Pages = {1715--1725},
	Title = {Neural Machine Translation of Rare Words with Subword Units},
	Url = {https://www.aclweb.org/anthology/P16-1162},
	Year = {2016},
}

@InProceedings{Sennrich16improving,
  author = 	"Sennrich, Rico	and Haddow, Barry and Birch, Alexandra",
  title = 	"Improving Neural Machine Translation Models with Monolingual Data",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"86--96",
  address = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1009",
  url = 	"http://aclweb.org/anthology/P16-1009"
}

@inproceedings{Sennrich16neural,
	Address = {Berlin, Germany},
	Author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Doi = {10.18653/v1/P16-1162},
	Pages = {1715--1725},
	Title = {Neural Machine Translation of Rare Words with Subword Units},
	Url = {http://aclweb.org/anthology/P16-1162},
	Year = {2016},
} 

@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@article{Kirkpatrick17overcoming,
        author = {Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A. and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and Hassabis, Demis and Clopath, Claudia and Kumaran, Dharshan and Hadsell, Raia},
        title = {Overcoming catastrophic forgetting in neural networks},
        volume = {114},
        number = {13},
        pages = {3521--3526},
        year = {2017},
        doi = {10.1073/pnas.1611835114},
        publisher = {National Academy of Sciences},
        abstract = {Deep neural networks are currently the most successful machine-learning technique for solving a variety of tasks, including language translation, image classification, and image generation. One weakness of such models is that, unlike humans, they are unable to learn multiple tasks sequentially. In this work we propose a practical solution to train such models sequentially by protecting the weights important for previous tasks. This approach, inspired by synaptic consolidation in neuroscience, enables state of the art results on multiple reinforcement learning problems experienced sequentially.The ability to learn tasks in a sequential fashion is crucial to the development of artificial intelligence. Until now neural networks have not been capable of this and it has been widely thought that catastrophic forgetting is an inevitable feature of connectionist models. We show that it is possible to overcome this limitation and train networks that can maintain expertise on tasks that they have not experienced for a long time. Our approach remembers old tasks by selectively slowing down learning on the weights important for those tasks. We demonstrate our approach is scalable and effective by solving a set of classification tasks based on a hand-written digit dataset and by learning several Atari 2600 games sequentially.},
        issn = {0027-8424},
        URL = {https://www.pnas.org/content/114/13/3521},
        eprint = {https://www.pnas.org/content/114/13/3521.full.pdf},
        journal = {Proceedings of the National Academy of Sciences}
}

@article{Shimodaira00improving,
title = "Improving predictive inference under covariate shift by weighting the log-likelihood function",
journal = "Journal of Statistical Planning and Inference",
doi = "10.1016/S0378-3758(00)00115-4",
volume = "90",
number = "2",
pages = "227 - 244",
year = "2000",
issn = "0378-3758",
author = "Hidetoshi Shimodaira",
}

@InProceedings{Steinberger06acquis,
  author = {Ralf Steinberger and Bruno Pouliquen and Anna Widiger and Camelia Ignat and Tomaž Erjavec and Dan Tufis and Dániel Varga },
  title = {The {JRC-Acquis}: A multilingual aligned parallel corpus with 20+ languages},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation},
  series = {LREC'06},
  year = {2006},
  month = {may},
  date = {24-26},
  address = {Genoa, Italy},
  publisher = {European Language Resources Association (ELRA)},
 }

@article{Su19exploring,
author={Jinsong Su and Jiali Zeng and Jun Xie and Huating Wen and Yongjing Yin and Yang Liu},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)},
title={Exploring Discriminative Word-Level Domain Contexts for Multi-domain Neural Machine Translation},
year={2019},
volume={},
number={},
pages={1-1},
keywords={Multi-domain Neural Machine Translation;Word-Level Context;Adversarial Training},
doi={10.1109/TPAMI.2019.2954406},
ISSN={1939-3539},
month={},}

@inproceedings{Sutskever14sequence,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
 title = {Sequence to Sequence Learning with Neural Networks},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 address = {Montreal, Canada},
 pages = {3104--3112},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
 acmid = {2969173},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
}

@inproceedings{platanios19competence,
    title = "Competence-based Curriculum Learning for Neural Machine Translation",
    author = "Platanios, Emmanouil Antonios  and
      Stretcu, Otilia  and
      Neubig, Graham  and
      Poczos, Barnabas  and
      Mitchell, Tom",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1119",
    doi = "10.18653/v1/N19-1119",
    pages = "1162--1172",
    abstract = "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70{\%} decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
}

@inproceedings{koehn18findings,
    title = "Findings of the {WMT} 2018 Shared Task on Parallel Corpus Filtering",
    author = "Koehn, Philipp  and
      Khayrallah, Huda  and
      Heafield, Kenneth  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6453",
    doi = "10.18653/v1/W18-6453",
    pages = "726--739",
    abstract = "We posed the shared task of assigning sentence-level quality scores for a very noisy corpus of sentence pairs crawled from the web, with the goal of sub-selecting 1{\%} and 10{\%} of high-quality data to be used to train machine translation systems. Seventeen participants from companies, national research labs, and universities participated in this task.",
}

@InProceedings{Tars18multidomain,
  author    = {Sander Tars and Mark Fishel},
  title     = {Multi-Domain Neural Machine Translation},
  booktitle = {Proceedings of the 21st Annual Conference of the European Association for Machine Translation},
  year = 	 2018,
  editor = 	 {Juan Antonio Pérez-Ortiz and Felipe Sánchez-Martínez and Miquel Esplà-Gomis and Maja Popović and Celia Rico and André Martins and Joachim Van den Bogaert and Mikel L. Forcada},
  series = 	 {EAMT},
  pages = 	 {259--269},
  address = 	 {Alicante, Spain},
  Url = {https://arxiv.org/pdf/1805.02282.pdf},
  organization = {EAMT}}

@inproceedings{Thompson18freezing,
	Address = {Belgium, Brussels},
	Author = {Thompson, Brian and Khayrallah, Huda and Anastasopoulos, Antonios and McCarthy, Arya D. and Duh, Kevin and Marvin, Rebecca and McNamee, Paul and Gwinnup, Jeremy and Anderson, Tim and Koehn, Philipp},
	Booktitle = {Proceedings of the Third Conference on Machine Translation: Research Papers},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:22:53 +0200},
	Pages = {124--132},
	Publisher = {Association for Computational Linguistics},
	Title = {Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation},
	Url = {http://aclweb.org/anthology/W18-6313},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/W18-6313}}

@inproceedings{Thompson19overcoming,
    title = "Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation",
    author = "Thompson, Brian and Gwinnup, Jeremy and Khayrallah, Huda and Duh, Kevin and Koehn, Philipp",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1209",
    doi = "10.18653/v1/N19-1209",
    pages = "2062--2068",
    abstract = "Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC){---}a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.",
}



@InCollection{Tiedemann09news,
  author =	  {J\"org Tiedemann},
  title =	  {News from {OPUS} - {A} Collection of Multilingual
                  Parallel Corpora with Tools and Interfaces},
  booktitle =	  {Recent Advances in Natural Language Processing},
  publisher =	  {John Benjamins, Amsterdam/Philadelphia},
  year =          2009,
  pages =         {237--248},
  editor =        {N. Nicolov and K. Bontcheva and G. Angelova and
                  R. Mitkov},
  volume =	  {V},
  address =	  {Borovets, Bulgaria},
  isbn =          {978 90 272 4825 1},
  pdf =           {http://stp.lingfil.uu.se/~joerg/published/ranlp-V.pdf},
  topic  =        {Parallel corpora}
}

@InProceedings{Tiedemann12parallel,
  author = {J\"org Tiedemann},
  title = {Parallel Data, Tools and Interfaces in {OPUS}},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation},
  series = {LREC'12},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
 }

@InProceedings{Utiyama03measure,
  author = 	"Utiyama, Masao
		and Isahara, Hitoshi",
  title = 	"Reliable Measures for Aligning {Japanese-English} News Articles and Sentences",
  booktitle = 	"Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
  address =    "Sapporo, Japan",
  year = 	"2003",
  url = 	"http://aclweb.org/anthology/P03-1010"
}

@incollection{Vaswani17attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{Vilar18learning,
	Address = {New Orleans, Louisiana},
	Author = {Vilar, David},
	Booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:22:53 +0200},
	Doi = {10.18653/v1/N18-2080},
	Pages = {500--505},
	Title = {Learning Hidden Unit Contribution for Adapting Neural Machine Translation Models},
	Url = {http://aclweb.org/anthology/N18-2080},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/N18-2080},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/N18-2080}}


@InProceedings{Wang14neural,
  author = 	"Wang, Rui and Zhao, Hai and Lu, Bao-Liang and Utiyama, Masao and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  address = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui and Zhao, Hai and Lu, Bao-Liang and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@Inproceedings{Wang16transfer,
author={Dong {Wang} and Thomas Feng {Zheng}},
booktitle={Proceedings of the 2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA)},
title={Transfer learning for speech and language processing},
year={2015},
pages={1225-1237}}

@inproceedings{Wang17instance,
	Author = {Wang, Rui and Utiyama, Masao and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2017-09-21 21:28:02 +0000},
	Date-Modified = {2017-09-21 21:28:10 +0000},
	Location = {Copenhagen, Denmark},
	Pages = {1483--1489},
	Publisher = {Association for Computational Linguistics},
	Title = {Instance Weighting for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/D17-1155},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D17-1155}}

@inproceedings{Wang17sentence,
	Address = {Vancouver, Canada},
	Author = {Wang, Rui and Finch, Andrew and Utiyama, Masao and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:22:53 +0200},
	Doi = {10.18653/v1/P17-2089},
	Pages = {560--566},
	Publisher = {Association for Computational Linguistics},
	Title = {Sentence Embedding for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/P17-2089},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/P17-2089},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P17-2089}}

@article{Wang18sentence,
  title={Sentence selection and weighting for neural machine translation domain adaptation},
  author={Wang, Rui and Utiyama, Masao and Finch, Andrew and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={26},
  number={10},
  pages={1727--1741},
  year={2018},
  publisher={IEEE}
}

@inproceedings{Wang20general,
    title={Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks},
    author={Yong Wang and Longyue Wang and Shuming Shi and Victor O. K. Li and Zhaopeng Tu},
    year={2020},
    url={http://arxiv.org/abs/1911.09912},
    booktitle={Proceedings of the Annual conference of the American Association for Artificial Intelligence},
    address = {New York, NY},
    series={AAAI 2020},
}

@article{Wen19domain,
  title={Domain Aggregation Networks for Multi-Source Domain Adaptation},
  author={Wen, Junfeng and Greiner, Russell and Schuurmans, Dale},
  journal={arXiv preprint arXiv:1909.05352},
  year={2019}
} 

@inproceedings{Yang15unified,
	title = {A unified perspective on multi-domain and multi-task learning},
	author = {Yongxin Yang and Timothy M. Hospedales},
	booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	year = {2015},
	url = {https://arxiv.org/abs/1412.7489},
}

@inproceedings{Yarowsky93onesense,
 author = {Yarowsky, David},
 title = {One Sense Per Collocation},
 booktitle = {Proceedings of the Workshop on Human Language Technology},
 series = {HLT '93},
 year = {1993},
 isbn = {1-55860-324-7},
 location = {Princeton, New Jersey},
 pages = {266--271},
 numpages = {6},
 url = {https://doi.org/10.3115/1075671.1075731},
 doi = {10.3115/1075671.1075731},
 acmid = {1075731},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@inproceedings{Zeng18multidomain,
	Address = {Brussels, Belgium},
	Author = {Zeng, Jiali and Su, Jinsong and Wen, Huating and Liu, Yang and Xie, Jun and Yin, Yongjing and Zhao, Jianqiang},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {447--457},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination},
	Url = {http://aclweb.org/anthology/D18-1041},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1041}} 

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian and Li, Liangyou and Way, Andy and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}


@inproceedings{Zhang18sentence,
	Address = {Santa Fe, New Mexico, USA},
	Author = {Zhang, Shiqi and Xiong, Deyi},
	Booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:23:22 +0200},
	Pages = {3181--3190},
	Title = {Sentence Weighting for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/C18-1269},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/C18-1269}}

@incollection{Zhao18adversarial,
title = {Adversarial Multiple Source Domain Adaptation},
author = {Zhao, Han and Zhang, Shanghang and Wu, Guanhang and Moura, Jos\'{e} M. F. and Costeira, Joao P and Gordon, Geoffrey J},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8559--8570},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8075-adversarial-multiple-source-domain-adaptation.pdf}
} 

@inproceedings{Xu19lexical,
  address = {Hong Kong, China},
  author = {Xu, Jitao and Crego, Josep and Senellart, Jean},
  booktitle = {Proceedings of the 16th International Workshop on Spoken Language Translation},
  title        = {Lexical Micro-adaptation for Neural Machine Translation},
  series = "IWSLT 2019",
  url = "https://zenodo.org/record/3524977",
  Year         = "2019",
  month        = "nov"
}

@inproceedings{li-etal-2018-one,
    title = "One Sentence One Model for Neural Machine Translation",
    author = "Li, Xiaoqing  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the 11th Language Resources and Evaluation Conference",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resource Association",
    url = "https://www.aclweb.org/anthology/L18-1146",
}

@inproceedings{currey20distilling,
    title = "Distilling Multiple Domains for Neural Machine Translation",
    author = "Currey, Anna  and
      Mathur, Prashant  and
      Dinu, Georgiana",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.364",
    pages = "4500--4511",
    abstract = "Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.",
}