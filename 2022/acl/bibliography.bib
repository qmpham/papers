@article{Cauchy1847method,
author="Cauchy, A.",
title="Méthode générale pour la résolution des systèmes d'équations simultanées",
journal="C.R. Acad. Sci. Paris",
ISSN="",
publisher="",
year="1847",
month="",
volume="25",
number="",
pages="536-538",
URL="https://ci.nii.ac.jp/naid/10026863174/en/",
DOI="",
}

@article{Kullback51On,
author = {S. Kullback and R. A. Leibler},
title = {{On Information and Sufficiency}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {79 -- 86},
year = {1951},
doi = {10.1214/aoms/1177729694},
URL = {https://doi.org/10.1214/aoms/1177729694}
}


@article{Herbert51stochastic,
author = {Herbert Robbins and Sutton Monro},
title = {{A Stochastic Approximation Method}},
volume = {22},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {400 -- 407},
year = {1951},
doi = {10.1214/aoms/1177729586},
URL = {https://doi.org/10.1214/aoms/1177729586}
}
@article{Kiefer52stochastic,
author = {J. Kiefer and J. Wolfowitz},
title = {{Stochastic Estimation of the Maximum of a Regression Function}},
volume = {23},
journal = {The Annals of Mathematical Statistics},
number = {3},
publisher = {Institute of Mathematical Statistics},
pages = {462 -- 466},
year = {1952},
doi = {10.1214/aoms/1177729392},
URL = {https://doi.org/10.1214/aoms/1177729392}
}

@article{Salton73On,
  title={On the Specification of Term Values in Automatic Indexing},
  author={G. Salton and C. S. Yang},
  journal={Journal of Documentation},
  year={1973},
  volume={29},
  pages={351-372}
}

@article {Tulving82Priming,
        title = {Priming effects in word-fragment completion are independent of recognition memory},
        journal = {Journal of Experimental Psychology: Learning, Memory, \& Cognition},
        volume = {8},
        year = {1982},
        pages = {336-342},
        author = {Tulving, E and D.L. Schacter and Stark, H.A.}
}

@inproceedings{Baum87Supervised,
author = {Baum, Eric B. and Wilczek, Frank},
title = {Supervised Learning of Probability Distributions by Neural Networks},
year = {1987},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {We propose that the back propagation algorithm for supervised learning can be generalized,
put on a satisfactory conceptual footing, and very likely made more efficient by defining
the values of the output and input neurons as probabilities and varying the synaptic
weights in the gradient direction of the log likelihood, rather than the 'error'.},
booktitle = {Proceedings of the 1987 International Conference on Neural Information Processing Systems},
pages = {52–61},
numpages = {10},
series = {NIPS'87}
}

@inbook{Rumelhart88learning,
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title = {Learning Representations by Back-Propagating Errors},
year = {1988},
isbn = {0262010976},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
booktitle = {Neurocomputing: Foundations of Research},
pages = {696–699},
numpages = {4}
}

@article{Michael89catastrophic,
title = "Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem",
author = "Michael McCloskey and Cohen, {Neal J.}",
year = "1989",
doi = "10.1016/S0079-7421(08)60536-8",
language = "English (US)",
volume = "24",
pages = "109--165",
journal = "Psychology of Learning and Motivation - Advances in Research and Theory",
issn = "0079-7421",
publisher = "Academic Press Inc.",
number = "C",
}

@inproceedings{Krogh91simple,
  author    = {Anders Krogh and
               John A. Hertz},
  editor    = {John E. Moody and
               Stephen Jose Hanson and
               Richard Lippmann},
  title     = {A Simple Weight Decay Can Improve Generalization},
  booktitle = {Advances in Neural Information Processing Systems 4, {[NIPS} Conference,
               Denver, Colorado, USA, December 2-5, 1991]},
  pages     = {950--957},
  publisher = {Morgan Kaufmann},
  year      = {1991},
  url       = {http://papers.nips.cc/paper/563-a-simple-weight-decay-can-improve-generalization},
  timestamp = {Thu, 21 Jan 2021 15:15:26 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/KroghH91.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Williams92simple,
author = {Williams, Ronald J.},
title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
year = {1992},
issue_date = {May 1992},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {8},
number = {3–4},
issn = {0885-6125},
url = {https://doi.org/10.1007/BF00992696},
doi = {10.1007/BF00992696},
abstract = {This article presents a general class of associative reinforcement learning algorithms
for connectionist networks containing stochastic units. These algorithms, called REINFORCE
algorithms, are shown to make weight adjustments in a direction that lies along the
gradient of expected reinforcement in both immediate-reinforcement tasks and certain
limited forms of delayed-reinforcement tasks, and they do this without explicitly
computing gradient estimates or even storing information from which such estimates
could be computed. Specific examples of such algorithms are presented, some of which
bear a close relationship to certain existing algorithms while others are novel but
potentially interesting in their own right. Also given are results that show how such
algorithms can be naturally integrated with backpropagation. We close with a brief
discussion of a number of additional issues surrounding the use of such algorithms,
including what is known about their limiting behaviors as well as further considerations
that might be used to help develop similar but potentially more powerful reinforcement
learning algorithms.},
journal = {Mach. Learn.},
month = may,
pages = {229–256},
numpages = {28},
keywords = {gradient descent, mathematical analysis, Reinforcement learning, connectionist networks}
}

@article{Wolpert92stacked,
title = "Stacked generalization",
journal = "Neural Networks",
volume = "5",
number = "2",
pages = "241 - 259",
year = "1992",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(05)80023-1",
url = "http://www.sciencedirect.com/science/article/pii/S0893608005800231",
author = "David H. Wolpert",
keywords = "Generalization and induction, Combining generalizers, Learning set preprocessing, cross-validation, Error estimation and correction",
abstract = "This paper introduces stacked generalization, a scheme for minimizing the generalization error rate of one or more generalizers. Stacked generalization works by deducing the biases of the generalizer(s) with respect to a provided learning set. This deduction proceeds by generalizing in a second space whose inputs are (for example) the guesses of the original generalizers when taught with part of the learning set and trying to guess the rest of it, and whose output is (for example) the correct guess. When used with multiple generalizers, stacked generalization can be seen as a more sophisticated version of cross-validation, exploiting a strategy more sophisticated than cross-validation's crude winner-takes-all for combining the individual generalizers. When used with a single generalizer, stacked generalization is a scheme for estimating (and then correcting for) the error of a generalizer which has been trained on a particular learning set and then asked a particular question. After introducing stacked generalization and justifying its use, this paper presents two numerical experiments. The first demonstrates how stacked generalization improves upon a set of separate generalizers for the NETtalk task of translating text to phonemes. The second demonstrates how stacked generalization improves the performance of a single surface-fitter. With the other experimental evidence in the literature, the usual arguments supporting cross-validation, and the abstract justifications presented in this paper, the conclusion is that for almost any real-world generalization problem one should use some version of stacked generalization to minimize the generalization error rate. This paper ends by discussing some of the variations of stacked generalization, and how it touches on other fields like chaos theory."
}

@inproceedings{Yarowsky93onesense,
author = {Yarowsky, David},
title = {One Sense per Collocation},
year = {1993},
isbn = {1558603247},
publisher = {Association for Computational Linguistics},
address = {USA},
url = {https://doi.org/10.3115/1075671.1075731},
doi = {10.3115/1075671.1075731},
abstract = {Previous work [Gale, Church and Yarowsky, 1992] showed that with high probability
a polysemous word has one sense per discourse. In this paper we show that for certain
definitions of collocation, a polysemous word exhibits essentially only one sense
per collocation. We test this empirical hypothesis for several definitions of sense
and collocation, and discover that it holds with 90--99% accuracy for binary ambiguities.
We utilize this property in a disambiguation algorithm that achieves precision of
92% using combined models of very local context.},
booktitle = {Proceedings of the Workshop on Human Language Technology},
pages = {266–271},
numpages = {6},
location = {Princeton, New Jersey},
series = {HLT '93}
}

@article{Gage94anew,
author = {Gage, Philip},
title = {A New Algorithm for Data Compression},
year = {1994},
issue_date = {Feb. 1994},
publisher = {R &amp; D Publications, Inc.},
address = {USA},
volume = {12},
number = {2},
issn = {0898-9788},
journal = {C Users J.},
month = feb,
pages = {23–38},
numpages = {16}
}

@article{Caruana97multitask,
 author = {Caruana, Rich},
 title = {Multitask Learning},
 journal = {Mach. Learn.},
 issue_date = {July 1997},
 volume = {28},
 number = {1},
 month = jul,
 year = {1997},
 issn = {0885-6125},
 pages = {41--75},
 numpages = {35},
 url = {https://doi.org/10.1023/A:1007379606734},
 doi = {10.1023/A:1007379606734},
 acmid = {262872},
 publisher = {Kluwer Academic Publishers},
 address = {Hingham, MA, USA},
 keywords = {backpropagation, generalization, inductive transfer, k-nearest neighbor, kernel regression, multitask learning, parallel transfer, supervised learning}
} 

@article{Hochreiter97long,
author = {Hochreiter, Sepp and Schmidhuber, J\"{u}rgen},
title = {Long Short-Term Memory},
year = {1997},
issue_date = {November 15, 1997},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
volume = {9},
number = {8},
issn = {0899-7667},
url = {https://doi.org/10.1162/neco.1997.9.8.1735},
doi = {10.1162/neco.1997.9.8.1735},
abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
journal = {Neural Comput.},
month = nov,
pages = {1735–1780},
numpages = {46}
}

@inproceedings{Och98improving,
    title = "Improving Statistical Natural Language Translation with Categories and Rules",
    author = "Och, Franz Josef  and
      Weber, Hans",
    booktitle = "36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",
    month = aug,
    year = "1998",
    address = "Montreal, Quebec, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P98-2162",
    doi = "10.3115/980691.980731",
    pages = "985--989",
}

@inbook{Thrun98learning,
author = {Thrun, Sebastian and Pratt, Lorien},
title = {Learning to Learn: Introduction and Overview},
year = {1998},
isbn = {0792380479},
publisher = {Kluwer Academic Publishers},
address = {USA},
booktitle = {Learning to Learn},
pages = {3–17},
numpages = {15}
}

@article{Shimodaira00improving,
title = "Improving predictive inference under covariate shift by weighting the log-likelihood function",
journal = "Journal of Statistical Planning and Inference",
doi = "10.1016/S0378-3758(00)00115-4",
volume = "90",
number = "2",
pages = "227 - 244",
year = "2000",
issn = "0378-3758",
author = "Hidetoshi Shimodaira"
}

@article{Vilalta01perspective,
author = {Vilalta, Ricardo and Drissi, Youssef},
title = {A Perspective View and Survey of Meta-Learning},
year = {2002},
issue_date = {October 2002},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {18},
number = {2},
issn = {0269-2821},
url = {https://doi.org/10.1023/A:1019956318069},
doi = {10.1023/A:1019956318069},
abstract = {Different researchers hold different views of what the term meta-learning exactly
means. The first part of this paper provides our own perspective view in which the
goal is to build self-adaptive learners (i.e. learning algorithms that improve their
bias dynamically through experience by accumulating meta-knowledge). The second part
provides a survey of meta-learning as reported by the machine-learning literature.
We find that, despite different views and research lines, a question remains constant:
how can we exploit knowledge about learning (i.e. meta-knowledge) to improve the performance
of learning algorithms? Clearly the answer to this question is key to the advancement
of the field and continues being the subject of intensive research.},
journal = {Artif. Intell. Rev.},
month = oct,
pages = {77–95},
numpages = {19},
keywords = {classification, meta-knowledge, inductive learning}
}

@article{Lee01Genres,
  title={Genres, Registers, Text Types, Domains and Styles: Clarifying the Concepts and Navigating a Path through the BNC Jungle},
  author={D. Y. Lee},
  journal={Language Learning \altand Technology},
  year={2001},
  volume={5},
  pages={37-72}
}

@article{Jaeger02tutorial,
author = {Jaeger, Herbert},
year = {2002},
month = {01},
pages = {},
title = {Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach},
volume = {5},
journal = {GMD-Forschungszentrum Informationstechnik, 2002.}
}

@inproceedings{Papineni02bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}	

@article{Bengio03aneural,
author = {Bengio, Yoshua and Ducharme, R\'{e}jean and Vincent, Pascal and Janvin, Christian},
title = {A Neural Probabilistic Language Model},
year = {2003},
issue_date = {3/1/2003},
publisher = {JMLR.org},
volume = {3},
number = {null},
issn = {1532-4435},
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
journal = {J. Mach. Learn. Res.},
month = mar,
pages = {1137–1155},
numpages = {19}
}

@inproceedings{Utiyama03reliable,
    title = "Reliable Measures for Aligning {J}apanese-{E}nglish News Articles and Sentences",
    author = "Utiyama, Masao  and
      Isahara, Hitoshi",
    booktitle = "Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2003",
    address = "Sapporo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P03-1010",
    doi = "10.3115/1075096.1075106",
    pages = "72--79",
}

@article{Villani03topics,
author = {Villani, C},
year = {2003},
month = {01},
pages = {},
title = {Topics in Optimal Transportation Theory},
volume = {58},
doi = {10.1090/gsm/058}
}

@inproceedings{Koehn04statistical,
    title = "Statistical Significance Tests for Machine Translation Evaluation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-3250",
    pages = "388--395",
}

@inproceedings{Koehn04pharaoh,
    title = "Pharaoh: a beam search decoder for phrase-based statistical machine translation models",
    author = "Koen, Philipp",
    booktitle = "Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers",
    month = sep # " 28 - " # oct # " 2",
    year = "2004",
    address = "Washington, USA",
    publisher = "Springer",
    url = "https://link.springer.com/chapter/10.1007/978-3-540-30194-3_13",
    pages = "115--124",
    abstract = "We describe Pharaoh, a freely available decoder for phrase-based statistical machine translation models. The decoder is the implement at ion of an efficient dynamic programming search algorithm with lattice generation and XML markup for external components.",
}

@InProceedings{Burago04metric,
author="Burago, Yuri
and Shoenthal, David",
editor="Bingham, Kenrick
and Kurylev, Yaroslav V.
and Somersalo, Erkki",
title="Metric Geometry",
booktitle="New Analytic and Geometric Methods in Inverse Problems",
year="2004",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="3--50",
abstract="Much of one's mathematical experience with regard to metric spaces begins at the level of the metric. Instead of starting with a metric, in many cases we must begin with the length of paths as the primary notion. From this, we will derive a distance function. More precisely, we can introduce a new distance which is measured along the shortest path between two points in a space (as opposed to simply measuring the Euclidean distance between the two points). One says that a distance function on a metric space is an intrinsic metric if the distance between two points can be realized by paths connecting the points (mathematically, it must be equal to the infimum of lengths of paths between the points---a shortest path may not exist). If the length of paths is to be our primary notion, we must ask for a rigorous definition, from where it may arise, and what the properties are of such structures (which we will call length structures).",
isbn="978-3-662-08966-8"
}

@inproceedings{Koehn05europarl,
    title = "{E}uroparl: A Parallel Corpus for Statistical Machine Translation",
    author = "Koehn, Philipp",
    booktitle = "Proceedings of Machine Translation Summit X: Papers",
    month = sep # " 13-15",
    year = "2005",
    address = "Phuket, Thailand",
    url = "https://aclanthology.org/2005.mtsummit-papers.11",
    pages = "79--86",
    abstract = "We collected a corpus of parallel text in 11 languages from the proceedings of the European Parliament, which are published on the web. This corpus has found widespread use in the NLP community. Here, we focus on its acquisition and its application as training data for statistical machine translation (SMT). We trained SMT systems for 110 language pairs, which reveal interesting clues into the challenges ahead.",
}

@article{Daume06domain,
author = {Daum\'{e}, Hal and Marcu, Daniel},
title = {Domain Adaptation for Statistical Classifiers},
year = {2006},
issue_date = {May 2006},
publisher = {AI Access Foundation},
address = {El Segundo, CA, USA},
volume = {26},
number = {1},
issn = {1076-9757},
abstract = {The most basic assumption used in statistical learning theory is that training data
and test data are drawn from the same underlying distribution. Unfortunately, in many
applications, the "in-domain" test data is drawn from a distribution that is related,
but not identical, to the "out-of-domain" distribution of the training data. We consider
the common case in which labeled out-of-domain data is plentiful, but labeled in-domain
data is scarce. We introduce a statistical formulation of this problem in terms of
a simple mixture model and present an instantiation of this framework to maximum entropy
classifiers and their linear chain counterparts. We present efficient inference algorithms
for this special case based on the technique of conditional expectation maximization.
Our experimental results show that our approach leads to improved performance on three
real world tasks on four different data sets from the natural language processing
domain.},
journal = {J. Artif. Int. Res.},
month = may,
pages = {101–126},
numpages = {26}
}

@InProceedings{Steinberger06acquis,
  author = {Ralf Steinberger and Bruno Pouliquen and Anna Widiger and Camelia Ignat and Tomaž Erjavec and Dan Tufis and Dániel Varga },
  title = {The {JRC-Acquis}: A multilingual aligned parallel corpus with 20+ languages},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation},
  series = {LREC'06},
  year = {2006},
  month = {may},
  date = {24-26},
  address = {Genoa, Italy},
  publisher = {European Language Resources Association (ELRA)},
 }

@inproceedings{Blitzer06Domain,
author = {Blitzer, John and McDonald, Ryan and Pereira, Fernando},
title = {Domain Adaptation with Structural Correspondence Learning},
year = {2006},
isbn = {1932432736},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {Discriminative learning methods are widely used in natural language processing. These
methods work best when their training and test data are drawn from the same distribution.
For many NLP tasks, however, we are confronted with new domains in which labeled data
is scarce or non-existent. In such cases, we seek to adapt existing models from a
resource-rich source domain to a resource-poor target domain. We introduce structural
correspondence learning to automatically induce correspondences among features from
different domains. We test our technique on part of speech tagging and show performance
gains for varying amounts of source and target training data, as well as improvements
in target domain parsing accuracy using our improved tagger.},
booktitle = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing},
pages = {120–128},
numpages = {9},
location = {Sydney, Australia},
series = {EMNLP '06}
}

@inproceedings{Jiang07instance,
    title = "Instance Weighting for Domain Adaptation in {NLP}",
    author = "Jiang, Jing  and
      Zhai, ChengXiang",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-1034",
    pages = "264--271",
}

@phdthesis{Blitzer07domain,
author = {Blitzer, John},
advisor = {Pereira, Fernando},
title = {Domain Adaptation of Natural Language Processing Systems},
year = {2008},
isbn = {9780549577409},
publisher = {University of Pennsylvania},
address = {USA},
abstract = {Statistical language processing models are being applied to an ever wider and more
varied range of linguistic domains. Collecting and curating training sets for each
different domain is prohibitively expensive, and at the same time differences in vocabulary
and writing style across domains can cause state-of-the-art supervised models to dramatically
increase in error. The first part of this thesis describes structural correspondence
learning (SCL), a method for adapting linear discriminative models from resource-rich
source  domains to resource-poor  target  domains. The key idea is the use of  pivot
features which occur frequently and behave similarly in both the source and target
domains. SCL builds a shared representation by searching for a low-dimensional feature
subspace that allows us to accurately predict the presence or absence of pivot features
on unlabeled data. We demonstrate SCL on two text processing problems: sentiment classification
of product reviews and part of speech tagging. For both tasks, SCL significantly improves
over state of the art supervised models using only unlabeled target data. In the second
part of the thesis, we develop a formal framework for analyzing domain adaptation
tasks. We first describe a measure of divergence, the  H D H  -divergence, that depends
on the hypothesis class  H  from which we estimate our supervised model. We then use
this measure to state an upper bound on the true target error of a model trained to
minimize a convex combination of empirical source and target errors. The bound characterizes
the tradeoff inherent in training on both the large quantity of biased source data
and the small quantity of unbiased target data, and we can compute it from finite
labeled and unlabeled samples of the source and target distributions under relatively
weak assumptions. Finally, we confirm experimentally that the bound corresponds well
to empirical target error for the task of sentiment classification.},
note = {AAI3309400}
}

@inproceedings{Ben07analysis,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Pereira, Fernando},
title = {Analysis of Representations for Domain Adaptation},
year = {2006},
publisher = {MIT Press},
address = {Cambridge, MA, USA},
abstract = {Discriminative learning methods for classification perform well when training and
test data are drawn from the same distribution. In many situations, though, we have
labeled training data for a source domain, and we wish to learn a classifier which
performs well on a target domain with a different distribution. Under what conditions
can we adapt a classifier trained on the source domain for use in the target domain?
Intuitively, a good feature representation is a crucial factor in the success of domain
adaptation. We formalize this intuition theoretically with a generalization bound
for domain adaption. Our theory illustrates the tradeoffs inherent in designing a
representation for domain adaptation and gives a new justification for a recently
proposed model. It also points toward a promising new model for domain adaptation:
one which explicitly minimizes the difference between the source and target domains,
while at the same time maximizing the margin of the training set.},
booktitle = {Proceedings of the 19th International Conference on Neural Information Processing Systems},
pages = {137–144},
numpages = {8},
location = {Canada},
series = {NIPS'06}
}

@inproceedings{Daume07frustratingly,
    title = "Frustratingly Easy Domain Adaptation",
    author = "Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P07-1033",
    pages = "256--263",
}

@inproceedings{Foster07mixture,
    title = "Mixture-Model Adaptation for {SMT}",
    author = "Foster, George  and
      Kuhn, Roland",
    booktitle = "Proceedings of the Second Workshop on Statistical Machine Translation",
    month = jun,
    year = "2007",
    address = "Prague, Czech Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W07-0717",
    pages = "128--135",
}

@inproceedings{Collobert08aunified,
author = {Collobert, Ronan and Weston, Jason},
title = {A Unified Architecture for Natural Language Processing: Deep Neural Networks with Multitask Learning},
year = {2008},
isbn = {9781605582054},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1390156.1390177},
doi = {10.1145/1390156.1390177},
abstract = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
booktitle = {Proceedings of the 25th International Conference on Machine Learning},
pages = {160–167},
numpages = {8},
location = {Helsinki, Finland},
series = {ICML '08}
}

@book{Quinonero08dataset,
author = {Quionero-Candela, Joaquin and Sugiyama, Masashi and Schwaighofer, Anton and Lawrence, Neil D.},
title = {Dataset Shift in Machine Learning},
year = {2009},
isbn = {0262170051},
publisher = {The MIT Press},
abstract = {Dataset shift is a common problem in predictive modeling that occurs when the joint
distribution of inputs and outputs differs between training and test stages. Covariate
shift, a particular case of dataset shift, occurs when only the input distribution
changes. Dataset shift is present in most practical applications, for reasons ranging
from the bias introduced by experimental design to the irreproducibility of the testing
conditions at training time. (An example is -email spam filtering, which may fail
to recognize spam that differs in form from the spam the automatic filter has been
built on.) Despite this, and despite the attention given to the apparently similar
problems of semi-supervised learning and active learning, dataset shift has received
relatively little attention in the machine learning community until recently. This
volume offers an overview of current efforts to deal with dataset and covariate shift.
The chapters offer a mathematical and philosophical introduction to the problem, place
dataset shift in relationship to transfer learning, transduction, local learning,
active learning, and semi-supervised learning, provide theoretical views of dataset
and covariate shift (including decision theoretic and Bayesian perspectives), and
present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel,
Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton,
Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller,
Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi
Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information
Processing series}
}

@inproceedings{Dredze08online,
author = {Dredze, Mark and Crammer, Koby},
title = {Online Methods for Multi-Domain Learning and Adaptation},
year = {2008},
publisher = {Association for Computational Linguistics},
address = {USA},
abstract = {NLP tasks are often domain specific, yet systems can learn behaviors across multiple
domains. We develop a new multi-domain online learning framework based on parameter
combination from multiple classifiers. Our algorithms draw from multi-task learning
and domain adaptation to adapt multiple source domain classifiers to a new target
domain, learn across multiple similar domains, and learn across a large number of
disparate domains. We evaluate our algorithms on two popular NLP domain adaptation
tasks: sentiment classification and spam filtering.},
booktitle = {Proceedings of the Conference on Empirical Methods in Natural Language Processing},
pages = {689–697},
numpages = {9},
location = {Honolulu, Hawaii},
series = {EMNLP '08}
}

@inproceedings{Bertoldi09domain,
    title = "Domain Adaptation for Statistical Machine Translation with Monolingual Resources",
    author = "Bertoldi, Nicola and Federico, Marcello",
    booktitle = "Proceedings of the Fourth Workshop on Statistical Machine Translation",
    month = mar,
    year = "2009",
    address = "Athens, Greece",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W09-0432",
    pages = "182--189",
}

@incollection{Mansour09domain,
title = {Domain Adaptation with Multiple Sources},
author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
booktitle = {Advances in Neural Information Processing Systems 21},
editor = {D. Koller and D. Schuurmans and Y. Bengio and L. Bottou},
pages = {1041--1048},
year = {2009},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/3550-domain-adaptation-with-multiple-sources.pdf}
}

@article{Pan10asurvey,
author = {Pan, Sinno Jialin and Yang, Qiang},
title = {A Survey on Transfer Learning},
year = {2010},
issue_date = {October 2010},
publisher = {IEEE Educational Activities Department},
address = {USA},
volume = {22},
number = {10},
issn = {1041-4347},
url = {https://doi.org/10.1109/TKDE.2009.191},
doi = {10.1109/TKDE.2009.191},
abstract = {A major assumption in many machine learning and data mining algorithms is that the
training and future data must be in the same feature space and have the same distribution.
However, in many real-world applications, this assumption may not hold. For example,
we sometimes have a classification task in one domain of interest, but we only have
sufficient training data in another domain of interest, where the latter data may
be in a different feature space or follow a different data distribution. In such cases,
knowledge transfer, if done successfully, would greatly improve the performance of
learning by avoiding much expensive data-labeling efforts. In recent years, transfer
learning has emerged as a new learning framework to address this problem. This survey
focuses on categorizing and reviewing the current progress on transfer learning for
classification, regression, and clustering problems. In this survey, we discuss the
relationship between transfer learning and other related machine learning techniques
such as domain adaptation, multitask learning and sample selection bias, as well as
covariate shift. We also explore some potential future issues in transfer learning
research.},
journal = {IEEE Trans. on Knowl. and Data Eng.},
month = oct,
pages = {1345–1359},
numpages = {15},
keywords = {Transfer learning, survey, machine learning, data mining., machine learning, data mining., survey, Transfer learning}
}

@InCollection{Tiedemann09news,
  author =	  {J\"org Tiedemann},
  title =	  {News from {OPUS} - {A} Collection of Multilingual
                  Parallel Corpora with Tools and Interfaces},
  booktitle =	  {Recent Advances in Natural Language Processing},
  publisher =	  {John Benjamins, Amsterdam/Philadelphia},
  year =          2009,
  pages =         {237--248},
  editor =        {N. Nicolov and K. Bontcheva and G. Angelova and
                  R. Mitkov},
  volume =	  {V},
  address =	  {Borovets, Bulgaria},
  isbn =          {978 90 272 4825 1},
  pdf =           {http://stp.lingfil.uu.se/~joerg/published/ranlp-V.pdf},
  topic  =        {Parallel corpora}
}

@inproceedings{Bengio09curriculum,
author = {Bengio, Yoshua and Louradour, J\'{e}r\^{o}me and Collobert, Ronan and Weston, Jason},
title = {Curriculum Learning},
year = {2009},
isbn = {9781605585161},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1553374.1553380},
doi = {10.1145/1553374.1553380},
abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them "curriculum learning". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
pages = {41–48},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {ICML '09}
}

@inproceedings{Finkel09hierarchical,
    title = "Hierarchical {B}ayesian Domain Adaptation",
    author = "Finkel, Jenny Rose  and
      Manning, Christopher D.",
    booktitle = "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics",
    month = jun,
    year = "2009",
    address = "Boulder, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N09-1068",
    pages = "602--610",
}

@inproceedings{Mansour09multiple,
author = {Mansour, Yishay and Mohri, Mehryar and Rostamizadeh, Afshin},
title = {Multiple Source Adaptation and the R\'{e}Nyi Divergence},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {This paper presents a novel theoretical study of the general problem of multiple source
adaptation using the notion of R\'{e}nyi divergence. Our results build on our previous
work [12], but significantly broaden the scope of that work in several directions.
We extend previous multiple source loss guarantees based on distribution weighted
combinations to arbitrary target distributions P, not necessarily mixtures of the
source distributions, analyze both known and unknown target distribution cases, and
prove a lower bound. We further extend our bounds to deal with the case where the
learner receives an approximate distribution for each source instead of the exact
one, and show that similar loss guarantees can be achieved depending on the divergence
between the approximate and true distributions. We also analyze the case where the
labeling functions of the source domains are somewhat different. Finally, we report
the results of experiments with both an artificial data set and a sentiment analysis
task, showing the performance benefits of the distribution weighted combinations and
the quality of our bounds based on the R\'{e}nyi divergence.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {367–374},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}

@inproceedings{Yishay09domain,
  author    = {Yishay Mansour and
               Mehryar Mohri and
               Afshin Rostamizadeh},
  title     = {Domain Adaptation: Learning Bounds and Algorithms},
  booktitle = {{COLT} 2009 - The 22nd Conference on Learning Theory, Montreal, Quebec,
               Canada, June 18-21, 2009},
  year      = {2009},
  url       = {http://www.cs.mcgill.ca/\%7Ecolt2009/papers/003.pdf\#page=1},
  timestamp = {Thu, 04 Feb 2021 08:43:06 +0100},
  biburl    = {https://dblp.org/rec/conf/colt/MansourMR09.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Daume09bayes,
author = {Daum\'{e}, Hal},
title = {Bayesian Multitask Learning with Latent Hierarchies},
year = {2009},
isbn = {9780974903958},
publisher = {AUAI Press},
address = {Arlington, Virginia, USA},
abstract = {We learn multiple hypotheses for related tasks under a latent hierarchical relationship
between tasks. We exploit the intuition that for domain adaptation, we wish to share
classifier structure, but for multitask learning, we wish to share covariance structure.
Our hierarchical model is seen to subsume several previously proposed multitask learning
models and performs well on three distinct real-world data sets.},
booktitle = {Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence},
pages = {135–142},
numpages = {8},
location = {Montreal, Quebec, Canada},
series = {UAI '09}
}

@book{Mccandless2010Lucene,
author = {McCandless, Michael and Hatcher, Erik and Gospodnetic, Otis},
title = {Lucene in Action, Second Edition: Covers Apache Lucene 3.0},
year = {2010},
isbn = {1933988177},
publisher = {Manning Publications Co.},
address = {USA},
abstract = {When Lucene first hit the scene five years ago, it was nothing short of amazing. By
using this open-source, highly scalable, super-fast search engine, developers could
integrate search into applications quickly and efficiently. A lot has changed since
then-search has grown from a "nice-to-have" feature into an indispensable part of
most enterprise applications. Lucene now powers search in diverse companies including
Akamai, Netflix, LinkedIn, Technorati, HotJobs, Epiphany, FedEx, Mayo Clinic, MIT,
New Scientist Magazine, and many others. Some things remain the same, though. Lucene
still delivers high-performance search features in a disarmingly easy-to-use API.
Due to its vibrant and diverse open-source community of developers and users, Lucene
is relentlessly improving, with evolutions to APIs, significant new features such
as payloads, and a huge increase (as much as 8x) in indexing speed with Lucene 2.3.
And with clear writing, reusable examples, and unmatched advice on best practices,
Lucene in Action, Second Edition is still the definitive guide to developing with
Lucene.}
}

@article{Ben10A,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
title = {A Theory of Learning from Different Domains},
year = {2010},
issue_date = {May       2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {1–2},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-009-5152-4},
doi = {10.1007/s10994-009-5152-4},
abstract = {Discriminative learning methods for classification perform well when training and
test data are drawn from the same distribution. Often, however, we have plentiful
labeled training data from a source domain but wish to learn a classifier which performs
well on a target domain with a different distribution and little or no labeled training
data. In this work we investigate two questions. First, under what conditions can
a classifier trained from source data be expected to perform well on target data?
Second, given a small amount of labeled target data, how should we combine it during
training with the large amount of labeled source data to achieve the lowest target
error at test time?We address the first question by bounding a classifier's target
error in terms of its source error and the divergence between the two domains. We
give a classifier-induced divergence measure that can be estimated from finite, unlabeled
samples from the domains. Under the assumption that there exists some hypothesis that
performs well in both domains, we show that this quantity together with the empirical
source error characterize the target error of a source-trained classifier.We answer
the second question by bounding the target error of a model which minimizes a convex
combination of the empirical source and target errors. Previous theoretical work has
considered minimizing just the source error, just the target error, or weighting instances
from the two domains equally. We show how to choose the optimal combination of source
and target error as a function of the divergence, the sample sizes of both domains,
and the complexity of the hypothesis class. The resulting bound generalizes the previously
studied cases and is always at least as tight as a bound which considers minimizing
only the target error or an equal weighting of source and target errors.},
journal = {Mach. Learn.},
month = may,
pages = {151–175},
numpages = {25},
keywords = {Learning theory, Sample-selection bias, Transfer learning, Domain adaptation}
}

@inproceedings{Nair10rectified,
author = {Nair, Vinod and Hinton, Geoffrey E.},
title = {Rectified Linear Units Improve Restricted Boltzmann Machines},
year = {2010},
isbn = {9781605589077},
publisher = {Omnipress},
address = {Madison, WI, USA},
abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these "Stepped Sigmoid Units" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
booktitle = {Proceedings of the 27th International Conference on International Conference on Machine Learning},
pages = {807–814},
numpages = {8},
location = {Haifa, Israel},
series = {ICML'10}
}

@inproceedings{Koehn10convergence,
    title = "Convergence of Translation Memory and Statistical Machine Translation",
    author = "Koehn, Philipp  and
      Senellart, Jean",
    booktitle = "Proceedings of the Second Joint EM+/CNGL Workshop: Bringing MT to the User: Research on Integrating MT in the Translation Industry",
    month = nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.jec-1.4",
    pages = "21--32",
    abstract = "We present two methods that merge ideas from statistical machine translation (SMT) and translation memories (TM). We use a TM to retrieve matches for source segments, and replace the mismatched parts with instructions to an SMT system to fill in the gap. We show that for fuzzy matches of over 70{\%}, one method outperforms both SMT and TM baselines.",
}

@inproceedings{Banerjee10combining,
    title = "Combining Multi-Domain Statistical Machine Translation Models using Automatic Classifiers",
    author = "Banerjee, Pratyush  and
      Du, Jinhua  and
      Li, Baoli  and
      Naskar, Sudip  and
      Way, Andy  and
      van Genabith, Josef",
    booktitle = "Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 31-" # nov # " 4",
    year = "2010",
    address = "Denver, Colorado, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2010.amta-papers.16",
    abstract = "This paper presents a set of experiments on Domain Adaptation of Statistical Machine Translation systems. The experiments focus on Chinese-English and two domain-specific corpora. The paper presents a novel approach for combining multiple domain-trained translation models to achieve improved translation quality for both domain-specific as well as combined sets of sentences. We train a statistical classifier to classify sentences according to the appropriate domain and utilize the corresponding domain-specific MT models to translate them. Experimental results show that the method achieves a statistically significant absolute improvement of 1.58 BLEU (2.86{\%} relative improvement) score over a translation model trained on combined data, and considerable improvements over a model using multiple decoding paths of the Moses decoder, for the combined domain test set. Furthermore, even for domain-specific test sets, our approach works almost as well as dedicated domain-specific models and perfect classification.",
}

@inproceedings{Moore10intelligent,
    title = "Intelligent Selection of Language Model Training Data",
    author = "Moore, Robert C.  and
      Lewis, William",
    booktitle = "Proceedings of the {ACL} 2010 Conference Short Papers",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P10-2041",
    pages = "220--224",
}

@inproceedings{Paul10overview,
    title = "Overview of the {IWSLT} 2010 evaluation campaign",
    author = {Paul, Michael  and
      Federico, Marcello  and
      St{\"u}ker, Sebastian},
    booktitle = "Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign",
    month = dec # " 2-3",
    year = "2010",
    address = "Paris, France",
    url = "https://aclanthology.org/2010.iwslt-evaluation.1",
}

@article{Shai10A,
author = {Ben-David, Shai and Blitzer, John and Crammer, Koby and Kulesza, Alex and Pereira, Fernando and Vaughan, Jennifer Wortman},
title = {A Theory of Learning from Different Domains},
year = {2010},
issue_date = {May       2010},
publisher = {Kluwer Academic Publishers},
address = {USA},
volume = {79},
number = {1–2},
issn = {0885-6125},
url = {https://doi.org/10.1007/s10994-009-5152-4},
doi = {10.1007/s10994-009-5152-4},
abstract = {Discriminative learning methods for classification perform well when training and
test data are drawn from the same distribution. Often, however, we have plentiful
labeled training data from a source domain but wish to learn a classifier which performs
well on a target domain with a different distribution and little or no labeled training
data. In this work we investigate two questions. First, under what conditions can
a classifier trained from source data be expected to perform well on target data?
Second, given a small amount of labeled target data, how should we combine it during
training with the large amount of labeled source data to achieve the lowest target
error at test time?We address the first question by bounding a classifier's target
error in terms of its source error and the divergence between the two domains. We
give a classifier-induced divergence measure that can be estimated from finite, unlabeled
samples from the domains. Under the assumption that there exists some hypothesis that
performs well in both domains, we show that this quantity together with the empirical
source error characterize the target error of a source-trained classifier.We answer
the second question by bounding the target error of a model which minimizes a convex
combination of the empirical source and target errors. Previous theoretical work has
considered minimizing just the source error, just the target error, or weighting instances
from the two domains equally. We show how to choose the optimal combination of source
and target error as a function of the divergence, the sample sizes of both domains,
and the complexity of the hypothesis class. The resulting bound generalizes the previously
studied cases and is always at least as tight as a bound which considers minimizing
only the target error or an equal weighting of source and target errors.},
journal = {Mach. Learn.},
month = may,
pages = {151–175},
numpages = {25},
keywords = {Learning theory, Transfer learning, Domain adaptation, Sample-selection bias}
}

@inproceedings{Chang10necessity,
    title = "The Necessity of Combining Adaptation Methods",
    author = "Chang, Ming-Wei  and
      Connor, Michael  and
      Roth, Dan",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D10-1075",
    pages = "767--777",
}

@article{Bottou10large,
author = {Bottou, Léon},
year = {2010},
month = {01},
pages = {},
title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
isbn = {978-3-7908-2603-6},
journal = {Proc. of COMPSTAT},
doi = {10.1007/978-3-7908-2604-3_16}
}

@InProceedings{Glorot10understanding,
  title = 	 {Understanding the difficulty of training deep feedforward neural networks},
  author = 	 {Glorot, Xavier and Bengio, Yoshua},
  booktitle = 	 {Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics},
  pages = 	 {249--256},
  year = 	 {2010},
  editor = 	 {Teh, Yee Whye and Titterington, Mike},
  volume = 	 {9},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Chia Laguna Resort, Sardinia, Italy},
  month = 	 {13--15 May},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf},
  url = 	 {https://proceedings.mlr.press/v9/glorot10a.html},
  abstract = 	 {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future.  We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1.  Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.}
}


@inproceedings{Foster10discriminative,
    title = "Discriminative Instance Weighting for Domain Adaptation in Statistical Machine Translation",
    author = "Foster, George  and
      Goutte, Cyril  and
      Kuhn, Roland",
    booktitle = "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2010",
    address = "Cambridge, MA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D10-1044",
    pages = "451--459",
}

@inproceedings{Daume11domain,
    title = "Domain Adaptation for Machine Translation by Mining Unseen Words",
    author = "Daum{\'e} III, Hal  and
      Jagarlamudi, Jagadeesh",
    booktitle = "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2011",
    address = "Portland, Oregon, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P11-2071",
    pages = "407--412",
}

@inproceedings{Allauzen11bayesian,
author = {Allauzen, Cyril and Riley, Michael},
year = {2011},
month = {01},
pages = {1429-1432},
title = {Bayesian Language Model Interpolation for Mobile Speech Input},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH}
}

@inproceedings{Axelrod11domain,
    title = "Domain Adaptation via Pseudo In-Domain Data Selection",
    author = "Axelrod, Amittai  and
      He, Xiaodong  and
      Gao, Jianfeng",
    booktitle = "Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland, UK.",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D11-1033",
    pages = "355--362",
}

@article{Collobert11natural,
author = {Collobert, Ronan and Weston, Jason and Bottou, L\'{e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
title = {Natural Language Processing (Almost) from Scratch},
year = {2011},
issue_date = {2/1/2011},
publisher = {JMLR.org},
volume = {12},
number = {null},
issn = {1532-4435},
abstract = {We propose a unified neural network architecture and learning algorithm that can be
applied to various natural language processing tasks including part-of-speech tagging,
chunking, named entity recognition, and semantic role labeling. This versatility is
achieved by trying to avoid task-specific engineering and therefore disregarding a
lot of prior knowledge. Instead of exploiting man-made input features carefully optimized
for each task, our system learns internal representations on the basis of vast amounts
of mostly unlabeled training data. This work is then used as a basis for building
a freely available tagging system with good performance and minimal computational
requirements.},
journal = {J. Mach. Learn. Res.},
month = nov,
pages = {2493–2537},
numpages = {45}
}

@inproceedings{Lambert11investigation,
    title = "Investigations on Translation Model Adaptation Using Monolingual Data",
    author = "Lambert, Patrik  and
      Schwenk, Holger  and
      Servan, Christophe  and
      Abdul-Rauf, Sadaf",
    booktitle = "Proceedings of the Sixth Workshop on Statistical Machine Translation",
    month = jul,
    year = "2011",
    address = "Edinburgh, Scotland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W11-2132",
    pages = "284--293",
}

@article{Memoli11gromov,
author = {M\'{e}moli, Facundo},
title = {Gromov–Wasserstein Distances and the Metric Approach to Object Matching},
year = {2011},
issue_date = {August 2011},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {11},
number = {4},
issn = {1615-3375},
abstract = {This paper discusses certain modifications of the ideas concerning the Gromov–Hausdorff
distance which have the goal of modeling and tackling the practical problems of object
matching and comparison. Objects are viewed as metric measure spaces, and based on
ideas from mass transportation, a Gromov–Wasserstein type of distance between objects
is defined. This reformulation yields a distance between objects which is more amenable
to practical computations but retains all the desirable theoretical underpinnings.
The theoretical properties of this new notion of distance are studied, and it is established
that it provides a strict metric on the collection of isomorphism classes of metric
measure spaces. Furthermore, the topology generated by this metric is studied, and
sufficient conditions for the pre-compactness of families of metric measure spaces
are identified. A&nbsp;second goal of this paper is to establish links to several other
practical methods proposed in the literature for comparing/matching shapes in precise
terms. This is done by proving explicit lower bounds for the proposed distance that
involve many of the invariants previously reported by researchers. These lower bounds
can be computed in polynomial time. The numerical implementations of the ideas are
discussed and computational examples are presented.},
journal = {Found. Comput. Math.},
month = aug,
pages = {417–487},
numpages = {71},
keywords = {Data analysis, Shape matching, Mass transport, Metric measure spaces, Gromov–Wasserstein distances}
}

@InProceedings{Mansour12simple,
author= {Mansour, Saab and Ney, Hermann},
title= {A Simple and Effective Weighted Phrase Extraction for Machine Translation Adaptation},
booktitle= {International Workshop on Spoken Language Translation},
year= 2012,
pages= {193-200},
address= {Hong Kong},
month= dec,
booktitlelink= {http://hltc.cs.ust.hk/iwslt/},
pdf = {https://www-i6.informatik.rwth-aachen.de/publications/downloader.php?id=832&row=pdf}
}

@inproceedings{tiedemann12parallel,
    title = "Parallel Data, Tools and Interfaces in {OPUS}",
    author = {Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12)",
    month = may,
    year = "2012",
    address = "Istanbul, Turkey",
    publisher = "European Language Resources Association (ELRA)",
    url = "http://www.lrec-conf.org/proceedings/lrec2012/pdf/463_Paper.pdf",
    pages = "2214--2218",
}

@inproceedings{Cettolo12wit,
        Address = {Trento, Italy},
        Author = {Mauro Cettolo and Christian Girardi and Marcello Federico},
        Booktitle = {Proceedings of the 16$^{th}$ Conference of the European Association for Machine Translation (EAMT)},
        Date = {28-30},
        Month = {May},
        Pages = {261--268},
        Title = {WIT$^3$: Web Inventory of Transcribed and Translated Talks},
        Year = {2012},
}

@inproceedings{Sennrich12perplexity,
    title = "Perplexity Minimization for Translation Model Domain Adaptation in Statistical Machine Translation",
    author = "Sennrich, Rico",
    booktitle = "Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2012",
    address = "Avignon, France",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/E12-1055",
    pages = "539--549",
}

@InProceedings{Tiedemann12parallel,
  author = {J\"org Tiedemann},
  title = {Parallel Data, Tools and Interfaces in {OPUS}},
  booktitle = {Proceedings of the Eight International Conference on Language Resources and Evaluation},
  series = {LREC'12},
  year = {2012},
  month = {may},
  date = {23-25},
  address = {Istanbul, Turkey},
  editor = {Nicoletta Calzolari (Conference Chair) and Khalid Choukri and Thierry Declerck and Mehmet Ugur Dogan and Bente Maegaard and Joseph Mariani and Jan Odijk and Stelios Piperidis},
  publisher = {European Language Resources Association (ELRA)},
  isbn = {978-2-9517408-7-7},
 }
 
@inproceedings{Schwenk12continuous,
    title = "Continuous Space Translation Models for Phrase-Based Statistical Machine Translation",
    author = "Schwenk, Holger",
    booktitle = "Proceedings of {COLING} 2012: Posters",
    month = dec,
    year = "2012",
    address = "Mumbai, India",
    publisher = "The COLING 2012 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C12-2104",
    pages = "1071--1080",
}

@inproceedings{Eidelman12topic,
    title = "Topic Models for Dynamic Translation Model Adaptation",
    author = "Eidelman, Vladimir and Boyd-Graber, Jordan and Resnik, Philip",
    booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P12-2023",
    pages = "115--119",
}

@inproceedings{Le12continuous,
    title = "Continuous Space Translation Models with Neural Networks",
    author = "Le, Hai Son  and
      Allauzen, Alexandre  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2012",
    address = "Montr{\'e}al, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N12-1005",
    pages = "39--48",
}

@inproceedings{Mike12japanese,
title	= {Japanese and Korean Voice Search},
author	= {Mike Schuster and Kaisuke Nakajima},
year	= {2012},
booktitle	= {International Conference on Acoustics, Speech and Signal Processing},
pages	= {5149--5152}
}

@inproceedings{Sennrich12mixture,
    title = "Mixture-Modeling with Unsupervised Clusters for Domain Adaptation in Statistical Machine Translation",
    author = "Sennrich, Rico",
    booktitle = "Proceedings of the 16th Annual conference of the European Association for Machine Translation",
    month = may # " 28{--}30",
    year = "2012",
    address = "Trento, Italy",
    publisher = "European Association for Machine Translation",
    url = "https://aclanthology.org/2012.eamt-1.43",
    pages = "185--192",
}

@inproceedings{Clark12onesystem,
    title = "One System, Many Domains: Open-Domain Statistical Machine Translation via Feature Augmentation",
    author = "Clark, Jonathan  and
      Lavie, Alon  and
      Dyer, Chris",
    booktitle = "Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers",
    month = oct # " 28-" # nov # " 1",
    year = "2012",
    address = "San Diego, California, USA",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2012.amta-papers.4",
    abstract = "In this paper, we introduce a simple technique for incorporating domain information into a statistical machine translation system that significantly improves translation quality when test data comes from multiple domains. Our approach augments (conjoins) standard translation model and language model features with domain indicator features and requires only minimal modifications to the optimization and decoding procedures. We evaluate our method on two language pairs with varying numbers of domains, and observe significant improvements of up to 1.0 BLEU.",
}

@inproceedings{Joshi12multidomain,
    title = "Multi-Domain Learning: When Do Domains Matter?",
    author = "Joshi, Mahesh  and
      Dredze, Mark  and
      Cohen, William W.  and
      Ros{\'e}, Carolyn",
    booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D12-1119",
    pages = "1302--1312",
}

@inproceedings{Pascanu13onthe,
author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
title = {On the Difficulty of Training Recurrent Neural Networks},
year = {2013},
publisher = {JMLR.org},
abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
booktitle = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
pages = {III–1310–III–1318},
location = {Atlanta, GA, USA},
series = {ICML'13}
}

@article{BenTal13robust,
author = {Ben-Tal, Aharon and den Hertog, Dick and De Waegenaere, Anja and Melenberg, Bertrand and Rennen, Gijs},
title = {Robust Solutions of Optimization Problems Affected by Uncertain Probabilities},
year = {2013},
issue_date = {02 2013},
publisher = {INFORMS},
address = {Linthicum, MD, USA},
volume = {59},
number = {2},
issn = {0025-1909},
url = {https://doi.org/10.1287/mnsc.1120.1641},
doi = {10.1287/mnsc.1120.1641},
abstract = {In this paper we focus on robust linear optimization problems with uncertainty regions
defined by φ-divergences for example, chi-squared, Hellinger, Kullback--Leibler. We
show how uncertainty regions based on φ-divergences arise in a natural way as confidence
sets if the uncertain parameters contain elements of a probability vector. Such problems
frequently occur in, for example, optimization problems in inventory control or finance
that involve terms containing moments of random variables, expected utility, etc.
We show that the robust counterpart of a linear optimization problem with φ-divergence
uncertainty is tractable for most of the choices of φ typically considered in the
literature. We extend the results to problems that are nonlinear in the optimization
variables. Several applications, including an asset pricing example and a numerical
multi-item newsvendor example, illustrate the relevance of the proposed approach.
This paper was accepted by G\'{e}rard P. Cachon, optimization. },
journal = {Manage. Sci.},
month = feb,
pages = {341–357},
numpages = {17},
keywords = {goodness-of-fit statistics, φ-divergence, robust optimization}
}

@inproceedings{Cuturi13sinkhorn,
 author = {Cuturi, Marco},
 title = {Sinkhorn Distances: Lightspeed Computation of Optimal Transport},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {2292--2300},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999868},
 acmid = {2999868},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@inproceedings{Mikolov13distributed,
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
title = {Distributed Representations of Words and Phrases and Their Compositionality},
year = {2013},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning
high-quality distributed vector representations that capture a large number of precise
syntactic and semantic word relationships. In this paper we present several extensions
that improve both the quality of the vectors and the training speed. By subsampling
of the frequent words we obtain significant speedup and also learn more regular word
representations. We also describe a simple alternative to the hierarchical softmax
called negative sampling.An inherent limitation of word representations is their indifference
to word order and their inability to represent idiomatic phrases. For example, the
meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated
by this example, we present a simple method for finding phrases in text, and show
that learning good vector representations for millions of phrases is possible.},
booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
pages = {3111–3119},
numpages = {9},
location = {Lake Tahoe, Nevada},
series = {NIPS'13}
}

@inproceedings{Carpuat13domain,
  title={Domain Adaptation in Machine Translation : Final Report},
  author={Marine Carpuat and Hal Daum{\'e} and Alexander Fraser and Chris Quirk and Fabienne Braune and Ann Clifton and A. Irvine and Jagadeesh Jagarlamudi and John Morgan and M. Razmara and A. Tamchyna and Katharine Henry and Rachel Rudinger},
  journal   = {Technical report},
  url = {https://www.cis.uni-muenchen.de/~fraser/pubs/DAMT_2012_final_report.pdf},
  year={2013}
}

@inproceedings{Mikolov13efficient,
  author    = {Tom{\'{a}}s Mikolov and
               Kai Chen and
               Greg Corrado and
               Jeffrey Dean},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Mon, 28 Dec 2020 11:31:01 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Ann13measuring,
  author    = {Ann Irvine and
               John Morgan and
               Marine Carpuat and
               Hal Daum{\'{e}} III and
               Dragos Stefan Munteanu},
  title     = {Measuring Machine Translation Errors in New Domains},
  journal   = {Trans. Assoc. Comput. Linguistics},
  volume    = {1},
  pages     = {429--440},
  year      = {2013},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/111},
  timestamp = {Wed, 17 Feb 2021 21:55:32 +0100},
  biburl    = {https://dblp.org/rec/journals/tacl/IrvineMCDM13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Duh13adaptation,
  author = 	"Duh, Kevin
		and Neubig, Graham
		and Sudoh, Katsuhito
		and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  location = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@InProceedings{Kalchbrenner13recurrent,
  author = 	"Kalchbrenner, Nal
		and Blunsom, Phil",
  title = 	"Recurrent Continuous Translation Models",
  booktitle = 	"Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1700--1709",
  location = 	"Seattle, Washington, USA",
  url = 	"http://aclweb.org/anthology/D13-1176"
}

@article{Irvine13measuring,
author = {Irvine, Ann and Morgan, John and Carpuat, Marine and Daumé, Hal and Munteanu, Dragos},
title = {Measuring Machine Translation Errors in New Domains},
journal = {Transactions of the Association for Computational Linguistics},
volume = {1},
number = {},
pages = {429-440},
year = {2013},
doi = {10.1162/tacl\_a\_00239},
URL = {https://doi.org/10.1162/tacl_a_00239},
abstract = { We develop two techniques for analyzing the effect of porting a machine translation system to a new domain. One is a macro-level analysis that measures how domain shift affects corpus-level evaluation; the second is a micro-level analysis for word-level errors. We apply these methods to understand what happens when a Parliament-trained phrase-based machine translation system is applied in four very different domains: news, medical texts, scientific articles and movie subtitles. We present quantitative and qualitative experiments that highlight opportunities for future research in domain adaptation for machine translation. }
}

@inproceedings{Sennrich13multidomain,
	Address = {Sofia, Bulgaria},
	Author = {Sennrich, Rico and Schwenk, Holger and Aransa, Walid},
	Booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Date-Added = {2019-05-14 22:42:31 +0200},
	Date-Modified = {2019-05-14 22:42:31 +0200},
	Month = aug,
	Pages = {832--840},
	Publisher = {Association for Computational Linguistics},
	Title = {A Multi-Domain Translation Model Framework for Statistical Machine Translation},
	Url = {https://www.aclweb.org/anthology/P13-1082},
	Year = {2013},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/P13-1082}}

@inproceedings{Rico13multi,
  author    = {Rico Sennrich and
               Holger Schwenk and
               Walid Aransa},
  title     = {A Multi-Domain Translation Model Framework for Statistical Machine
               Translation},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2013, 4-9 August 2013, Sofia, Bulgaria, Volume
               1: Long Papers},
  pages     = {832--840},
  year      = {2013},
  crossref  = {DBLP:conf/acl/2013-1},
  url       = {https://www.aclweb.org/anthology/P13-1082/},
  timestamp = {Mon, 19 Aug 2019 18:10:05 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/acl/SennrichSA13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Duh13selection,
  author = 	"Duh, Kevin
		and Neubig, Graham
		and Sudoh, Katsuhito
		and Tsukada, Hajime",
  title = 	"Adaptation Data Selection using Neural Language Models: Experiments in Machine Translation",
  booktitle = 	"Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2013",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"678--683",
  address = 	"Sofia, Bulgaria",
  url = 	"http://aclweb.org/anthology/P13-2119"
}

@article{Ann13measuring,
  author    = {Ann Irvine and
               John Morgan and
               Marine Carpuat and
               Hal Daum{\'{e}} III and
               Dragos Stefan Munteanu},
  title     = {Measuring Machine Translation Errors in New Domains},
  journal   = {{TACL}},
  volume    = {1},
  pages     = {429--440},
  year      = {2013},
  url       = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/111},
  timestamp = {Thu, 28 May 2015 17:23:25 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/tacl/IrvineMCDM13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mikolov13efficient,
  author    = {Tomas Mikolov and Kai Chen and Greg Corrado and Jeffrey Dean},
  editor    = {Yoshua Bengio and Yann LeCun},
  title     = {Efficient Estimation of Word Representations in Vector Space},
  booktitle = {1st International Conference on Learning Representations, {ICLR} 2013,
               Scottsdale, Arizona, USA, May 2-4, 2013, Workshop Track Proceedings},
  year      = {2013},
  url       = {http://arxiv.org/abs/1301.3781},
  timestamp = {Thu, 25 Jul 2019 14:25:36 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1301-3781.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@incollection{mikolov13distributed,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@phdthesis{Rico13domain,
           title = {Domain adaptation for translation models in statistical machine translation},
          school = {University of Zurich},
          author = {Rico Sennrich},
            year = {2013},
        language = {english},
             url = {https://doi.org/10.5167/uzh-88574},
        abstract = {We investigate methods to adapt translation models in SMT to a specific target domain. We discuss two major problems, unknown words because of data sparseness in the (in-domain) training data, and ambiguities arising from out-of-domain parallel texts with different domain-specific translations. We propose novel solutions to both problems.
The main contributions of this thesis are as follows:
* We present a novel translation model architecture that supports domain adaptation at decoding time from a vector of component models. The combination is implemented through instance weighting, and all statistics necessary for the computation of translation probabilities are stored in the models.
* We present an architecture to combine multiple MT systems, using techniques and ideas from domain adaptation. The hypotheses by external MT systems are treated as out-of-domain knowledge, and combined with in-domain data through instance weighting.
* We introduce a sentence alignment algorithm that is able to robustly align even noisy parallel texts. We found that higher-quality sentence alignment of the in-domain parallel text has a significant effect on translation quality in our target domain.
* We propose new translation model features that express how flexible, or general, translation units are, in order to prevent translations that only occur in the context of multiword expressions from being overgeneralised.}
}

@inproceedings{Pennington14glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}

@inproceedings{Cho14properties,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@InProceedings{Wang14neural,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  location = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}
@inproceedings{Hasler14dynamic,
    title = "Dynamic Topic Adaptation for Phrase-based {MT}",
    author = "Hasler, Eva  and
      Blunsom, Phil  and
      Koehn, Philipp  and
      Haddow, Barry",
    booktitle = "Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2014",
    address = "Gothenburg, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E14-1035",
    doi = "10.3115/v1/E14-1035",
    pages = "328--337",
}

@inproceedings{Cuong14latent,
  author    = {Cuong Hoang and
               Khalil Sima'an},
  title     = {Latent Domain Translation Models in Mix-of-Domains Haystack},
  booktitle = {{COLING} 2014, 25th International Conference on Computational Linguistics,
               Proceedings of the Conference: Technical Papers, August 23-29, 2014,
               Dublin, Ireland},
  pages     = {1928--1939},
  year      = {2014},
  crossref  = {DBLP:conf/coling/2014},
  url       = {https://www.aclweb.org/anthology/C14-1182/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/coling/HoangS14},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Carpuat14linear,
    title = "Linear Mixture Models for Robust Machine Translation",
    author = "Carpuat, Marine  and
      Goutte, Cyril  and
      Foster, George",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3363",
    doi = "10.3115/v1/W14-3363",
    pages = "499--509",
}

@InProceedings{Hoang14latent,
  author = 	"Hoang, Cuong
		and Sima'an, Khalil",
  title = 	"Latent Domain Translation Models in Mix-of-Domains Haystack",
  booktitle = 	"Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2014",
  publisher = 	"Dublin City University and Association for Computational Linguistics",
  pages = 	"1928--1939",
  location = 	"Dublin, Ireland",
  url = 	"http://aclweb.org/anthology/C14-1182"
}

@inproceedings{Bahdanau14neural,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural Machine Translation by Jointly Learning to Align and Translate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Sutskever14sequence,
 author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
 title = {Sequence to Sequence Learning with Neural Networks},
 booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'14},
 year = {2014},
 location = {Montreal, Canada},
 pages = {3104--3112},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
 acmid = {2969173},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA}
 } 
 
@book{Nesterov14introductory,
author = {Nesterov, Yurii},
title = {Introductory Lectures on Convex Optimization: A Basic Course},
year = {2014},
isbn = {1461346916},
publisher = {Springer Publishing Company, Incorporated},
edition = {1},
abstract = {It was in the middle of the 1980s, when the seminal paper by Kar markar opened a new epoch in nonlinear optimization. The importance of this paper, containing a new polynomial-time algorithm for linear op timization problems, was not only in its complexity bound. At that time, the most surprising feature of this algorithm was that the theoretical pre diction of its high efficiency was supported by excellent computational results. This unusual fact dramatically changed the style and direc tions of the research in nonlinear optimization. Thereafter it became more and more common that the new methods were provided with a complexity analysis, which was considered a better justification of their efficiency than computational experiments. In a new rapidly develop ing field, which got the name "polynomial-time interior-point methods", such a justification was obligatory. Afteralmost fifteen years of intensive research, the main results of this development started to appear in monographs [12, 14, 16, 17, 18, 19]. Approximately at that time the author was asked to prepare a new course on nonlinear optimization for graduate students. The idea was to create a course which would reflect the new developments in the field. Actually, this was a major challenge. At the time only the theory of interior-point methods for linear optimization was polished enough to be explained to students. The general theory of self-concordant functions had appeared in print only once in the form of research monograph [12].}
}

@InProceedings{Bojar14findings,
  author    = {Bojar, Ondrej  and  Buck, Christian  and  Federmann, Christian  and  Haddow, Barry  and  Koehn, Philipp  and  Leveling, Johannes  and  Monz, Christof  and  Pecina, Pavel  and  Post, Matt  and  Saint-Amand, Herve  and  Soricut, Radu  and  Specia, Lucia  and  Tamchyna, Ale\v{s}},
  title     = {Findings of the 2014 Workshop on Statistical Machine Translation},
  booktitle = {Proceedings of the Ninth Workshop on Statistical Machine Translation},
  month     = {June},
  year      = {2014},
  address   = {Baltimore, Maryland, USA},
  publisher = {Association for Computational Linguistics},
  pages     = {12--58},
  url       = {http://www.aclweb.org/anthology/W/W14/W14-3302}
}

@inproceedings{Diederick14auto,
  author    = {Diederik P. Kingma and Max Welling},
  title     = {Auto-Encoding Variational {Bayes}},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/KingmaW13},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
	
@InProceedings{Wang14neural,
  author = 	"Wang, Rui and Zhao, Hai and Lu, Bao-Liang and Utiyama, Masao and Sumita, Eiichiro",
  title = 	"Neural Network Based Bilingual Language Model Growing for Statistical Machine Translation ",
  booktitle = 	"Proceedings of the 2014 Conference on Empirical Methods in Natural      Language Processing (EMNLP)    ",
  year = 	"2014",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"189--195",
  address = 	"Doha, Qatar",
  doi = 	"10.3115/v1/D14-1023",
  url = 	"http://aclweb.org/anthology/D14-1023"
}

@inproceedings{Chen14systematic,
    title = "A Systematic Comparison of Smoothing Techniques for Sentence-Level {BLEU}",
    author = "Chen, Boxing  and Cherry, Colin",
    booktitle = "Proceedings of the Ninth Workshop on Statistical Machine Translation",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-3346",
    doi = "10.3115/v1/W14-3346",
    pages = "362--367",
}

@article{Srivastava14dropout,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  number  = {56},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}

@inproceedings{Huck15mixed,
    title = "Mixed domain vs. multi-domain statistical machine translation",
    author = "Huck, Matthias  and
      Birch, Alexandra  and
      Haddow, Barry",
    booktitle = "Proceedings of Machine Translation Summit XV: Papers",
    month = oct # " 30 {--} " # nov # " 3",
    year = "2015",
    address = "Miami, USA",
    url = "https://aclanthology.org/2015.mtsummit-papers.19",
}

@inproceedings{kingma13variational,
  author    = {Diederik P. Kingma and
               Max Welling},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {ICLR} 2014,
               Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year      = {2014},
  url       = {http://arxiv.org/abs/1312.6114},
  timestamp = {Thu, 04 Apr 2019 13:20:07 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Luong15stanford,
  Address = {Da Nang, Vietnam},
  Author = {Luong, Minh-Thang  and Manning, Christopher D.},
  Booktitle = {International Workshop on Spoken Language Translation},
  Title = {Stanford Neural Machine Translation Systems for Spoken Language Domain},
  Year = {2015}}

@article{levy15improving,
    title = "Improving Distributional Similarity with Lessons Learned from Word Embeddings",
    author = "Levy, Omer  and
      Goldberg, Yoav  and
      Dagan, Ido",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "3",
    year = "2015",
    url = "https://aclanthology.org/Q15-1016",
    doi = "10.1162/tacl_a_00134",
    pages = "211--225",
    abstract = "Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distributional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter optimizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.",
}

@inproceedings{Ranzato15sequence,
  author    = {Marc'Aurelio Ranzato and
               Sumit Chopra and
               Michael Auli and
               Wojciech Zaremba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Sequence Level Training with Recurrent Neural Networks},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.06732},
  timestamp = {Thu, 25 Jul 2019 14:25:39 +0200}
}

@inproceedings{Kingma15adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Durrani15using,
    title = "Using joint models or domain adaptation in statistical machine translation",
    author = "Durrani, Nadir  and
      Sajjad, Hassan  and
      Joty, Shafiq  and
      Abdelali, Ahmed  and
      Vogel, Stephan",
    booktitle = "Proceedings of Machine Translation Summit XV: Papers",
    month = oct # " 30 {--} " # nov # " 3",
    year = "2015",
    address = "Miami, USA",
    url = "https://aclanthology.org/2015.mtsummit-papers.10",
}

@inproceedings{Huck15mixeddomain,
    title = "Mixed domain vs. multi-domain statistical machine translation",
    author = "Huck, Matthias  and
      Birch, Alexandra  and
      Haddow, Barry",
    booktitle = "Proceedings of Machine Translation Summit XV: Papers",
    month = oct # " 30 {--} " # nov # " 3",
    year = "2015",
    address = "Miami, USA",
    url = "https://aclanthology.org/2015.mtsummit-papers.19",
}

@article{Wang15character,
  author    = {Wang Ling and
               Isabel Trancoso and
               Chris Dyer and
               Alan W. Black},
  title     = {Character-based Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1511.04586},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.04586},
  archivePrefix = {arXiv},
  eprint    = {1511.04586},
  timestamp = {Mon, 13 Aug 2018 16:48:11 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/LingTDB15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Hinton15Distilling,
title	= {Distilling the Knowledge in a Neural Network},
author	= {Geoffrey Hinton and Oriol Vinyals and Jeffrey Dean},
year	= {2015},
URL	= {http://arxiv.org/abs/1503.02531},
booktitle	= {NIPS Deep Learning and Representation Learning Workshop}
}

@inproceedings{Wees15whats,
    title = "What{'}s in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Weerkamp, Wouter  and
      Monz, Christof",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2092",
    doi = "10.3115/v1/P15-2092",
    pages = "560--566",
}

@inproceedings{Ioffe15batch,
author = {Ioffe, Sergey and Szegedy, Christian},
title = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
year = {2015},
publisher = {JMLR.org},
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
pages = {448–456},
numpages = {9},
location = {Lille, France},
series = {ICML'15}
}

@inproceedings{jean15using,
    title = "On Using Very Large Target Vocabulary for Neural Machine Translation",
    author = "Jean, S{\'e}bastien  and
      Cho, Kyunghyun  and
      Memisevic, Roland  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1001",
    doi = "10.3115/v1/P15-1001",
    pages = "1--10",
}

@inproceedings{luong15addressing,
    title = "Addressing the Rare Word Problem in Neural Machine Translation",
    author = "Luong, Thang  and
      Sutskever, Ilya  and
      Le, Quoc  and
      Vinyals, Oriol  and
      Zaremba, Wojciech",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P15-1002",
    doi = "10.3115/v1/P15-1002",
    pages = "11--19",
}

@inproceedings{Yang15unified,
	title = {A unified perspective on multi-domain and multi-task learning},
	author = {Yongxin Yang and Timothy M. Hospedales},
	booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	year = {2015},
	url = {https://arxiv.org/abs/1412.7489},
}

@inproceedings{Bahdanau15learning,
	Author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the International Conference on Learning Representations},
	series = {ICLR},
	address = {San Diego, CA},
	Title = {Neural machine translation by jointly learning to align and translate},
	Year = {2015},
	url = {https://arxiv.org/pdf/1409.0473.pdf},
}

@inproceedings{Sennrich16neural,
	Address = {Berlin, Germany},
	Author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	Booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	Doi = {10.18653/v1/P16-1162},
	Month = aug,
	Pages = {1715--1725},
	Title = {Neural Machine Translation of Rare Words with Subword Units},
	Url = {https://www.aclweb.org/anthology/P16-1162},
	Year = {2016},
}

@inproceedings{zhang16variational,
    title = "Variational Neural Machine Translation",
    author = "Zhang, Biao  and
      Xiong, Deyi  and
      Su, Jinsong  and
      Duan, Hong  and
      Zhang, Min",
    booktitle = "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2016",
    address = "Austin, Texas",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D16-1050",
    doi = "10.18653/v1/D16-1050",
    pages = "521--530",
}

@inproceedings{Li16towards,
author = {Li, Xiaoqing and Zhang, Jiajun and Zong, Chengqing},
title = {Towards Zero Unknown Word in Neural Machine Translation},
year = {2016},
isbn = {9781577357704},
publisher = {AAAI Press},
abstract = {Neural Machine translation has shown promising results in recent years. In order to
control the computational complexity, NMT has to employ a small vocabulary, and massive
rare words outside the vocabulary are all replaced with a single  unk  symbol. Besides
the inability to translate rare words, this kind of simple approach leads to much
increased ambiguity of the sentences since meaningless  unk s break the structure
of sentences, and thus hurts the translation and reordering of the in-vocabulary words.
To tackle this problem, we propose a novel substitution-translation-restoration method.
In substitution step, the rare words in a testing sentence are replaced with similar
in-vocabulary words based on a similarity model learnt from monolingual data. In translation
and restoration steps, the sentence will be translated with a model trained on new
bilingual data with rare words replaced, and finally the translations of the replaced
words will be substituted by that of original ones. Experiments on Chinese-to-English
translation demonstrate that our proposed method can achieve more than 4 BLEU points
over the attention-based NMT. When compared to the recently proposed method handling
rare words in NMT, our method can also obtain an improvement by nearly 3 BLEU points.},
booktitle = {Proceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence},
pages = {2852–2858},
numpages = {7},
location = {New York, New York, USA},
series = {IJCAI'16}
}

@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico and Haddow, Barry and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian and Li, Liangyou and Way, Andy and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@inproceedings{Chen16guided,
  title={Guided Alignment Training for Topic-Aware Neural Machine Translation},
  author={Wenhu Chen and Evgeny Matusov and Shahram Khadivi and Jan-Thorsten Peter},
  address = {Austin, Texas},
  year={2016},
  Booktitle = {Proceedings of the Twelth Biennial Conference of the Association for Machine Translation in the Americas},
  Series = {AMTA 2012}
}

@article{He16deep,
  title={Deep Residual Learning for Image Recognition},
  author={Kaiming He and X. Zhang and Shaoqing Ren and Jian Sun},
  journal={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2016},
  pages={770-778}
}

@article{Jimmy16layer,
  title={Layer Normalization},
  author={Jimmy Ba and J. Kiros and Geoffrey E. Hinton},
  journal={ArXiv},
  year={2016},
  volume={abs/1607.06450}
}

@inproceedings{Xiao16learning,
  author    = {Tong Xiao and
               Hongsheng Li and
               Wanli Ouyang and
               Xiaogang Wang},
  title     = {Learning Deep Feature Representations with Domain Guided Dropout for
               Person Re-identification},
  booktitle = {2016 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2016, Las Vegas, NV, USA, June 27-30, 2016},
  pages     = {1249--1258},
  publisher = {{IEEE} Computer Society},
  year      = {2016},
  url       = {https://doi.org/10.1109/CVPR.2016.140},
  doi       = {10.1109/CVPR.2016.140},
  timestamp = {Fri, 26 Feb 2021 08:54:57 +0100},
  biburl    = {https://dblp.org/rec/conf/cvpr/XiaoLOW16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@inproceedings{Bousmalis16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@inproceedings{Costa16character,
    title = "Character-based Neural Machine Translation",
    author = "Costa-juss{\`a}, Marta R.  and
      Fonollosa, Jos{\'e} A. R.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2058",
    doi = "10.18653/v1/P16-2058",
    pages = "357--361",
}

@inproceedings{Luong16achieving,
    title = "Achieving Open Vocabulary Neural Machine Translation with Hybrid Word-Character Models",
    author = "Luong, Minh-Thang  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1100",
    doi = "10.18653/v1/P16-1100",
    pages = "1054--1063",
}

@inproceedings{Liu16stein,
 author = {Liu, Qiang and Wang, Dilin},
 title = {Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {2378--2386},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157362},
 acmid = {3157362},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@inproceedings{Brebisson16anexploration,
  author={Alexandre de Brébisson and Pascal Vincent},
  title={An Exploration of Softmax Alternatives Belonging to the Spherical Loss Family},
  year={2016},
  url={http://arxiv.org/abs/1511.05042},
  booktitle={Proceedings of the 4th International Conference on Learning Representation},
  series = {ICLR},
  editor   = {Yoshua Bengio and Yann LeCun},
  address  = {San Juan, Puerto Rico},
  year      = {2016},
}

@InProceedings{Sennrich16controlling,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  location = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Sennrich16improving,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Improving Neural Machine Translation Models with Monolingual Data",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"86--96",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1009",
  url = 	"http://aclweb.org/anthology/P16-1009"
}

@InProceedings{Sennrich16neural,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Neural Machine Translation of Rare Words with Subword Units",
  booktitle = 	"Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1715--1725",
  location = 	"Berlin, Germany",
  doi = 	"10.18653/v1/P16-1162",
  url = 	"http://aclweb.org/anthology/P16-1162"
}

@InProceedings{Liu16deep,
  author = 	"Liu, Pengfei
		and Qiu, Xipeng
		and Huang, Xuanjing",
  title = 	"Deep Multi-Task Learning with Shared Memory for Text Classification",
  booktitle = 	"Proceedings of the 2016 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--127",
  location = 	"Austin, Texas",
  doi = 	"10.18653/v1/D16-1012",
  url = 	"http://aclweb.org/anthology/D16-1012"
}

@INPROCEEDINGS{Szegedy16rethinking,
author = {C. Szegedy and V. Vanhoucke and S. Ioffe and J. Shlens and Z. Wojna},
booktitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
title = {Rethinking the Inception Architecture for Computer Vision},
year = {2016},
volume = {},
issn = {1063-6919},
pages = {2818-2826},
keywords = {convolution;computer architecture;training;computational efficiency;computer vision;benchmark testing;computational modeling},
doi = {10.1109/CVPR.2016.308},
url = {https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.308},
publisher = {IEEE Computer Society},
address = {Los Alamitos, CA, USA},
month = {jun}
}


@ARTICLE{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@InProceedings{Zhang16topic,
  author = 	"Zhang, Jian
		and Li, Liangyou
		and Way, Andy
		and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  location = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@inproceedings{Ha16towards,
	Address = {Vancouver, Canada},
	Author = {Ha, Thanh-He and Niehues, Jan and Waibel, Alex},
	Booktitle = {Proceedings of the International Workshop on Spoken Language Translation},
	Date-Added = {2019-05-19 16:26:43 +0200},
	Date-Modified = {2019-05-19 16:28:13 +0200},
	Organization = {IWSLT},
	Title = {Toward Multilingual Neural Machine Translationwith Universal Encoder and Decoder},
	Year = {2016}}

@inproceedings{Firat16multiway,
	Author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
	Booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
	Doi = {10.18653/v1/N16-1101},
	Location = {San Diego, California},
	Pages = {866--875},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Way, Multilingual Neural Machine Translation with a Shared Attention Mechanism},
	Url = {http://www.aclweb.org/anthology/N16-1101},
	Year = {2016},
}
@InProceedings{Sennrich16politeness,
  author = 	"Sennrich, Rico
		and Haddow, Barry
		and Birch, Alexandra",
  title = 	"Controlling Politeness in Neural Machine Translation via Side Constraints",
  booktitle = 	"Proceedings of the 2016 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies    ",
  year = 	"2016",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"35--40",
  address = 	"San Diego, California",
  doi = 	"10.18653/v1/N16-1005",
  url = 	"http://aclweb.org/anthology/N16-1005"
}

@InProceedings{Wang16connecting,
  author = 	"Wang, Rui
		and Zhao, Hai
		and Lu, Bao-Liang
		and Utiyama, Masao
		and Sumita, Eiichiro",
  title = 	"Connecting Phrase based Statistical Machine Translation Adaptation",
  booktitle = 	"Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"3135--3145",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1295"
}

@inproceedings{rebecca16neural,
author = "Rebecca Knowles and Philipp Koehn",
title = "Neural Interactive Translation Prediction",
year = 2016,
booktitle = "Proceedings of the Conference of the Association for Machine Translation in the Americas (AMTA)"
}

@article{Cuong16Adapting,
    author = {Cuong, Hoang and Sima’an, Khalil and Titov, Ivan},
    title = "{Adapting to All Domains at Once: Rewarding Domain Invariance in
                    SMT}",
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {4},
    pages = {99-112},
    year = {2016},
    month = {04},
    abstract = "{Existing work on domain adaptation for statistical machine translation has
                    consistently assumed access to a small sample from the test distribution (target
                    domain) at training time. In practice, however, the target domain may not be
                    known at training time or it may change to match user needs. In such situations,
                    it is natural to push the system to make safer choices, giving higher preference
                    to domain-invariant translations, which work well across domains, over risky
                    domain-specific alternatives. We encode this intuition by (1) inducing latent
                    subdomains from the training data only; (2) introducing features which measure
                    how specialized phrases are to individual induced sub-domains; (3) estimating
                    feature weights on out-of-domain data (rather than on the target domain). We
                    conduct experiments on three language pairs and a number of different domains.
                    We observe consistent improvements over a baseline which does not explicitly
                    reward domain invariance.}",
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00086},
    url = {https://doi.org/10.1162/tacl\_a\_00086},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00086/1567354/tacl\_a\_00086.pdf},
}

@ARTICLE{Gulcehre16monolingual,
    author = {G{\"{u}}l{\c c}ehre, {\c C}ağlar and Firat, Orhan and Xu, Kelvin and Cho, Kyunghyun and Barrault, Lo{\"{\i}}c and Lin, Huei-Chi and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  keywords = {Computer Science - Computation and Language},
     month = mar,
     title = {On Using Monolingual Corpora in Neural Machine Translation},
   journal = {arXiv e-prints},
    volume = {abs/1503.03535},
      year = {2015},
       url = {https://arxiv.org/abs/1503.03535}
}

@inproceedings{Sennrich16improving,
    title = "Improving Neural Machine Translation Models with Monolingual Data",
    author = "Sennrich, Rico  and
      Haddow, Barry  and
      Birch, Alexandra",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P16-1009",
    doi = "10.18653/v1/P16-1009",
    pages = "86--96",
}

@article{Papernot16distillation,
  title={Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks},
  author={Nicolas Papernot and P. Mcdaniel and Xi Wu and S. Jha and A. Swami},
  journal={2016 IEEE Symposium on Security and Privacy (SP)},
  year={2016},
  pages={582-597}
}

@InProceedings{Zhang16topicinformed,
  author = 	"Zhang, Jian
		and Li, Liangyou
		and Way, Andy
		and Liu, Qun",
  title = 	"Topic-Informed Neural Machine Translation",
  booktitle = 	"Proceedings of the 26th International Conference on Computational Linguistics: Technical Papers",
  series =      "COLING 2016",
  year = 	"2016",
  publisher = 	"The COLING 2016 Organizing Committee",
  pages = 	"1807--1817",
  address = 	"Osaka, Japan",
  url = 	"http://aclweb.org/anthology/C16-1170"
}

@article{Freitag16fast,
  title={Fast Domain Adaptation for Neural Machine Translation},
  author={Markus Freitag and Yaser Al-Onaizan},
  journal={CoRR},
  year={2016},
  volume={abs/1612.06897}
}
@ARTICLE{Kirk16overcoming,
       author = {{Kirkpatrick}, James and {Pascanu}, Razvan and {Rabinowitz}, Neil and
         {Veness}, Joel and {Desjardins}, Guillaume and {Rusu}, Andrei A. and
         {Milan}, Kieran and {Quan}, John and {Ramalho}, Tiago and
         {Grabska-Barwinska}, Agnieszka and {Hassabis}, Demis and
         {Clopath}, Claudia and {Kumaran}, Dharshan and {Hadsell}, Raia},
        title = "{Overcoming catastrophic forgetting in neural networks}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Statistics - Machine Learning},
         year = "2016",
        month = "Dec",
          eid = {arXiv:1612.00796},
        pages = {arXiv:1612.00796},
archivePrefix = {arXiv},
       eprint = {1612.00796},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv161200796K},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Niehues16pretranslation,
    title = "Pre-Translation for Neural Machine Translation",
    author = "Niehues, Jan  and
      Cho, Eunah  and
      Ha, Thanh-Le  and
      Waibel, Alex",
    booktitle = "Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",
    month = dec,
    year = "2016",
    address = "Osaka, Japan",
    publisher = "The COLING 2016 Organizing Committee",
    url = "https://www.aclweb.org/anthology/C16-1172",
    pages = "1828--1836",
}

@inproceedings{Chung16character,
    title = "A Character-level Decoder without Explicit Segmentation for Neural Machine Translation",
    author = "Chung, Junyoung  and
      Cho, Kyunghyun  and
      Bengio, Yoshua",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1160",
    doi = "10.18653/v1/P16-1160",
    pages = "1693--1703",
}

@inproceedings{Konstantinos16domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 address = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA}
} 

@article{Li16mutual,
  title={Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  author={Jiwei Li and Dan Jurafsky},
  journal={ArXiv},
  year={2016},
  volume={abs/1601.00372}
}

@inproceedings{Bousmalis16Domain,
 author = {Bousmalis, Konstantinos and Trigeorgis, George and Silberman, Nathan and Krishnan, Dilip and Erhan, Dumitru},
 title = {Domain Separation Networks},
 booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
 series = {NIPS'16},
 year = {2016},
 isbn = {978-1-5108-3881-9},
 location = {Barcelona, Spain},
 pages = {343--351},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=3157096.3157135},
 acmid = {3157135},
 publisher = {Curran Associates Inc.},
 address = {USA},
}

@article{Servan16Domain,
  author    = {Christophe Servan and
               Josep Maria Crego and
               Jean Senellart},
  title     = {Domain specialization: a post-training domain adaptation for Neural
               Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1612.06141},
  year      = {2016},
  url       = {http://arxiv.org/abs/1612.06141},
  eprinttype = {arXiv},
  eprint    = {1612.06141},
  timestamp = {Mon, 13 Aug 2018 16:46:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/ServanCS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Chen16Bilingual,
  title={Bilingual Methods for Adaptive Training Data Selection for Machine Translation},
  author={Boxing Chen and Roland Kuhn and George F. Foster and Colin Cherry and Fei Huang},
  url = {https://amtaweb.org/wp-content/uploads/2016/10/AMTA2016_Research_Proceedings_v7.pdf#page=99},
  year={2016}
}

@article{Kenji17multi,
  title={Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation},
  author={Kenji Imamura and Eiichiro Sumita},
  journal={Journal of Natural Language Processing},
  volume={24},
  number={4},
  pages={597-618},
  year={2017},
  doi={10.5715/jnlp.24.597}
}

@inproceedings{Dakwle17fine,
  author    = {Dakwale, Praveen and Monz, Christof},
  title     = {Fine-Tuning for Neural Machine Translation with Limited Degradation across In- and Out-of-Domain Data},
  booktitle = {Proceedings of the 16th Machine Translation Summit (MT-Summit 2017)},
  pages = {156--169},
  year      = {2017}
}

@inproceedings{Tu17neural,
author = {Tu, Zhaopeng and Liu, Yang and Shang, Lifeng and Liu, Xiaohua and Li, Hang},
title = {Neural Machine Translation with Reconstruction},
year = {2017},
publisher = {AAAI Press},
abstract = {Although end-to-end Neural Machine Translation (NMT) has achieved remarkable progress
in the past two years, it suffers from a major drawback: translations generated by
NMT systems often lack of adequacy. It has been widely observed that NMT tends to
repeatedly translate some source words while mistakenly ignoring other words. To alleviate
this problem, we propose a novel encoder-decoder-reconstructor framework for NMT.
The reconstructor, incorporated into the NMT model, manages to reconstruct the input
source sentence from the hidden layer of the output target sentence, to ensure that
the information in the source side is transformed to the target side as much as possible.
Experiments show that the proposed framework significantly improves the adequacy of
NMT output and achieves superior translation result over state-of-the-art NMT and
statistical MT systems.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3097–3103},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17}
}

@incollection{Vaswani17attention,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, {\L}ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}

@inproceedings{Sajjad17neural,
  title={Neural Machine Translation Training in a Multi-Domain Scenario},
  author={Hassan Sajjad and Nadir Durrani and Fahim Dalvi and Yonatan Belinkov and Stephan Vogel},
  booktitle={Proceedings of the 14th International Workshop on Spoken Language Translation},
  series = {IWSLT 2017},
  address = {Tokyo, Japan},
  year={2017},
  url ={http://arxiv.org/abs/1708.08712}
}

@inproceedings{Costa17byte,
    title = "Byte-based Neural Machine Translation",
    author = "Costa-juss{\`a}, Marta R.  and
      Escolano, Carlos  and
      Fonollosa, Jos{\'e} A. R.",
    booktitle = "Proceedings of the First Workshop on Subword and Character Level Models in {NLP}",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4123",
    doi = "10.18653/v1/W17-4123",
    pages = "154--158",
    abstract = "This paper presents experiments comparing character-based and byte-based neural machine translation systems. The main motivation of the byte-based neural machine translation system is to build multi-lingual neural machine translation systems that can share the same vocabulary. We compare the performance of both systems in several language pairs and we see that the performance in test is similar for most language pairs while the training time is slightly reduced in the case of byte-based neural machine translation.",
}

@InProceedings{Kobus17domain,
  author = 	"Kobus, Catherine
		and Crego, Josep
		and Senellart, Jean",
  title = 	"Domain Control for Neural Machine Translation",
  booktitle = 	"Proceedings of the International Conference Recent Advances in Natural Language Processing, RANLP 2017",
  year = 	"2017",
  publisher = 	"INCOMA Ltd.",
  pages = 	"372--378",
  location = 	"Varna, Bulgaria",
  doi = 	"10.26615/978-954-452-049-6_049",
  url = 	"https://doi.org/10.26615/978-954-452-049-6_049"
}

@incollection{Mccann17learn,
title = {Learned in Translation: Contextualized Word Vectors},
author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {6294--6305},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7209-learned-in-translation-contextualized-word-vectors.pdf}
}

@article{Lee17fully,
    title = "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Hofmann, Thomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1026",
    doi = "10.1162/tacl_a_00067",
    pages = "365--378",
    abstract = "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT{'}15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
}

@article{Imamura17Multi,
author = {Imamura, Kenji and Sumita, Eiichiro},
year = {2017},
month = {09},
pages = {597-618},
title = {Multi-domain Adaptation for Statistical Machine Translation Based on Feature Augmentation},
volume = {24},
journal = {Journal of Natural Language Processing},
doi = {10.5715/jnlp.24.597}
}

@inproceedings{koehn17six,
    title = "Six Challenges for Neural Machine Translation",
    author = "Koehn, Philipp  and
      Knowles, Rebecca",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-3204",
    doi = "10.18653/v1/W17-3204",
    pages = "28--39",
    abstract = "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",
}

@inproceedings{jang17categorical,
  author    = {Eric Jang and
               Shixiang Gu and
               Ben Poole},
  title     = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {ICLR} 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2017},
  url       = {https://openreview.net/forum?id=rkE3y85ee},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Chen17cost,
  author = 	"Chen, Boxing
		and Cherry, Colin
		and Foster, George
		and Larkin, Samuel",
  title = 	"Cost Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the First Workshop on Neural Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"40--46",
  location = 	"Vancouver",
  doi = 	"10.18653/v1/W17-3205",
  url = 	"http://aclweb.org/anthology/W17-3205"
}

@InProceedings{Miceli17regularization,
  author = 	"Miceli Barone, Antonio Valerio
		and Haddow, Barry
		and Germann, Ulrich
		and Sennrich, Rico",
  title = 	"Regularization techniques for fine-tuning in neural machine translation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1489--1494",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1156",
  url = 	"http://aclweb.org/anthology/D17-1156"
}

@InProceedings{Britz17effective,
  author = 	"Britz, Denny
		and Le, Quoc
		and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/W17-4712",
  url = 	"http://aclweb.org/anthology/W17-4712"
}

@InProceedings{Chu17empirical,
  author = 	"Chu, Chenhui
		and Dabre, Raj
		and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine      Translation    ",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"385--391",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-2061",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@article{johnson17googles,
    title = "{G}oogle{'}s Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation",
    author = "Johnson, Melvin  and
      Schuster, Mike  and
      Le, Quoc V.  and
      Krikun, Maxim  and
      Wu, Yonghui  and
      Chen, Zhifeng  and
      Thorat, Nikhil  and
      Vi{\'e}gas, Fernanda  and
      Wattenberg, Martin  and
      Corrado, Greg  and
      Hughes, Macduff  and
      Dean, Jeffrey",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://aclanthology.org/Q17-1024",
    doi = "10.1162/tacl_a_00065",
    pages = "339--351",
    abstract = "We propose a simple solution to use a single Neural Machine Translation (NMT) model to translate between multiple languages. Our solution requires no changes to the model architecture from a standard NMT system but instead introduces an artificial token at the beginning of the input sentence to specify the required target language. Using a shared wordpiece vocabulary, our approach enables Multilingual NMT systems using a single model. On the WMT{'}14 benchmarks, a single multilingual model achieves comparable performance for English→French and surpasses state-of-theart results for English→German. Similarly, a single multilingual model surpasses state-of-the-art results for French→English and German→English on WMT{'}14 and WMT{'}15 benchmarks, respectively. On production corpora, multilingual models of up to twelve language pairs allow for better translation of many individual pairs. Our models can also learn to perform implicit bridging between language pairs never seen explicitly during training, showing that transfer learning and zero-shot translation is possible for neural translation. Finally, we show analyses that hints at a universal interlingua representation in our models and also show some interesting examples when mixing languages.",
}

@InProceedings{Wang17instance,
  author = 	"Wang, Rui
		and Utiyama, Masao
		and Liu, Lemao
		and Chen, Kehai
		and Sumita, Eiichiro",
  title = 	"Instance Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the 2017 Conference on Empirical Methods in Natural      Language Processing    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1482--1488",
  location = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/D17-1155",
  url = 	"http://aclweb.org/anthology/D17-1155"
}
@inproceedings{Matteo17neural,
  author    = {Matteo Negri and
               Marco Turchi and
               Marcello Federico and
               Nicola Bertoldi and
               M. Amin Farajian},
  title     = {Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario},
  booktitle = {Proceedings of the 15th Conference of the European Chapter of the
               Association for Computational Linguistics, {EACL} 2017, Valencia,
               Spain, April 3-7, 2017, Volume 2: Short Papers},
  pages     = {280--284},
  year      = {2017},
  crossref  = {DBLP:conf/eacl/2017-2},
  url       = {https://www.aclweb.org/anthology/E17-2045/},
  timestamp = {Tue, 17 Sep 2019 13:40:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/eacl/NegriTFBF17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Zhang17context,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA},
}

@article{Hassan17multi,
  author    = {Hassan Sajjad and
               Nadir Durrani and
               Fahim Dalvi and
               Yonatan Belinkov and
               Stephan Vogel},
  title     = {Neural Machine Translation Training in a Multi-Domain Scenario},
  journal   = {CoRR},
  volume    = {abs/1708.08712},
  year      = {2017},
  url       = {http://arxiv.org/abs/1708.08712},
  archivePrefix = {arXiv},
  eprint    = {1708.08712},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1708-08712},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Klein17opennmt,
  author = 	"Klein, Guillaume
		and Kim, Yoon
		and Deng, Yuntian
		and Senellart, Jean
		and Rush, Alexander",
  title = 	"OpenNMT: Open-Source Toolkit for Neural Machine Translation",
  booktitle = 	"Proceedings of ACL 2017, System Demonstrations",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"67--72",
  location = 	"Vancouver, Canada",
  url = 	"http://aclweb.org/anthology/P17-4012"
}

@incollection{Rebuffi17learning,
 title = {Learning multiple visual domains with residual adapters},
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 booktitle = {Advances in Neural Information Processing Systems 30},
 editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {506--516},
 year = {2017},
 publisher = {Curran Associates, Inc.},
 url = {http://papers.nips.cc/paper/6654-learning-multiple-visual-domains-with-  residual-adapters.pdf}
}

@InProceedings{Peng17multitask,
  author = 	"Peng, Nanyun
		and Dredze, Mark",
  title = 	"Multi-task Domain Adaptation for Sequence Tagging",
  booktitle = 	"Proceedings of the 2nd Workshop on Representation Learning for NLP",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"91--100",
  location = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/W17-2612",
  url = 	"http://aclweb.org/anthology/W17-2612"
}

@InProceedings{Chelsea17modelagnostic,
  title = 	 {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  author = 	 {Chelsea Finn and Pieter Abbeel and Sergey Levine},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1126--1135},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/finn17a/finn17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/finn17a.html},
  abstract = 	 {We propose an algorithm for meta-learning that is model-agnostic, in the sense that it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning. The goal of meta-learning is to train a model on a variety of learning tasks, such that it can solve new learning tasks using only a small number of training samples. In our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on that task. In effect, our method trains the model to be easy to fine-tune. We demonstrate that this approach leads to state-of-the-art performance on two few-shot image classification benchmarks, produces good results on few-shot regression, and accelerates fine-tuning for policy gradient reinforcement learning with neural network policies.}
}

@InProceedings{Ghering17convolutional,
  title = 	 {Convolutional Sequence to Sequence Learning},
  author = 	 {Jonas Gehring and Michael Auli and David Grangier and Denis Yarats and Yann N. Dauphin},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1243--1252},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  url = 	 {http://proceedings.mlr.press/v70/gehring17a.html},
  abstract = 	 {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training to better exploit the GPU hardware and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT’14 English-German and WMT’14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.}
}

@article{Jiatao17non,
  author    = {Jiatao Gu and
               James Bradbury and
               Caiming Xiong and
               Victor O. K. Li and
               Richard Socher},
  title     = {Non-Autoregressive Neural Machine Translation},
  journal   = {CoRR},
  volume    = {abs/1711.02281},
  year      = {2017},
  url       = {http://arxiv.org/abs/1711.02281},
  archivePrefix = {arXiv},
  eprint    = {1711.02281},
  timestamp = {Thu, 21 Mar 2019 11:19:44 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1711-02281.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Johnson17google,
	Author = {Johnson, Melvin and Schuster, Mike and Le, Quoc and Krikun, Maxim and Wu, Yonghui and Chen, Zhifeng and Thorat, Nikhil and Vi{\'e}gas, Fernand a and Wattenberg, Martin and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
	Issn = {2307-387X},
	Journal = {Transactions of the Association for Computational Linguistics},
	Pages = {339--351},
	Title = {Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation},
	Url = {https://transacl.org/ojs/index.php/tacl/article/view/1081},
	Volume = {5},
	Year = {2017},
	Bdsk-Url-1 = {https://transacl.org/ojs/index.php/tacl/article/view/1081}}

@inproceedings{tiedemann17neural,
    title = "Neural Machine Translation with Extended Context",
    author = {Tiedemann, J{\"o}rg  and
      Scherrer, Yves},
    booktitle = "Proceedings of the Third Workshop on Discourse in Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4811",
    doi = "10.18653/v1/W17-4811",
    pages = "82--92",
    abstract = "We investigate the use of extended context in attention-based neural machine translation. We base our experiments on translated movie subtitles and discuss the effect of increasing the segments beyond single translation units. We study the use of extended source language context as well as bilingual context extensions. The models learn to distinguish between information from different segments and are surprisingly robust with respect to translation quality. In this pilot study, we observe interesting cross-sentential attention patterns that improve textual coherence in translation at least in some selected cases.",
}

@inproceedings{Farajian17neural,
    title = "Neural vs. Phrase-Based Machine Translation in a Multi-Domain Scenario",
    author = "Farajian, M. Amin  and Turchi, Marco and Negri, Matteo and Bertoldi, Nicola and Federico, Marcello",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2045",
    pages = "280--284",
    abstract = "State-of-the-art neural machine translation (NMT) systems are generally trained on specific domains by carefully selecting the training sets and applying proper domain adaptation techniques. In this paper we consider the real world scenario in which the target domain is not predefined, hence the system should be able to translate text from multiple domains. We compare the performance of a generic NMT system and phrase-based statistical machine translation (PBMT) system by training them on a generic parallel corpus composed of data from different domains. Our results on multi-domain English-French data show that, in these realistic conditions, PBMT outperforms its neural counterpart. This raises the question: is NMT ready for deployment as a generic/multi-purpose MT backbone in real-world settings?",
}

@inproceedings{Farajian17multidomain,
	Address = {Copenhagen, Denmark},
	Author = {Farajian, M. Amin and Turchi, Marco and Negri, Matteo and Federico, Marcello},
	Booktitle = {Proceedings of the Second Conference on Machine Translation},
	Doi = {10.18653/v1/W17-4713},
	Month = sep,
	Pages = {127--137},
	Title = {Multi-Domain Neural Machine Translation through Unsupervised Adaptation},
	Url = {https://www.aclweb.org/anthology/W17-4713},
	Year = {2017},
	Bdsk-Url-1 = {https://www.aclweb.org/anthology/W17-4713},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/W17-4713}}

@inproceedings{Wang17sentence,
	Address = {Vancouver, Canada},
	Author = {Wang, Rui and Finch, Andrew and Utiyama, Masao and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	Date-Added = {2019-04-28 19:22:53 +0200},
	Date-Modified = {2019-04-28 19:22:53 +0200},
	Doi = {10.18653/v1/P17-2089},
	Pages = {560--566},
	Publisher = {Association for Computational Linguistics},
	Title = {Sentence Embedding for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/P17-2089},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/P17-2089},
	Bdsk-Url-2 = {https://doi.org/10.18653/v1/P17-2089}}

@article{Biao17acontextaware,
 author = {Zhang, Biao and Xiong, Deyi and Su, Jinsong and Duan, Hong},
 title = {A Context-Aware Recurrent Encoder for Neural Machine Translation},
 journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
 issue_date = {December 2017},
 volume = {25},
 number = {12},
 month = dec,
 year = {2017},
 issn = {2329-9290},
 pages = {2424--2432},
 numpages = {9},
 url = {https://doi.org/10.1109/TASLP.2017.2751420},
 doi = {10.1109/TASLP.2017.2751420},
 acmid = {3180106},
 publisher = {IEEE Press},
 address = {Piscataway, NJ, USA}
}

@inproceedings{Wang17instance,
	Author = {Wang, Rui and Utiyama, Masao and Liu, Lemao and Chen, Kehai and Sumita, Eiichiro},
	Booktitle = {Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing},
	Date-Added = {2017-09-21 21:28:02 +0000},
	Date-Modified = {2017-09-21 21:28:10 +0000},
	Location = {Copenhagen, Denmark},
	Pages = {1483--1489},
	Publisher = {Association for Computational Linguistics},
	Title = {Instance Weighting for Neural Machine Translation Domain Adaptation},
	Url = {http://aclweb.org/anthology/D17-1155},
	Year = {2017},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D17-1155}}

@inproceedings{Domhan17using,
    title = "Using Target-side Monolingual Data for Neural Machine Translation through Multi-task Learning",
    author = "Domhan, Tobias  and
      Hieber, Felix",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1158",
    doi = "10.18653/v1/D17-1158",
    pages = "1500--1505",
    abstract = "The performance of Neural Machine Translation (NMT) models relies heavily on the availability of sufficient amounts of parallel data, and an efficient and effective way of leveraging the vastly available amounts of monolingual data has yet to be found. We propose to modify the decoder in a neural sequence-to-sequence model to enable multi-task learning for two strongly related tasks: target-side language modeling and translation. The decoder predicts the next target word through two channels, a target-side language model on the lowest layer, and an attentional recurrent model which is conditioned on the source representation. This architecture allows joint training on both large amounts of monolingual and moderate amounts of bilingual data to improve NMT performance. Initial results in the news domain for three language pairs show moderate but consistent improvements over a baseline trained on bilingual data only.",
}

@inproceedings{schwenk17learning,
    title = "Learning Joint Multilingual Sentence Representations with Neural Machine Translation",
    author = "Schwenk, Holger  and
      Douze, Matthijs",
    booktitle = "Proceedings of the 2nd Workshop on Representation Learning for {NLP}",
    month = aug,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-2619",
    doi = "10.18653/v1/W17-2619",
    pages = "157--167",
    abstract = "In this paper, we use the framework of neural machine translation to learn joint sentence representations across six very different languages. Our aim is that a representation which is independent of the language, is likely to capture the underlying semantics. We define a new cross-lingual similarity measure, compare up to 1.4M sentence representations and study the characteristics of close sentences. We provide experimental evidence that sentences that are close in embedding space are indeed semantically highly related, but often have quite different structure and syntax. These relations also hold when comparing sentences in different languages.",
}

@InProceedings{Graves17automated, title = {Automated Curriculum Learning for Neural Networks}, author = {Alex Graves and Marc G. Bellemare and Jacob Menick and R{\'e}mi Munos and Koray Kavukcuoglu}, booktitle = {Proceedings of the 34th International Conference on Machine Learning}, pages = {1311--1320}, year = {2017}, editor = {Precup, Doina and Teh, Yee Whye}, volume = {70}, series = {Proceedings of Machine Learning Research}, month = {06--11 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v70/graves17a/graves17a.pdf}, url = { http://proceedings.mlr.press/v70/graves17a.html }, abstract = {We introduce a method for automatically selecting the path, or syllabus, that a neural network follows through a curriculum so as to maximise learning efficiency. A measure of the amount that the network learns from each data sample is provided as a reward signal to a nonstationary multi-armed bandit algorithm, which then determines a stochastic syllabus. We consider a range of signals derived from two distinct indicators of learning progress: rate of increase in prediction accuracy, and rate of increase in network complexity. Experimental results for LSTM networks on three curricula demonstrate that our approach can significantly accelerate learning, in some cases halving the time required to attain a satisfactory performance level.} }

@inproceedings{Wees17dynamic,
    title = "Dynamic Data Selection for Neural Machine Translation",
    author = "van der Wees, Marlies  and
      Bisazza, Arianna  and
      Monz, Christof",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D17-1147",
    doi = "10.18653/v1/D17-1147",
    pages = "1400--1410",
    abstract = "Intelligent selection of training data has proven a successful technique to simultaneously increase training efficiency and translation performance for phrase-based machine translation (PBMT). With the recent increase in popularity of neural machine translation (NMT), we explore in this paper to what extent and how NMT can also benefit from data selection. While state-of-the-art data selection (Axelrod et al., 2011) consistently performs well for PBMT, we show that gains are substantially lower for NMT. Next, we introduce {`}dynamic data selection{'} for NMT, a method in which we vary the selected subset of training data between different training epochs. Our experiments show that the best results are achieved when applying a technique we call {`}gradual fine-tuning{'}, with improvements up to +2.6 BLEU over the original data selection approach and up to +3.1 BLEU over a general baseline.",
}

@inproceedings{Wees17whats,
  title={What’s in a domain?: Towards fine-grained adaptation for machine translation},
  author={van der Wees, Marlies},
  year={2017}
}

@article{Lee17fully,
    title = "Fully Character-Level Neural Machine Translation without Explicit Segmentation",
    author = "Lee, Jason  and
      Cho, Kyunghyun  and
      Hofmann, Thomas",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "5",
    year = "2017",
    url = "https://www.aclweb.org/anthology/Q17-1026",
    doi = "10.1162/tacl_a_00067",
    pages = "365--378",
    abstract = "Most existing machine translation systems operate at the level of words, relying on explicit segmentation to extract tokens. We introduce a neural machine translation (NMT) model that maps a source character sequence to a target character sequence without any segmentation. We employ a character-level convolutional network with max-pooling at the encoder to reduce the length of source representation, allowing the model to be trained at a speed comparable to subword-level models while capturing local regularities. Our character-to-character model outperforms a recently proposed baseline with a subword-level encoder on WMT{'}15 DE-EN and CS-EN, and gives comparable performance on FI-EN and RU-EN. We then demonstrate that it is possible to share a single character-level encoder across multiple languages by training a model on a many-to-one translation task. In this multilingual setting, the character-level encoder significantly outperforms the subword-level encoder on all the language pairs. We observe that on CS-EN, FI-EN and RU-EN, the quality of the multilingual character-level translation even surpasses the models specifically trained on that language pair alone, both in terms of the BLEU score and human judgment.",
}

@InProceedings{Britz2017mixing,
  author = 	"Britz, Denny
		and Le, Quoc
		and Pryzant, Reid",
  title = 	"Effective Domain Mixing for Neural Machine Translation",
  booktitle = 	"Proceedings of the Second Conference on Machine Translation",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"118--126",
  address = 	"Copenhagen, Denmark",
  doi = 	"10.18653/v1/W17-4712",
  url = 	"http://aclweb.org/anthology/W17-4712"
}

@misc{Khresmoi17test,
 title = {Khresmoi Summary Translation Test Data 2.0},
 author = {Du{\v s}ek, Ond{\v r}ej and Haji{\v c}, Jan and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Libovick{\'y}, Jind{\v r}ich and Pecina, Pavel and Tamchyna, Ale{\v s} and Ure{\v s}ov{\'a}, Zde{\v n}ka},
 url = {http://hdl.handle.net/11234/1-2122},
 note = {{LINDAT}/{CLARIN} digital library at the Institute of Formal and Applied Linguistics ({{\'U}FAL}), Faculty of Mathematics and Physics, Charles University},
 copyright = {Creative Commons - Attribution-{NonCommercial} 4.0 International ({CC} {BY}-{NC} 4.0)},
 year = {2017} }

@inproceedings{Assylbekov17syllable,
    title = "Syllable-aware Neural Language Models: A Failure to Beat Character-aware Ones",
    author = "Assylbekov, Zhenisbek  and
      Takhanov, Rustem  and
      Myrzakhmetov, Bagdat  and
      Washington, Jonathan N.",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1199",
    doi = "10.18653/v1/D17-1199",
    pages = "1866--1872",
    abstract = "Syllabification does not seem to improve word-level RNN language modeling quality when compared to character-based segmentation. However, our best syllable-aware language model, achieving performance comparable to the competitive character-aware model, has 18{\%}-33{\%} fewer parameters and is trained 1.2-2.2 times faster.",
}

@article{Ataman17linguistically,
  title={Linguistically Motivated Vocabulary Reduction for Neural Machine Translation from Turkish to English},
  author={Duygu Ataman and Matteo Negri and M. Turchi and Marcello Federico},
  journal={The Prague Bulletin of Mathematical Linguistics},
  year={2017},
  volume={108},
  pages={331 - 342}
}

@inproceedings{Huck17target,
    title = "Target-side Word Segmentation Strategies for Neural Machine Translation",
    author = "Huck, Matthias  and
      Riess, Simon  and
      Fraser, Alexander",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4706",
    doi = "10.18653/v1/W17-4706",
    pages = "56--67",
}

@InProceedings{Chu17comparison,
  author = 	"Chu, Chenhui
		and Dabre, Raj
		and Kurohashi, Sadao",
  title = 	"An Empirical Comparison of Domain Adaptation Methods for Neural Machine      Translation    ",
  booktitle = 	"Proceedings of the 55th Annual Meeting of the Association for      Computational Linguistics (Volume 2: Short Papers)    ",
  year = 	"2017",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"385--391",
  address = 	"Vancouver, Canada",
  doi = 	"10.18653/v1/P17-2061",
  url = 	"http://aclweb.org/anthology/P17-2061"
}

@inproceedings{Currey17copied,
    title = "Copied Monolingual Data Improves Low-Resource Neural Machine Translation",
    author = "Currey, Anna  and
      Miceli Barone, Antonio Valerio  and
      Heafield, Kenneth",
    booktitle = "Proceedings of the Second Conference on Machine Translation",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W17-4715",
    doi = "10.18653/v1/W17-4715",
    pages = "148--156",
}

@InProceedings{Khayrallah17neural,
  author    = {Khayrallah, Huda and Kumar, Gaurav and Duh, Kevin and Post, Matt and Koehn, Philipp},
  title     = {Neural Lattice Search for Domain Adaptation in Machine Translation},
  booktitle = {Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
  year      = {2017},
  publisher = {Asian Federation of Natural Language Processing},
  month     = {November},
  pages     = {20--25},
  url       = {http://www.aclweb.org/anthology/I17-2004},
}

@inproceedings{Chelsea17model,
  author    = {Chelsea Finn and
               Pieter Abbeel and
               Sergey Levine},
  editor    = {Doina Precup and
               Yee Whye Teh},
  title     = {Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning,
               {ICML} 2017, Sydney, NSW, Australia, 6-11 August 2017},
  series    = {Proceedings of Machine Learning Research},
  volume    = {70},
  pages     = {1126--1135},
  publisher = {{PMLR}},
  year      = {2017},
  url       = {http://proceedings.mlr.press/v70/finn17a.html},
  timestamp = {Wed, 29 May 2019 08:41:45 +0200},
  biburl    = {https://dblp.org/rec/conf/icml/FinnAL17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Machcek18morphological,
  title={Morphological and Language-Agnostic Word Segmentation for NMT},
  author={Dominik Mach{\'a}cek and Jon{\'a}s Vidra and Ondrej Bojar},
  booktitle={International Conference on Text, Speech, and Dialogue},
  year={2018},
  pages = {277--284},}

@inproceedings{wuebker18compact,
    title = "Compact Personalized Models for Neural Machine Translation",
    author = "Wuebker, Joern  and
      Simianer, Patrick  and
      DeNero, John",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1104",
    doi = "10.18653/v1/D18-1104",
    pages = "881--886",
    abstract = "We propose and compare methods for gradient-based domain adaptation of self-attentive neural machine translation models. We demonstrate that a large proportion of model parameters can be frozen during adaptation with minimal or no reduction in translation quality by encouraging structured sparsity in the set of offset tensors during learning via group lasso regularization. We evaluate this technique for both batch and incremental adaptation across multiple data sets and language pairs. Our system architecture{--}combining a state-of-the-art self-attentive model with compact domain adaptation{--}provides high quality personalized machine translation that is both space and time efficient.",
}

@inproceedings{Ott18scaling,
    title = "Scaling Neural Machine Translation",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6301",
    doi = "10.18653/v1/W18-6301",
    pages = "1--9",
    abstract = "Sequence to sequence learning models still require several days to reach state of the art performance on large benchmark datasets using a single machine. This paper shows that reduced precision and large batch training can speedup training by nearly 5x on a single 8-GPU machine with careful tuning and implementation. On WMT{'}14 English-German translation, we match the accuracy of Vaswani et al. (2017) in under 5 hours when training on 8 GPUs and we obtain a new state of the art of 29.3 BLEU after training for 85 minutes on 128 GPUs. We further improve these results to 29.8 BLEU by training on the much larger Paracrawl dataset. On the WMT{'}14 English-French task, we obtain a state-of-the-art BLEU of 43.2 in 8.5 hours on 128 GPUs.",
}

@inproceedings{Burlot18using,
    title = "Using Monolingual Data in Neural Machine Translation: a Systematic Study",
    author = "Burlot, Franck  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6315",
    doi = "10.18653/v1/W18-6315",
    pages = "144--155",
    abstract = "Neural Machine Translation (MT) has radically changed the way systems are developed. A major difference with the previous generation (Phrase-Based MT) is the way monolingual target data, which often abounds, is used in these two paradigms. While Phrase-Based MT can seamlessly integrate very large language models trained on billions of sentences, the best option for Neural MT developers seems to be the generation of artificial parallel data through back-translation - a technique that fails to fully take advantage of existing datasets. In this paper, we conduct a systematic study of back-translation, comparing alternative uses of monolingual data, as well as multiple data generation procedures. Our findings confirm that back-translation is very effective and give new explanations as to why this is the case. We also introduce new data simulation techniques that are almost as effective, yet much cheaper to implement.",
}

@inproceedings{Rebuffi18efficient,
  author    = {Sylvestre{-}Alvise Rebuffi and
               Hakan Bilen and
               Andrea Vedaldi},
  title     = {Efficient Parametrization of Multi-Domain Deep Neural Networks},
  booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition,
               {CVPR} 2018, Salt Lake City, UT, USA, June 18-22, 2018},
  pages     = {8119--8127},
  publisher = {{IEEE} Computer Society},
  year      = {2018},
  url       = {http://openaccess.thecvf.com/content\_cvpr\_2018/html/Rebuffi\_Efficient\_Parametrization\_of\_CVPR\_2018\_paper.html},
  doi       = {10.1109/CVPR.2018.00847},
  timestamp = {Wed, 16 Oct 2019 14:14:50 +0200},
  biburl    = {https://dblp.org/rec/conf/cvpr/RebuffiBV18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Rosenfeld18Priming,
  title={Priming Neural Networks},
  author={Amir Rosenfeld and Mahdi Biparva and John K. Tsotsos},
  journal={2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},
  year={2018},
  pages={2092-209209}
}

@inproceedings{Post18call,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@inproceedings{bawden18evaluating,
    title = "Evaluating Discourse Phenomena in Neural Machine Translation",
    author = "Bawden, Rachel  and
      Sennrich, Rico  and
      Birch, Alexandra  and
      Haddow, Barry",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1118",
    doi = "10.18653/v1/N18-1118",
    pages = "1304--1313",
    abstract = "For machine translation to tackle discourse phenomena, models must have access to extra-sentential linguistic context. There has been recent interest in modelling context in neural machine translation (NMT), but models have been principally evaluated with standard automatic metrics, poorly adapted to evaluating discourse phenomena. In this article, we present hand-crafted, discourse test sets, designed to test the models{'} ability to exploit previous source and target sentences. We investigate the performance of recently proposed multi-encoder NMT models trained on subtitles for English to French. We also explore a novel way of exploiting context from the previous sentence. Despite gains using BLEU, multi-encoder models give limited improvement in the handling of discourse phenomena: 50{\%} accuracy on our coreference test set and 53.5{\%} for coherence/cohesion (compared to a non-contextual baseline of 50{\%}). A simple strategy of decoding the concatenation of the previous and current sentence leads to good performance, and our novel strategy of multi-encoding and decoding of two sentences leads to the best performance (72.5{\%} for coreference and 57{\%} for coherence/cohesion), highlighting the importance of target-side context.",
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{Zeng18multidomain,
	Address = {Brussels, Belgium},
	Author = {Zeng, Jiali and Su, Jinsong and Wen, Huating and Liu, Yang and Xie, Jun and Yin, Yongjing and Zhao, Jianqiang},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {447--457},
	Publisher = {Association for Computational Linguistics},
	Title = {Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination},
	Url = {http://aclweb.org/anthology/D18-1041},
	Year = 2018,
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1041}
} 

@inproceedings{Neubig18rapid,
    title = "Rapid Adaptation of Neural Machine Translation to New Languages",
    author = "Neubig, Graham  and
      Hu, Junjie",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1103",
    doi = "10.18653/v1/D18-1103",
    pages = "875--880",
    abstract = "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual {``}seed models{''}, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of {``}similar-language regularization{''}, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",
}

@inproceedings{Poncelas18Feature,
  title={Feature decay algorithms for neural machine translation},
  author={Alberto Poncelas and G. M. D. B. Wenniger and A. Way},
  booktitle = {Proceedings of the 21st Annual Conference of the European Association for Machine Translation},
  pages = {239-248},
  editor= {European Association for Machine Translation},
  url = {http://rua.ua.es/dspace/handle/10045/76084},
  
  year= 2018
}

@inproceedings{Taku18subword,
  author    = {Taku Kudo},
  editor    = {Iryna Gurevych and
               Yusuke Miyao},
  title     = {Subword Regularization: Improving Neural Network Translation Models
               with Multiple Subword Candidates},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2018, Melbourne, Australia, July 15-20, 2018, Volume
               1: Long Papers},
  pages     = {66--75},
  publisher = {Association for Computational Linguistics},
  year      = {2018},
  url       = {https://www.aclweb.org/anthology/P18-1007/},
  doi       = {10.18653/v1/P18-1007},
  timestamp = {Mon, 16 Sep 2019 13:46:41 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/Kudo18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Michel18extreme,
  author = 	"Michel, Paul
		and Neubig, Graham",
  title = 	"Extreme Adaptation for Personalized Neural Machine Translation",
  booktitle = 	"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"312--318",
  address = 	"Melbourne, Australia",
  url = 	"http://aclweb.org/anthology/P18-2050"
}

@inproceedings{Cherry18revisiting,
    title = "Revisiting Character-Based Neural Machine Translation with Capacity and Compression",
    author = "Cherry, Colin  and
      Foster, George  and
      Bapna, Ankur  and
      Firat, Orhan  and
      Macherey, Wolfgang",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1461",
    doi = "10.18653/v1/D18-1461",
    pages = "4295--4305",
    abstract = "Translating characters instead of words or word-fragments has the potential to simplify the processing pipeline for neural machine translation (NMT), and improve results by eliminating hyper-parameters and manual feature engineering. However, it results in longer sequences in which each symbol contains less information, creating both modeling and computational challenges. In this paper, we show that the modeling problem can be solved by standard sequence-to-sequence architectures of sufficient depth, and that deep models operating at the character level outperform identical models operating over word fragments. This result implies that alternative architectures for handling character input are better viewed as methods for reducing computation time than as improved ways of modeling longer sequences. From this perspective, we evaluate several techniques for character-level NMT, verify that they do not match the performance of our deep character baseline model, and evaluate the performance versus computation time tradeoffs they offer. Within this framework, we also perform the first evaluation for NMT of conditional computation over time, in which the model learns which timesteps can be skipped, rather than having them be dictated by a fixed schedule specified before training begins.",
}

@inproceedings{Koehn18findings,
    title = "Findings of the {WMT} 2018 Shared Task on Parallel Corpus Filtering",
    author = "Koehn, Philipp  and
      Khayrallah, Huda  and
      Heafield, Kenneth  and
      Forcada, Mikel L.",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Shared Task Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6453",
    doi = "10.18653/v1/W18-6453",
    pages = "726--739",
    abstract = "We posed the shared task of assigning sentence-level quality scores for a very noisy corpus of sentence pairs crawled from the web, with the goal of sub-selecting 1{\%} and 10{\%} of high-quality data to be used to train machine translation systems. Seventeen participants from companies, national research labs, and universities participated in this task.",
}
@InProceedings{Tars18multidomain,
  author    = {Sander Tars and Mark Fishel},
  title     = {Multi-Domain Neural Machine Translation},
  booktitle = {Proceedings of the 21st Annual Conference of the European Association for Machine Translation},
  year = 	 2018,
  editor = 	 {Juan Antonio Pérez-Ortiz and Felipe Sánchez-Martínez and Miquel Esplà-Gomis and Maja Popović and Celia Rico and André Martins and Joachim Van den Bogaert and Mikel L. Forcada},
  series = 	 {EAMT},
  pages = 	 {259--269},
  address = 	 {Alicante, Spain},
  Url = {https://arxiv.org/pdf/1805.02282.pdf},
  organization = {EAMT}}
@inproceedings{Pham18fixing,
    title = "Fixing Translation Divergences in Parallel Corpora for Neural {MT}",
    author = "Pham, MinhQuang  and
      Crego, Josep  and
      Senellart, Jean  and
      Yvon, Fran{\c{c}}ois",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1328",
    doi = "10.18653/v1/D18-1328",
    pages = "2967--2973",
    abstract = "Corpus-based approaches to machine translation rely on the availability of clean parallel corpora. Such resources are scarce, and because of the automatic processes involved in their preparation, they are often noisy. This paper describes an unsupervised method for detecting translation divergences in parallel sentences. We rely on a neural network that computes cross-lingual sentence similarity scores, which are then used to effectively filter out divergent translations. Furthermore, similarity scores predicted by the network are used to identify and fix some partial divergences, yielding additional parallel segments. We evaluate these methods for English-French and English-German machine translation tasks, and show that using filtered/corrected corpora actually improves MT performance.",
}

@inproceedings{Guillaume18unsupervised,
  author    = {Guillaume Lample and
               Alexis Conneau and
               Ludovic Denoyer and
               Marc'Aurelio Ranzato},
  title     = {Unsupervised Machine Translation Using Monolingual Corpora Only},
  booktitle = {6th International Conference on Learning Representations, {ICLR} 2018,
               Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher = {OpenReview.net},
  year      = {2018},
  url       = {https://openreview.net/forum?id=rkYTTf-AZ},
  timestamp = {Thu, 25 Jul 2019 14:25:53 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/LampleCDR18.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Chu18asurvey,
  author = 	"Chu, Chenhui and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  series = "COLING 2018",
  year = 	"2018",
  pages = 	"1304--1319",
  address = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}

@article{Alex18first,
  author    = {Alex Nichol and
               Joshua Achiam and
               John Schulman},
  title     = {On First-Order Meta-Learning Algorithms},
  journal   = {CoRR},
  volume    = {abs/1803.02999},
  year      = {2018},
  url       = {http://arxiv.org/abs/1803.02999},
  archivePrefix = {arXiv},
  eprint    = {1803.02999},
  timestamp = {Mon, 13 Aug 2018 16:48:00 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1803-02999.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Peters18deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
    abstract = "We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",
}

@inproceedings{raganato18analysis,
    title = "An Analysis of Encoder Representations in Transformer-Based Machine Translation",
    author = {Raganato, Alessandro  and
      Tiedemann, J{\"o}rg},
    booktitle = "Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP}",
    month = nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-5431",
    doi = "10.18653/v1/W18-5431",
    pages = "287--297",
    abstract = "The attention mechanism is a successful technique in modern NLP, especially in tasks like machine translation. The recently proposed network architecture of the \textit{Transformer} is based entirely on attention mechanisms and achieves new state of the art results in neural machine translation, outperforming other sequence-to-sequence models. However, so far not much is known about the internal properties of the model and the representations it learns to achieve that performance. To study this question, we investigate the information that is learned by the attention mechanism in Transformer models with different translation quality. We assess the representations of the encoder by extracting dependency relations based on self-attention weights, we perform four probing tasks to study the amount of syntactic and semantic captured information and we also test attention in a transfer learning scenario. Our analysis sheds light on the relative strengths and weaknesses of the various encoder representations. We observe that specific attention heads mark syntactic dependency relations and we can also confirm that lower layers tend to learn more about syntax while higher layers tend to encode more semantics.",
}

@incollection{Hoffman18algorithms,
title = {Algorithms and Theory for Multiple-Source Adaptation},
author = {Hoffman, Judy and Mohri, Mehryar and Zhang, Ningshan},
booktitle = {Advances in Neural Information Processing Systems 31},
editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},
pages = {8246--8256},
year = {2018},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8046-algorithms-and-theory-for-multiple-source-adaptation.pdf}
}
@inproceedings{Chu18survey,
    title = "A Survey of Domain Adaptation for Neural Machine Translation",
    author = "Chu, Chenhui  and
      Wang, Rui",
    booktitle = "Proceedings of the 27th International Conference on Computational Linguistics",
    month = aug,
    year = "2018",
    address = "Santa Fe, New Mexico, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/C18-1111",
    pages = "1304--1319",
    abstract = "Neural machine translation (NMT) is a deep learning based approach for machine translation, which yields the state-of-the-art translation performance in scenarios where large-scale parallel corpora are available. Although the high-quality and domain-specific translation is crucial in the real world, domain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT performs poorly in such scenarios. Domain adaptation that leverages both out-of-domain parallel corpora as well as monolingual corpora for in-domain translation, is very important for domain-specific translation. In this paper, we give a comprehensive survey of the state-of-the-art domain adaptation techniques for NMT.",
}
@inproceedings{Pei18multi,
  author    = {Zhongyi Pei and
               Zhangjie Cao and
               Mingsheng Long and
               Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},
  crossref  = {DBLP:conf/aaai/2018},
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Silva18extracting,
    title = "Extracting In-domain Training Corpora for Neural Machine Translation Using Data Selection Methods",
    author = "Silva, Catarina Cruz  and
      Liu, Chao-Hong  and
      Poncelas, Alberto  and
      Way, Andy",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Belgium, Brussels",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6323",
    doi = "10.18653/v1/W18-6323",
    pages = "224--231",
}

@InProceedings{Zheng18multi,
  author = 	"Zeng, Jiali
		and Su, Jinsong
		and Wen, Huating
		and Liu, Yang
		and Xie, Jun
		and Yin, Yongjing
		and Zhao, Jianqiang",
  title = 	"Multi-Domain Neural Machine Translation with Word-Level Domain Context Discrimination",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"447--457",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1041"
}

@InProceedings{Zhang18sentence,
  author = 	"Zhang, Shiqi
		and Xiong, Deyi",
  title = 	"Sentence Weighting for Neural Machine Translation Domain Adaptation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"3181--3190",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1269"
}

@InProceedings{Thompson18freezing,
  author = 	"Thompson, Brian
		and Khayrallah, Huda
		and Anastasopoulos, Antonios
		and McCarthy, Arya D.
		and Duh, Kevin
		and Marvin, Rebecca
		and McNamee, Paul
		and Gwinnup, Jeremy
		and Anderson, Tim
		and Koehn, Philipp",
  title = 	"Freezing Subnetworks to Analyze Domain Adaptation in Neural Machine Translation",
  booktitle = 	"Proceedings of the Third Conference on Machine Translation: Research Papers",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"124--132",
  location = 	"Belgium, Brussels",
  url = 	"http://aclweb.org/anthology/W18-6313"
}

@InProceedings{Vilar18learning,
  author = 	"Vilar, David",
  title = 	"Learning Hidden Unit Contribution for Adapting Neural Machine Translation      Models    ",
  booktitle = 	"Proceedings of the 2018 Conference of the North American Chapter of the      Association for Computational Linguistics: Human Language Technologies,      Volume 2 (Short Papers)    ",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"500--505",
  location = 	"New Orleans, Louisiana",
  doi = 	"10.18653/v1/N18-2080",
  url = 	"http://aclweb.org/anthology/N18-2080"
}

@InProceedings{Chu18survey,
  author = 	"Chu, Chenhui
		and Wang, Rui",
  title = 	"A Survey of Domain Adaptation for Neural Machine Translation",
  booktitle = 	"Proceedings of the 27th International Conference on Computational Linguistics",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1304--1319",
  location = 	"Santa Fe, New Mexico, USA",
  url = 	"http://aclweb.org/anthology/C18-1111"
}
@inproceedings{Li18onesentence,
    title = "One Sentence One Model for Neural Machine Translation",
    author = "Li, Xiaoqing  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    booktitle = "Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018)",
    month = may,
    year = "2018",
    address = "Miyazaki, Japan",
    publisher = "European Language Resources Association (ELRA)",
    url = "https://www.aclweb.org/anthology/L18-1146",
}
@inproceedings{Zhongyi18multi,
  author    = {Zhongyi Pei and
               Zhangjie Cao and
               Mingsheng Long and
               Jianmin Wang},
  title     = {Multi-Adversarial Domain Adaptation},
  booktitle = {Proceedings of the Thirty-Second {AAAI} Conference on Artificial Intelligence,
               (AAAI-18), the 30th innovative Applications of Artificial Intelligence
               (IAAI-18), and the 8th {AAAI} Symposium on Educational Advances in
               Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February
               2-7, 2018},
  pages     = {3934--3941},
  year      = {2018},  
  url       = {https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/view/17067},
  timestamp = {Tue, 23 Oct 2018 06:42:15 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/aaai/PeiCLW18},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{Alvarez18gromov,
  author = 	"Alvarez-Melis, David
		and Jaakkola, Tommi",
  title = 	"Gromov-Wasserstein Alignment of Word Embedding Spaces",
  booktitle = 	"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
  year = 	"2018",
  publisher = 	"Association for Computational Linguistics",
  pages = 	"1881--1890",
  location = 	"Brussels, Belgium",
  url = 	"http://aclweb.org/anthology/D18-1214"
}

@inproceedings{Niu18multitask,
  author    = {Xing Niu and Sudha Rao and Marine Carpuat},
  title     = {Multi-Task Neural Models for Translating Between Styles Within and Across Languages},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  pages     = {1008--1021},
  series    = {COLING},
  address   = {Santa Fe, New Mexico, USA},
  year      = {2018},
  editor    = {Emily M. Bender and Leon Derczynski and Pierre Isabelle},
  crossref  = {DBLP:conf/coling/2018},
  url       = {https://aclanthology.info/papers/C18-1086/c18-1086},
}

@inproceedings{pagliardini18unsupervised,
    title = "Unsupervised Learning of Sentence Embeddings Using Compositional n-Gram Features",
    author = "Pagliardini, Matteo  and
      Gupta, Prakhar  and
      Jaggi, Martin",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1049",
    doi = "10.18653/v1/N18-1049",
    pages = "528--540",
    abstract = "The recent tremendous success of unsupervised word embeddings in a multitude of applications raises the obvious question if similar methods could be derived to improve embeddings (i.e. semantic representations) of word sequences as well. We present a simple but efficient unsupervised objective to train distributed representations of sentences. Our method outperforms the state-of-the-art unsupervised models on most benchmark tasks, highlighting the robustness of the produced general-purpose sentence embeddings.",
}

@inproceedings{Post18A,
    title = "A Call for Clarity in Reporting {BLEU} Scores",
    author = "Post, Matt",
    booktitle = "Proceedings of the Third Conference on Machine Translation: Research Papers",
    month = oct,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W18-6319",
    doi = "10.18653/v1/W18-6319",
    pages = "186--191",
    abstract = "The field of machine translation faces an under-recognized problem because of inconsistency in the reporting of scores from its dominant metric. Although people refer to {``}the{''} BLEU score, BLEU is in fact a parameterized metric whose values can vary wildly with changes to these parameters. These parameters are often not reported or are hard to find, and consequently, BLEU scores between papers cannot be directly compared. I quantify this variation, finding differences as high as 1.8 between commonly used configurations. The main culprit is different tokenization and normalization schemes applied to the reference. Pointing to the success of the parsing community, I suggest machine translation researchers settle upon the BLEU scheme used by the annual Conference on Machine Translation (WMT), which does not allow for user-supplied reference processing, and provide a new tool, SACREBLEU, to facilitate this.",
}

@InProceedings{Chu18multilingual,
  author = 	 {Chenhui Chu and Raj Dabre},
  title = 	 {Multilingual and multi-domain adaptation for neural machine translation},
  booktitle = {Proceedings of the 24st Annual Meeting of the Association for Natural Language Processing (NLP 2018)},
  year = 	 2018,
  pages = 	 {909-–912},
  address = 	 {Okayama, Japan}}

@inproceedings{Lample18unsupervised,
title={Unsupervised Machine Translation Using Monolingual Corpora Only},
author={Guillaume Lample and Alexis Conneau and Ludovic Denoyer and Marc'Aurelio Ranzato},
booktitle={International Conference on Learning Representations},
year={2018},
address = {Vancouver, Canada},
url={https://openreview.net/forum?id=rkYTTf-AZ},
}

@inproceedings{Artetxe18unsupervised,
title={Unsupervised Neural Machine Translation},
author={Mikel Artetxe and Gorka Labaka and Eneko Agirre and Kyunghyun Cho},
booktitle={International Conference on Learning Representations},
year={2018},
address = {Vancouver, Canada},
url={https://openreview.net/forum?id=Sy2ogebAW},
}

@inproceedings{qi18when,
    title = "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
    author = "Qi, Ye  and
      Sachan, Devendra  and
      Felix, Matthieu  and
      Padmanabhan, Sarguna  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N18-2084",
    doi = "10.18653/v1/N18-2084",
    pages = "529--535",
    abstract = "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases {--} providing gains of up to 20 BLEU points in the most favorable setting.",
}

@inproceedings{Platanios18contextual,
	Address = {Brussels, Belgium},
	Author = {Platanios, Emmanouil Antonios and Sachan, Mrinmaya and Neubig, Graham and Mitchell, Tom},
	Booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
	Pages = {425--435},
	Publisher = {Association for Computational Linguistics},
	Title = {Contextual Parameter Generation for Universal Neural Machine Translation},
	Url = {http://aclweb.org/anthology/D18-1039},
	Year = {2018},
	Bdsk-Url-1 = {http://aclweb.org/anthology/D18-1039}}

@inproceedings{Khayrallah2018regularized,
    title = "Regularized Training Objective for Continued Training for Domain Adaptation in Neural Machine Translation",
    author = "Khayrallah, Huda  and
      Thompson, Brian  and
      Duh, Kevin  and
      Koehn, Philipp",
    booktitle = "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2705",
    doi = "10.18653/v1/W18-2705",
    pages = "36--44",
    abstract = "Supervised domain adaptation{---}where a large generic corpus and a smaller in-domain corpus are both available for training{---}is a challenge for neural machine translation (NMT). Standard practice is to train a generic model and use it to initialize a second model, then continue training the second model on in-domain data to produce an in-domain model. We add an auxiliary term to the training objective during continued training that minimizes the cross entropy between the in-domain model{'}s output word distribution and that of the out-of-domain model to prevent the model{'}s output from differing too much from the original out-of-domain model. We perform experiments on EMEA (descriptions of medicines) and TED (rehearsed presentations), initialized from a general domain (WMT) model. Our method shows improvements over standard continued training by up to 1.5 BLEU.",
}

@inproceedings{Devlin19bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
  booktitle={NAACL-HLT},
  year={2019}
}

@incollection{Conneau19crosslingual,
title = {Cross-lingual Language Model Pretraining},
author = {Conneau, Alexis and Lample, Guillaume},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. {d'Alch\'{e}-Buc} and E. Fox and R. Garnett},
pages = {7059--7069},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/8928-cross-lingual-language-model-pretraining.pdf}
}

@article{schwenk2019ccmatrix,
  title={CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB},
  author={Schwenk, Holger and Wenzek, Guillaume and Edunov, Sergey and Grave, Edouard and Joulin, Armand},
  journal={arXiv preprint arXiv:1911.04944},
  url="https://arxiv.org/pdf/1911.04944.pdf",
  year={2019}
}

@inproceedings{nathan19facebook,
    title = "{F}acebook {FAIR}{'}s {WMT}19 News Translation Task Submission",
    author = "Ng, Nathan  and
      Yee, Kyra  and
      Baevski, Alexei  and
      Ott, Myle  and
      Auli, Michael  and
      Edunov, Sergey",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-5333",
    doi = "10.18653/v1/W19-5333",
    pages = "314--319",
    abstract = "This paper describes Facebook FAIR{'}s submission to the WMT19 shared news translation task. We participate in four language directions, English {\textless}-{\textgreater} German and English {\textless}-{\textgreater} Russian in both directions. Following our submission from last year, our baseline systems are large BPE-based transformer models trained with the FAIRSEQ sequence modeling toolkit. This year we experiment with different bitext data filtering schemes, as well as with adding filtered back-translated data. We also ensemble and fine-tune our models on domain-specific data, then decode using noisy channel model reranking. Our system improves on our previous system{'}s performance by 4.5 BLEU points and achieves the best case-sensitive BLEU score for the translation direction English→Russian.",
}

@InProceedings{houlsby19parameter,
  title =        {Parameter-Efficient Transfer Learning for {NLP}},
  author =       {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
  pages =        {2790--2799},
  year =         {2019},
  editor =       {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume =       {97},
  series =       {Proceedings of Machine Learning Research},
  address =      {Long Beach, California, USA},
  month =        {09--15 Jun},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v97/houlsby19a/houlsby19a.pdf},
  url =          {http://proceedings.mlr.press/v97/houlsby19a.html},
  abstract =     {Fine-tuning large pretrained models is an effective transfer mechanism in NLP. However, in the presence of many downstream tasks, fine-tuning is parameter inefficient: an entire new model is required for every task. As an alternative, we propose transfer with adapter modules. Adapter modules yield a compact and extensible model; they add only a few trainable parameters per task, and new tasks can be added without revisiting previous ones. The parameters of the original network remain fixed, yielding a high degree of parameter sharing. To demonstrate adapter’s effectiveness, we transfer the recently proposed BERT Transformer model to $26$ diverse text classification tasks, including the GLUE benchmark. Adapters attain near state-of-the-art performance, whilst adding only a few parameters per task. On GLUE, we attain within $0.8%$ of the performance of full fine-tuning, adding only $3.6%$ parameters per task. By contrast, fine-tuning trains $100%$ of the parameters per task.}
}

@inproceedings{Dou19unsupervised,
    title = "Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings",
    author = "Dou, Zi-Yi  and
      Hu, Junjie  and
      Anastasopoulos, Antonios  and
      Neubig, Graham",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1147",
    doi = "10.18653/v1/D19-1147",
    pages = "1417--1422",
    abstract = "The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.",
}

@article{Arivazhagan19massively,
  author    = {Naveen Arivazhagan and Ankur Bapna and Orhan Firat and Dmitry Lepikhin and
               Melvin Johnson and Maxim Krikun and Mia Xu Chen and Yuan Cao and
               George Foster and Colin Cherry and Wolfgang Macherey and Zhifeng Chen and Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings and Challenges},
  journal   = {arXiv e-prints},
  volume    = {abs/1907.05019},
  year      = {2019},
  primaryClass = {cs.CL},
  url       = {http://arxiv.org/abs/1907.05019}
}

@ARTICLE{Yan19wordbased,
       author = {{Yan}, Shen and {Dahlmann}, Leonard and {Petrushkov}, Pavel and
         {Hewavitharana}, Sanjika and {Khadivi}, Shahram},
        title = "{Word-based Domain Adaptation for Neural Machine Translation}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = "2019",
        month = "Jun",
          eid = {arXiv:1906.03129},
        pages = {arXiv:1906.03129},
archivePrefix = {arXiv},
       eprint = {1906.03129},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190603129Y},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Jiang19multidomain,
    title = "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing",
    author = "Jiang, Haoming  and
      Liang, Chen  and
      Wang, Chong  and
      Zhao, Tuo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.165",
    doi = "10.18653/v1/2020.acl-main.165",
    pages = "1823--1834",
    abstract = "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
}

@inproceedings{Platanios19competence,
    title = "Competence-based Curriculum Learning for Neural Machine Translation",
    author = "Platanios, Emmanouil Antonios and Stretcu, Otilia and Neubig, Graham and Poczos, Barnabas and Mitchell, Tom",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1119",
    doi = "10.18653/v1/N19-1119",
    pages = "1162--1172",
    abstract = "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70{\%} decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
}

@article{So19theevolved,
  author    = {David R. So and
               Chen Liang and
               Quoc V. Le},
  title     = {The Evolved Transformer},
  journal   = {CoRR},
  volume    = {abs/1901.11117},
  year      = {2019},
  url       = {http://arxiv.org/abs/1901.11117},
  archivePrefix = {arXiv},
  eprint    = {1901.11117},
  timestamp = {Mon, 04 Feb 2019 08:11:03 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1901-11117},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@inproceedings{Junjie19domain,
    title = {Domain Adaptation of Neural Machine Translation by Lexicon Induction},
    author = {Junjie Hu and Mengzhou Xia and Graham Neubig and Jaime Carbonell},
    booktitle = {The 57th Annual Meeting of the Association for Computational Linguistics (ACL)},
    address = {Florence, Italy},
    month = {July},
    url = {https://arxiv.org/abs/1906.00376},
    year = {2019}
}

@article{amos19lml,
  author    = {Brandon Amos and
               Vladlen Koltun and
               J. Zico Kolter},
  title     = {The Limited Multi-Label Projection Layer},
  journal   = {CoRR},
  volume    = {abs/1906.08707},
  year      = {2019},
  url       = {http://arxiv.org/abs/1906.08707},
  eprinttype = {arXiv},
  eprint    = {1906.08707},
  timestamp = {Mon, 24 Jun 2019 17:28:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1906-08707.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@ARTICLE{Li19semisupervised,
       author = {{Li}, Yitong and {Baldwin}, Timothy and {Cohn}, Trevor},
        title = "{Semi-supervised Stochastic Multi-Domain Learning using Variational Inference}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = "2019",
        month = "Jun",
          eid = {arXiv:1906.02897},
        pages = {arXiv:1906.02897},
archivePrefix = {arXiv},
       eprint = {1906.02897},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190602897L},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Wang19dynamically,
    title = "Dynamically Composing Domain-Data Selection with Clean-Data Selection by {``}Co-Curricular Learning{''} for Neural Machine Translation",
    author = "Wang, Wei  and
      Caswell, Isaac  and
      Chelba, Ciprian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1123",
    doi = "10.18653/v1/P19-1123",
    pages = "1282--1292",
    abstract = "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a {``}co-curricular learning{''} method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the {``}co-curriculum{''}. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",
}

@misc{sabet2019robust,
    title={Robust Cross-lingual Embeddings from Parallel Sentences},
    author={Ali Sabet and Prakhar Gupta and Jean-Baptiste Cordonnier and Robert West and Martin Jaggi},
    year={2019},
    eprint={1912.12481},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@inproceedings{Neubig19compare-mt,
    title = "compare-mt: A Tool for Holistic Comparison of Language Generation Systems",
    author = "Neubig, Graham  and
      Dou, Zi-Yi  and
      Hu, Junjie  and
      Michel, Paul  and
      Pruthi, Danish  and
      Wang, Xinyi",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-4007",
    doi = "10.18653/v1/N19-4007",
    pages = "35--41",
    abstract = "In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation. The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement. It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system. It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis. compare-mt is a pure-Python open source package, that has already proven useful to generate analyses that have been used in our published papers. Demo Video: https://youtu.be/NyJEQT7t2CA",
}

@inproceedings{Saunders19ucam,
    title = "{UCAM} Biomedical Translation at {WMT}19: Transfer Learning Multi-domain Ensembles",
    author = "Saunders, Danielle and Stahlberg, Felix and Byrne, Bill",
    booktitle = "Proceedings of the Fourth Conference on Machine Translation (Volume 3: Shared Task Papers, Day 2)",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-5421",
    pages = "169--174",
    abstract = "The 2019 WMT Biomedical translation task involved translating Medline abstracts. We approached this using transfer learning to obtain a series of strong neural models on distinct domains, and combining them into multi-domain ensembles. We further experimented with an adaptive language-model ensemble weighting scheme. Our submission achieved the best submitted results on both directions of English-Spanish.",
}

@inproceedings{bapna19non,
    title = "Non-Parametric Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1191",
    doi = "10.18653/v1/N19-1191",
    pages = "1921--1931",
    abstract = "Neural Networks trained with gradient descent are known to be susceptible to catastrophic forgetting caused by parameter shift during the training process. In the context of Neural Machine Translation (NMT) this results in poor performance on heterogeneous datasets and on sub-tasks like rare phrase translation. On the other hand, non-parametric approaches are immune to forgetting, perfectly complementing the generalization ability of NMT. However, attempts to combine non-parametric or retrieval based approaches with NMT have only been successful on narrow domains, possibly due to over-reliance on sentence level retrieval. We propose a novel n-gram level retrieval approach that relies on local phrase level similarities, allowing us to retrieve neighbors that are useful for translation even when overall sentence similarity is low. We complement this with an expressive neural network, allowing our model to extract information from the noisy retrieved context. We evaluate our Semi-parametric NMT approach on a heterogeneous dataset composed of WMT, IWSLT, JRC-Acquis and OpenSubtitles, and demonstrate gains on all 4 evaluation sets. The Semi-parametric nature of our approach also opens the door for non-parametric domain adaptation, demonstrating strong inference-time adaptation performance on new domains without the need for any parameter updates.",
}

@inproceedings{peters18deep,
    title = "Deep Contextualized Word Representations",
    author = "Peters, Matthew  and
      Neumann, Mark  and
      Iyyer, Mohit  and
      Gardner, Matt  and
      Clark, Christopher  and
      Lee, Kenton  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1202",
    doi = "10.18653/v1/N18-1202",
    pages = "2227--2237",
}

@inproceedings{Miller19simplified,
    title = "Simplified Neural Unsupervised Domain Adaptation",
    author = "Miller, Timothy",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1039",
    doi = "10.18653/v1/N19-1039",
    pages = "414--419",
    abstract = "Unsupervised domain adaptation (UDA) is the task of training a statistical model on labeled data from a source domain to achieve better performance on data from a target domain, with access to only unlabeled data in the target domain. Existing state-of-the-art UDA approaches use neural networks to learn representations that are trained to predict the values of subset of important features called {``}pivot features{''} on combined data from the source and target domains. In this work, we show that it is possible to improve on existing neural domain adaptation algorithms by 1) jointly training the representation learner with the task learner; and 2) removing the need for heuristically-selected {``}pivot features.{''} Our results show competitive performance with a simpler model.",
}

@article{Su19exploring,
author={Jinsong Su and Jiali Zeng and Jun Xie and Huating Wen and Yongjing Yin and Yang Liu},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)},
title={Exploring Discriminative Word-Level Domain Contexts for Multi-domain Neural Machine Translation},
year={2019},
volume={},
number={},
pages={1-1},
keywords={Multi-domain Neural Machine Translation;Word-Level Context;Adversarial Training},
doi={10.1109/TPAMI.2019.2954406},
ISSN={1939-3539},
month={},}


@InProceedings{Kool19stochastic,
  title = 	 {Stochastic Beams and Where To Find Them: The {G}umbel-Top-k Trick for Sampling Sequences Without Replacement},
  author =       {Kool, Wouter and Van Hoof, Herke and Welling, Max},
  booktitle = 	 {Proceedings of the 36th International Conference on Machine Learning},
  pages = 	 {3499--3508},
  year = 	 {2019},
  editor = 	 {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  volume = 	 {97},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {09--15 Jun},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v97/kool19a/kool19a.pdf},
  url = 	 {https://proceedings.mlr.press/v97/kool19a.html},
  abstract = 	 {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample $k$ elements without replacement. We show how to implicitly apply this ’Gumbel-Top-$k$’ trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in $k$ and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.}
}


@inproceedings{Liu19reinforced,
    title = "Reinforced Training Data Selection for Domain Adaptation",
    author = "Liu, Miaofeng  and
      Song, Yan  and
      Zou, Hongbin  and
      Zhang, Tong",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1189",
    doi = "10.18653/v1/P19-1189",
    pages = "1957--1968",
    abstract = "Supervised models suffer from the problem of domain shifting where distribution mismatch in the data across domains greatly affect model performance. To solve the problem, training data selection (TDS) has been proven to be a prospective solution for domain adaptation in leveraging appropriate data. However, conventional TDS methods normally requires a predefined threshold which is neither easy to set nor can be applied across tasks, and models are trained separately with the TDS process. To make TDS self-adapted to data and task, and to combine it with model training, in this paper, we propose a reinforcement learning (RL) framework that synchronously searches for training instances relevant to the target domain and learns better representations for them. A selection distribution generator (SDG) is designed to perform the selection and is updated according to the rewards computed from the selected data, where a predictor is included in the framework to ensure a task-specific model can be trained on the selected data and provides feedback to rewards. Experimental results from part-of-speech tagging, dependency parsing, and sentiment analysis, as well as ablation studies, illustrate that the proposed framework is not only effective in data selection and representation, but also generalized to accommodate different NLP tasks.",
}

@inproceedings{Oren19distributionally,
    title = "Distributionally Robust Language Modeling",
    author = "Oren, Yonatan  and
      Sagawa, Shiori  and
      Hashimoto, Tatsunori B.  and
      Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1432",
    doi = "10.18653/v1/D19-1432",
    pages = "4227--4237",
    abstract = "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
}

@inproceedings{sen19multilingual,
    title = "Multilingual Unsupervised {NMT} using Shared Encoder and Language-Specific Decoders",
    author = "Sen, Sukanta  and
      Gupta, Kamal Kumar  and
      Ekbal, Asif  and
      Bhattacharyya, Pushpak",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1297",
    doi = "10.18653/v1/P19-1297",
    pages = "3083--3089",
    abstract = "In this paper, we propose a multilingual unsupervised NMT scheme which jointly trains multiple languages with a shared encoder and multiple decoders. Our approach is based on denoising autoencoding of each language and back-translating between English and multiple non-English languages. This results in a universal encoder which can encode any language participating in training into an inter-lingual representation, and language-specific decoders. Our experiments using only monolingual corpora show that multilingual unsupervised model performs better than the separately trained bilingual models achieving improvement of up to 1.48 BLEU points on WMT test sets. We also observe that even if we do not train the network for all possible translation directions, the network is still able to translate in a many-to-many fashion leveraging encoder{'}s ability to generate interlingual representation.",
}

@inproceedings{santy19inmt,
    title = "{INMT}: Interactive Neural Machine Translation Prediction",
    author = "Santy, Sebastin  and
      Dandapat, Sandipan  and
      Choudhury, Monojit  and
      Bali, Kalika",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-3018",
    doi = "10.18653/v1/D19-3018",
    pages = "103--108",
    abstract = "In this paper, we demonstrate an Interactive Machine Translation interface, that assists human translators with on-the-fly hints and suggestions. This makes the end-to-end translation process faster, more efficient and creates high-quality translations. We augment the OpenNMT backend with a mechanism to accept the user input and generate conditioned translations.",
}

@inproceedings{bulte19neural,
    title = "Neural Fuzzy Repair: Integrating Fuzzy Matches into Neural Machine Translation",
    author = "Bult\'{e}, Bram  and Tezcan, Arda",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1175",
    doi = "10.18653/v1/P19-1175",
    pages = "1800--1809"
}

@ARTICLE{Dou19Unsupervised,
       author = {{Dou}, Zi-Yi and {Hu}, Junjie and {Anastasopoulos}, Antonios and
         {Neubig}, Graham},
        title = "{Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language},
         year = "2019",
        month = "Aug",
          eid = {arXiv:1908.10430},
        pages = {arXiv:1908.10430},
archivePrefix = {arXiv},
       eprint = {1908.10430},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2019arXiv190810430D},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Zhou19synchronous,
    title = "Synchronous Bidirectional Neural Machine Translation",
    author = "Zhou, Long  and
      Zhang, Jiajun  and
      Zong, Chengqing",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "7",
    month = mar,
    year = "2019",
    url = "https://www.aclweb.org/anthology/Q19-1006",
    doi = "10.1162/tacl_a_00256",
    pages = "91--105",
    abstract = "Existing approaches to neural machine translation (NMT) generate the target language sequence token-by-token from left to right. However, this kind of unidirectional decoding framework cannot make full use of the target-side future contexts which can be produced in a right-to-left decoding direction, and thus suffers from the issue of unbalanced outputs. In this paper, we introduce a synchronous bidirectional{--}neural machine translation (SB-NMT) that predicts its outputs using left-to-right and right-to-left decoding simultaneously and interactively, in order to leverage both of the history and future information at the same time. Specifically, we first propose a new algorithm that enables synchronous bidirectional decoding in a single model. Then, we present an interactive decoding model in which left-to-right (right-to-left) generation does not only depend on its previously generated outputs, but also relies on future contexts predicted by right-to-left (left-to-right) decoding. We extensively evaluate the proposed SB-NMT model on large-scale NIST Chinese{--}English, WMT14 English{--}German, and WMT18 Russian{--}English translation tasks. Experimental results demonstrate that our model achieves significant improvements over the strong Transformer model by 3.92, 1.49, and 1.04 BLEU points, respectively, and obtains the state-of-the-art performance on Chinese{--}English and English{--}German translation tasks.",
}

@inproceedings{Bapna19simple,
    title = "Simple, Scalable Adaptation for Neural Machine Translation",
    author = "Bapna, Ankur  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1165",
    doi = "10.18653/v1/D19-1165",
    pages = "1538--1548",
    abstract = "Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation.",
}
@inproceedings{Aharoni19massively,
    title = "Massively Multilingual Neural Machine Translation",
    author = "Aharoni, Roee  and
      Johnson, Melvin  and
      Firat, Orhan",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1388",
    doi = "10.18653/v1/N19-1388",
    pages = "3874--3884",
    abstract = "Multilingual Neural Machine Translation enables training a single model that supports translation from multiple source languages into multiple target languages. We perform extensive experiments in training massively multilingual NMT models, involving up to 103 distinct languages and 204 translation directions simultaneously. We explore different setups for training such models and analyze the trade-offs between translation quality and various modeling decisions. We report results on the publicly available TED talks multilingual corpus where we show that massively multilingual many-to-many models are effective in low resource settings, outperforming the previous state-of-the-art while supporting up to 59 languages in 116 translation directions in a single model. Our experiments on a large-scale dataset with 103 languages, 204 trained directions and up to one million examples per direction also show promising results, surpassing strong bilingual baselines and encouraging future work on massively multilingual NMT.",
}

@article{Wang19neural, title={Neural Machine Translation with Byte-Level Subwords}, volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6451}, DOI={10.1609/aaai.v34i05.6451}, abstractNote={&lt;p&gt;Almost all existing machine translation models are built on top of character-based vocabularies: characters, subwords or words. Rare characters from noisy text or character-rich languages such as Japanese and Chinese however can unnecessarily take up vocabulary slots and limit its compactness. Representing text at the level of bytes and using the 256 byte set as vocabulary is a potential solution to this issue. High computational cost has however prevented it from being widely deployed or used in practice. In this paper, we investigate byte-level subwords, specifically byte-level BPE (BBPE), which is compacter than character vocabulary and has no out-of-vocabulary tokens, but is more efficient than using pure bytes only is. We claim that contextualizing BBPE embeddings is necessary, which can be implemented by a convolutional or recurrent layer. Our experiments show that BBPE has comparable performance to BPE while its size is only 1/8 of that for BPE. In the multilingual setting, BBPE maximizes vocabulary sharing across many languages and achieves better translation quality. Moreover, we show that BBPE enables transferring models between languages with non-overlapping character sets.&lt;/p&gt;}, number={05}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, author={Wang, Changhan and Cho, Kyunghyun and Gu, Jiatao}, year={2020}, month={Apr.}, pages={9154-9160} }


@InProceedings{Welleck19non, title = {Non-Monotonic Sequential Text Generation}, author = {Welleck, Sean and Brantley, Kiant{\'e} and Iii, Hal Daum{\'e} and Cho, Kyunghyun}, booktitle = {Proceedings of the 36th International Conference on Machine Learning}, pages = {6716--6726}, year = {2019}, editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov}, volume = {97}, series = {Proceedings of Machine Learning Research}, month = {09--15 Jun}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v97/welleck19a/welleck19a.pdf}, url = { http://proceedings.mlr.press/v97/welleck19a.html }, abstract = {Standard sequential generation methods assume a pre-specified generation order, such as text generation methods which generate words from left to right. In this work, we propose a framework for training models of text generation that operate in non-monotonic orders; the model directly learns good orders, without any additional annotation. Our framework operates by generating a word at an arbitrary position, and then recursively generating words to its left and then words to its right, yielding a binary tree. Learning is framed as imitation learning, including a coaching method which moves from imitating an oracle to reinforcing the policy’s own preferences. Experimental results demonstrate that using the proposed method, it is possible to learn policies which generate text without pre-specifying a generation order, while achieving competitive performance with conventional left-to-right generation.} }

@inproceedings{Wang19dynamically,
    title = "Dynamically Composing Domain-Data Selection with Clean-Data Selection by {``}Co-Curricular Learning{''} for Neural Machine Translation",
    author = "Wang, Wei  and
      Caswell, Isaac  and
      Chelba, Ciprian",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1123",
    doi = "10.18653/v1/P19-1123",
    pages = "1282--1292",
    abstract = "Noise and domain are important aspects of data quality for neural machine translation. Existing research focus separately on domain-data selection, clean-data selection, or their static combination, leaving the dynamic interaction across them not explicitly examined. This paper introduces a {``}co-curricular learning{''} method to compose dynamic domain-data selection with dynamic clean-data selection, for transfer learning across both capabilities. We apply an EM-style optimization procedure to further refine the {``}co-curriculum{''}. Experiment results and analysis with two domains demonstrate the effectiveness of the method and the properties of data scheduled by the co-curriculum.",
}

@article{Naveen19massively,
  author    = {Naveen Arivazhagan and
               Ankur Bapna and
               Orhan Firat and
               Dmitry Lepikhin and
               Melvin Johnson and
               Maxim Krikun and
               Mia Xu Chen and
               Yuan Cao and
               George Foster and
               Colin Cherry and
               Wolfgang Macherey and
               Zhifeng Chen and
               Yonghui Wu},
  title     = {Massively Multilingual Neural Machine Translation in the Wild: Findings
               and Challenges},
  journal   = {CoRR},
  volume    = {abs/1907.05019},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.05019},
  archivePrefix = {arXiv},
  eprint    = {1907.05019},
  timestamp = {Wed, 17 Jul 2019 10:27:36 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1907-05019},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dinu19training,
    title = "Training Neural Machine Translation to Apply Terminology Constraints",
    author = "Dinu, Georgiana  and
      Mathur, Prashant  and
      Federico, Marcello  and
      Al-Onaizan, Yaser",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1294",
    doi = "10.18653/v1/P19-1294",
    pages = "3063--3068",
    abstract = "This paper proposes a novel method to inject custom terminology into neural machine translation at run time. Previous works have mainly proposed modifications to the decoding algorithm in order to constrain the output to include run-time-provided target terms. While being effective, these constrained decoding methods add, however, significant computational overhead to the inference step, and, as we show in this paper, can be brittle when tested in realistic conditions. In this paper we approach the problem by training a neural MT system to learn how to use custom terminology when provided with the input. Comparative experiments show that our method is not only more effective than a state-of-the-art implementation of constrained decoding, but is also as fast as constraint-free decoding.",
}

@inproceedings{Xuan19curriculum,
  author    = {Xuan Zhang and
               Pamela Shapiro and
               Gaurav Kumar and
               Paul McNamee and
               Marine Carpuat and
               Kevin Duh},
  title     = {Curriculum Learning for Domain Adaptation in Neural Machine Translation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {1903--1915},
  year      = {2019},
  crossref  = {DBLP:conf/naacl/2019-1},
  url       = {https://www.aclweb.org/anthology/N19-1189/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/ZhangSKMCD19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Brian19overcoming,
  author    = {Brian Thompson and
               Jeremy Gwinnup and
               Huda Khayrallah and
               Kevin Duh and
               Philipp Koehn},
  title     = {Overcoming Catastrophic Forgetting During Domain Adaptation of Neural
               Machine Translation},
  booktitle = {Proceedings of the 2019 Conference of the North American Chapter of
               the Association for Computational Linguistics: Human Language Technologies,
               {NAACL-HLT} 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long
               and Short Papers)},
  pages     = {2062--2068},
  year      = {2019},
  crossref  = {DBLP:conf/naacl/2019-1},
  url       = {https://www.aclweb.org/anthology/N19-1209/},
  timestamp = {Mon, 16 Sep 2019 17:08:53 +0200},
  biburl    = {https://dblp.org/rec/bib/conf/naacl/ThompsonGKDK19},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Conneau19cross,
 author = {Conneau, Alexis and Lample, Guillaume},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Cross-lingual Language Model Pretraining},
 url = {https://proceedings.neurips.cc/paper/2019/file/c04c19c2c2474dbf5f7ac4372c5b9af1-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@inproceedings{Schwenk19wikimatrix,
    title = "{W}iki{M}atrix: Mining 135{M} Parallel Sentences in 1620 Language Pairs from {W}ikipedia",
    author = "Schwenk, Holger  and
      Chaudhary, Vishrav  and
      Sun, Shuo  and
      Gong, Hongyu  and
      Guzm{\'a}n, Francisco",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.115",
    pages = "1351--1361",
    abstract = "We present an approach based on multilingual sentence embeddings to automatically extract parallel sentences from the content of Wikipedia articles in 96 languages, including several dialects or low-resource languages. We do not limit the extraction process to alignments with English, but we systematically consider all possible language pairs. In total, we are able to extract 135M parallel sentences for 16720 different language pairs, out of which only 34M are aligned with English. This corpus is freely available. To get an indication on the quality of the extracted bitexts, we train neural MT baseline systems on the mined data only for 1886 languages pairs, and evaluate them on the TED corpus, achieving strong BLEU scores for many language pairs. The WikiMatrix bitexts seem to be particularly interesting to train MT systems between distant languages without the need to pivot through English.",
}

@inproceedings{Saunders19domain,
    title = "Domain Adaptive Inference for Neural Machine Translation",
    author = "Saunders, Danielle  and
      Stahlberg, Felix  and
      de Gispert, Adri{\`a}  and
      Byrne, Bill",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1022",
    doi = "10.18653/v1/P19-1022",
    pages = "222--228",
    abstract = "We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label.",
}

@inproceedings{Zhang19curriculum,
    title = "Curriculum Learning for Domain Adaptation in Neural Machine Translation",
    author = "Zhang, Xuan and Shapiro, Pamela and Kumar, Gaurav  and McNamee, Paul  and Carpuat, Marine  and Duh, Kevin",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1189",
    doi = "10.18653/v1/N19-1189",
    pages = "1903--1915",
    abstract = "We introduce a curriculum learning approach to adapt generic neural machine translation models to a specific domain. Samples are grouped by their similarities to the domain of interest and each group is fed to the training algorithm with a particular schedule. This approach is simple to implement on top of any neural framework or architecture, and consistently outperforms both unadapted and adapted baselines in experiments with two distinct domains and two language pairs.",
}
	
@inproceedings{Pham19generic,
  TITLE = {{Generic and Specialized Word Embeddings for Multi-Domain Machine Translation}},
  AUTHOR = {Pham, Minh Quang and Crego, Josep-Maria and Yvon, Fran{\c c}ois and Senellart, Jean},
  URL = {https://hal.archives-ouvertes.fr/hal-02343215},
  BOOKTITLE = {{International Workshop on Spoken Language Translation}},
  ADDRESS = {Hong-Kong, China},
  SERIES = {Proceedings of the 16th International Workshop on Spoken Language Translation (IWSLT)},
  YEAR = {2019},
  MONTH = Nov,
  DOI = {10.5281/zenodo.3524978},
  KEYWORDS = {Machine Translation ; Domain Adaptation},
  PDF = {https://hal.archives-ouvertes.fr/hal-02343215/file/IWSLT2019_paper_10.pdf},
  HAL_ID = {hal-02343215},
  HAL_VERSION = {v1},
}

@inproceedings{Kumar19reinforcement,
    title = "Reinforcement Learning based Curriculum Optimization for Neural Machine Translation",
    author = "Kumar, Gaurav and Foster, George and Cherry, Colin and Krikun, Maxim",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1208",
    doi = "10.18653/v1/N19-1208",
    pages = "2054--2061",
    abstract = "We consider the problem of making efficient use of heterogeneous training data in neural machine translation (NMT). Specifically, given a training dataset with a sentence-level feature such as noise, we seek an optimal curriculum, or order for presenting examples to the system during training. Our curriculum framework allows examples to appear an arbitrary number of times, and thus generalizes data weighting, filtering, and fine-tuning schemes. Rather than relying on prior knowledge to design a curriculum, we use reinforcement learning to learn one automatically, jointly with the NMT system, in the course of a single training run. We show that this approach can beat uniform baselines on Paracrawl and WMT English-to-French datasets by +3.4 and +1.3 BLEU respectively. Additionally, we match the performance of strong filtering baselines and hand-designed, state-of-the-art curricula.",
}

@article{radford19language,
  title={Language Models are Unsupervised Multitask Learners},
  author={Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year={2019}
}

@inproceedings{ramponi20neural,
    title = "Neural Unsupervised Domain Adaptation in {NLP}{---}{A} Survey",
    author = "Ramponi, Alan  and
      Plank, Barbara",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = dec,
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://aclanthology.org/2020.coling-main.603",
    doi = "10.18653/v1/2020.coling-main.603",
    pages = "6838--6855",
    abstract = "Deep neural networks excel at learning from labeled data and achieve state-of-the-art results on a wide array of Natural Language Processing tasks. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. Motivated by the latest advances, in this survey we review neural unsupervised domain adaptation techniques which do not require labeled target domain data. This is a more challenging yet a more widely applicable setup. We outline methods, from early traditional non-neural methods to pre-trained model transfer. We also revisit the notion of domain, and we uncover a bias in the type of Natural Language Processing tasks which received most attention. Lastly, we outline future directions, particularly the broader need for out-of-distribution generalization of future NLP.",
}

@inproceedings{pfeiffer20adapterhub,
    title = "{A}dapter{H}ub: A Framework for Adapting Transformers",
    author = {Pfeiffer, Jonas  and
      R{\"u}ckl{\'e}, Andreas  and
      Poth, Clifton  and
      Kamath, Aishwarya  and
      Vuli{\'c}, Ivan  and
      Ruder, Sebastian  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-demos.7",
    doi = "10.18653/v1/2020.emnlp-demos.7",
    pages = "46--54",
    abstract = "The current modus operandi in NLP involves downloading and fine-tuning pre-trained models consisting of millions or billions of parameters. Storing and sharing such large trained models is expensive, slow, and time-consuming, which impedes progress towards more general and versatile NLP methods that learn from and for many tasks. Adapters{---}small learnt bottleneck layers inserted within each layer of a pre-trained model{---} ameliorate this issue by avoiding full fine-tuning of the entire model. However, sharing and integrating adapter layers is not straightforward. We propose AdapterHub, a framework that allows dynamic {``}stiching-in{''} of pre-trained adapters for different tasks and languages. The framework, built on top of the popular HuggingFace Transformers library, enables extremely easy and quick adaptations of state-of-the-art pre-trained models (e.g., BERT, RoBERTa, XLM-R) across tasks and languages. Downloading, sharing, and training adapters is as seamless as possible using minimal changes to the training scripts and a specialized infrastructure. Our framework enables scalable and easy access to sharing of task-specific models, particularly in low-resource scenarios. AdapterHub includes all recent adapter architectures and can be found at AdapterHub.ml",
}

@ARTICLE{Gu20domain,
       author = {{Gu}, Yu and {Tinn}, Robert and {Cheng}, Hao and {Lucas}, Michael and
         {Usuyama}, Naoto and {Liu}, Xiaodong and {Naumann}, Tristan and
         {Gao}, Jianfeng and {Poon}, Hoifung},
        title = "{Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
         year = 2020,
        month = jul,
          eid = {arXiv:2007.15779},
        pages = {arXiv:2007.15779},
archivePrefix = {arXiv},
       eprint = {2007.15779},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200715779G},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@inproceedings{Banon20Paracrawl,
    title = "{P}ara{C}rawl: Web-Scale Acquisition of Parallel Corpora",
    author = "Ba{\~n}{\'o}n, Marta  and
      Chen, Pinzhen  and
      Haddow, Barry  and
      Heafield, Kenneth  and
      Hoang, Hieu  and
      Espl{\`a}-Gomis, Miquel  and
      Forcada, Mikel L.  and
      Kamran, Amir  and
      Kirefu, Faheem  and
      Koehn, Philipp  and
      Ortiz Rojas, Sergio  and
      Pla Sempere, Leopoldo  and
      Ram{\'\i}rez-S{\'a}nchez, Gema  and
      Sarr{\'\i}as, Elsa  and
      Strelec, Marek  and
      Thompson, Brian  and
      Waites, William  and
      Wiggins, Dion  and
      Zaragoza, Jaume",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.417",
    doi = "10.18653/v1/2020.acl-main.417",
    pages = "4555--4567",
    abstract = "We report on methods to create the largest publicly available parallel corpora by crawling the web, using open source software. We empirically compare alternative methods and publish benchmark data sets for sentence alignment and sentence pair filtering. We also describe the parallel corpora released and evaluate their quality and their usefulness to create machine translation systems.",
}

@inproceedings{Oren19distributionally,
    title = "Distributionally Robust Language Modeling",
    author = "Oren, Yonatan and Sagawa, Shiori and Hashimoto, Tatsunori and Liang, Percy",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1432",
    doi = "10.18653/v1/D19-1432",
    pages = "4227--4237",
    abstract = "Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews.",
}

@inproceedings{Muller20domain,
    title = "Domain Robustness in Neural Machine Translation",
    author = {M{\"u}ller, Mathias  and
      Rios, Annette  and
      Sennrich, Rico},
    booktitle = "Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track)",
    month = oct,
    year = "2020",
    address = "Virtual",
    publisher = "Association for Machine Translation in the Americas",
    url = "https://aclanthology.org/2020.amta-research.14",
    pages = "151--164",
}

@inproceedings{artetxe20cross,
    title = "On the Cross-lingual Transferability of Monolingual Representations",
    author = "Artetxe, Mikel  and
      Ruder, Sebastian  and
      Yogatama, Dani",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.421",
    pages = "4623--4637",
    abstract = "State-of-the-art unsupervised multilingual models (e.g., multilingual BERT) have been shown to generalize in a zero-shot cross-lingual setting. This generalization ability has been attributed to the use of a shared subword vocabulary and joint training across multiple languages giving rise to deep multilingual abstractions. We evaluate this hypothesis by designing an alternative approach that transfers a monolingual model to new languages at the lexical level. More concretely, we first train a transformer-based masked language model on one language, and transfer it to a new language by learning a new embedding matrix with the same masked language modeling objective, freezing parameters of all other layers. This approach does not rely on a shared vocabulary or joint training. However, we show that it is competitive with multilingual BERT on standard cross-lingual classification benchmarks and on a new Cross-lingual Question Answering Dataset (XQuAD). Our results contradict common beliefs of the basis of the generalization ability of multilingual models and suggest that deep monolingual models learn some abstractions that generalize across languages. We also release XQuAD as a more comprehensive cross-lingual benchmark, which comprises 240 paragraphs and 1190 question-answer pairs from SQuAD v1.1 translated into ten languages by professional translators.",
}

@inproceedings{aji20neural,
    title = "In Neural Machine Translation, What Does Transfer Learning Transfer?",
    author = "Aji, Alham Fikri  and
      Bogoychev, Nikolay  and
      Heafield, Kenneth  and
      Sennrich, Rico",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.688",
    pages = "7701--7710",
    abstract = "Transfer learning improves quality for low-resource machine translation, but it is unclear what exactly it transfers. We perform several ablation studies that limit information transfer, then measure the quality impact across three language pairs to gain a black-box understanding of transfer learning. Word embeddings play an important role in transfer learning, particularly if they are properly aligned. Although transfer learning can be performed without embeddings, results are sub-optimal. In contrast, transferring only the embeddings but nothing else yields catastrophic results. We then investigate diagonal alignments with auto-encoders over real languages and randomly generated sequences, finding even randomly generated sequences as parents yield noticeable but smaller gains. Finally, transfer learning can eliminate the need for a warm-up phase when training transformer models in high resource language pairs.",
}


@InProceedings{amos20differential,
  title = 	 {The Differentiable Cross-Entropy Method},
  author =       {Amos, Brandon and Yarats, Denis},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {291--302},
  year = 	 {2020},
  editor = 	 {III, Hal Daumé and Singh, Aarti},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/amos20a/amos20a.pdf},
  url = 	 {https://proceedings.mlr.press/v119/amos20a.html},
  abstract = 	 {We study the Cross-Entropy Method (CEM) for the non-convex optimization of a continuous and parameterized objective function and introduce a differentiable variant that enables us to differentiate the output of CEM with respect to the objective function’s parameters. In the machine learning setting this brings CEM inside of the end-to-end learning pipeline where this has otherwise been impossible. We show applications in a synthetic energy-based structured prediction task and in non-convex continuous control. In the control setting we show how to embed optimal action sequences into a lower-dimensional space. This enables us to use policy optimization to fine-tune modeling components by differentiating through the CEM-based controller.}
}


@inproceedings{xu20boosting,
    title = "Boosting Neural Machine Translation with Similar Translations",
    author = "Xu, Jitao  and
      Crego, Josep  and
      Senellart, Jean",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.144",
    doi = "10.18653/v1/2020.acl-main.144",
    pages = "1580--1590",
    abstract = "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with {``}copy{''} information while translations based on embedding similarities tend to extend the translation {``}context{''}. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",
}

@inproceedings{santos20word,
    title = "Word Embedding Evaluation in Downstream Tasks and Semantic Analogies",
    author = "Santos, Joaquim  and
      Consoli, Bernardo  and
      Vieira, Renata",
    booktitle = "Proceedings of the 12th Language Resources and Evaluation Conference",
    month = may,
    year = "2020",
    address = "Marseille, France",
    publisher = "European Language Resources Association",
    url = "https://aclanthology.org/2020.lrec-1.594",
    pages = "4828--4834",
    abstract = "Language Models have long been a prolific area of study in the field of Natural Language Processing (NLP). One of the newer kinds of language models, and some of the most used, are Word Embeddings (WE). WE are vector space representations of a vocabulary learned by a non-supervised neural network based on the context in which words appear. WE have been widely used in downstream tasks in many areas of study in NLP. These areas usually use these vector models as a feature in the processing of textual data. This paper presents the evaluation of newly released WE models for the Portuguese langauage, trained with a corpus composed of 4.9 billion tokens. The first evaluation presented an intrinsic task in which WEs had to correctly build semantic and syntactic relations. The second evaluation presented an extrinsic task in which the WE models were used in two downstream tasks: Named Entity Recognition and Semantic Similarity between Sentences. Our results show that a diverse and comprehensive corpus can often outperform a larger, less textually diverse corpus, and that batch training may cause quality loss in WE models.",
    language = "English",
    ISBN = "979-10-95546-34-4",
}

@inproceedings{Li20shallow,
    title = "Shallow-to-Deep Training for Neural Machine Translation",
    author = "Li, Bei  and
      Wang, Ziyang  and
      Liu, Hui  and
      Jiang, Yufan  and
      Du, Quan  and
      Xiao, Tong  and
      Wang, Huizhen  and
      Zhu, Jingbo",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.72",
    doi = "10.18653/v1/2020.emnlp-main.72",
    pages = "995--1005",
    abstract = "Deep encoders have been proven to be effective in improving neural machine translation (NMT) systems, but training an extremely deep encoder is time consuming. Moreover, why deep models help NMT is an open question. In this paper, we investigate the behavior of a well-tuned deep Transformer system. We find that stacking layers is helpful in improving the representation ability of NMT models and adjacent layers perform similarly. This inspires us to develop a shallow-to-deep training method that learns deep models by stacking shallow models. In this way, we successfully train a Transformer system with a 54-layer encoder. Experimental results on WMT{'}16 English-German and WMT{'}14 English-French translation tasks show that it is 1:4 faster than training from scratch, and achieves a BLEU score of 30:33 and 43:29 on two tasks. The code is publicly available at \url{https://github.com/libeineu/SDT-Training}.",
}

@inproceedings{wang20negative,
    title = "On Negative Interference in Multilingual Models: Findings and A Meta-Learning Treatment",
    author = "Wang, Zirui  and
      Lipton, Zachary C.  and
      Tsvetkov, Yulia",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.359",
    doi = "10.18653/v1/2020.emnlp-main.359",
    pages = "4438--4450",
    abstract = "Modern multilingual models are trained on concatenated text from multiple languages in hopes of conferring benefits to each (positive transfer), with the most pronounced benefits accruing to low-resource languages. However, recent work has shown that this approach can degrade performance on high-resource languages, a phenomenon known as negative interference. In this paper, we present the first systematic study of negative interference. We show that, contrary to previous belief, negative interference also impacts low-resource languages. While parameters are maximally shared to learn language-universal structures, we demonstrate that language-specific parameters do exist in multilingual models and they are a potential cause of negative interference. Motivated by these observations, we also present a meta-learning algorithm that obtains better cross-lingual transferability and alleviates negative interference, by adding language-specific layers as meta-parameters and training them in a manner that explicitly improves shared layers{'} generalization on all languages. Overall, our results show that negative interference is more common than previously known, suggesting new directions for improving multilingual representations.",
}

@inproceedings{Aharoni20unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
    abstract = "The notion of {``}in-domain data{''} in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision {--} suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",
}

@misc{feng20languageagnostic,
    title={Language-agnostic {BERT} Sentence Embedding},
    author={Fangxiaoyu Feng and Yinfei Yang and Daniel Cer and Naveen Arivazhagan and Wei Wang},
    year={2020},
    eprint={2007.01852},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@article{Johnson19billion,
  title={Billion-scale similarity search with {GPUs}},
  author={Johnson, Jeff and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  journal={IEEE Transactions on Big Data},
  year={2019},
  url="https://arxiv.org/pdf/1702.08734.pdf",
  publisher={IEEE}
}

@inproceedings{xu19lexical,
        Title = {Lexical Micro-adaptation for Neural Machine Translation},
        Address = {Honk Kong, China},
        Author = {Xu, Jitao and Crego, Josep and Senellart, Jean},
        Booktitle = {International Workshop on Spoken Language Translation},
        url = "https://zenodo.org/record/3524977/files/IWSLT2019_paper_9.pdf?download=1",
	month = {nov},
        Year = {2019},
}

@misc{chang20pretraining,
    title={Pre-training Tasks for Embedding-based Large-scale Retrieval},
    author={Wei-Cheng Chang and Felix X. Yu and Yin-Wen Chang and Yiming Yang and Sanjiv Kumar},
    year={2020},
    eprint={2002.03932},
    archivePrefix={arXiv},
    primaryClass={cs.LG}
}

@inproceedings{xu20boosting,
    title = "Boosting Neural Machine Translation with Similar Translations",
    author = "Xu, Jitao  and
      Crego, Josep  and
      Senellart, Jean",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.144",
    doi = "10.18653/v1/2020.acl-main.144",
    pages = "1580--1590",
    abstract = "This paper explores data augmentation methods for training Neural Machine Translation to make use of similar translations, in a comparable way a human translator employs fuzzy matches. In particular, we show how we can simply present the neural model with information of both source and target sides of the fuzzy matches, we also extend the similarity to include semantically related translations retrieved using sentence distributed representations. We show that translations based on fuzzy matching provide the model with {``}copy{''} information while translations based on embedding similarities tend to extend the translation {``}context{''}. Results indicate that the effect from both similar sentences are adding up to further boost accuracy, combine naturally with model fine-tuning and are providing dynamic adaptation for unseen translation pairs. Tests on multiple data sets and domains show consistent accuracy improvements. To foster research around these techniques, we also release an Open-Source toolkit with efficient and flexible fuzzy-match implementation.",
}

@inproceedings{Brown20language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1877--1901},
 publisher = {Curran Associates, Inc.},
 title = {Language Models are Few-Shot Learners},
 url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
 volume = {33},
 year = {2020}
}

@inproceedings{currey20distilling,
    title = "Distilling Multiple Domains for Neural Machine Translation",
    author = "Currey, Anna  and
      Mathur, Prashant  and
      Dinu, Georgiana",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.364",
    doi = "10.18653/v1/2020.emnlp-main.364",
    pages = "4500--4511",
    abstract = "Neural machine translation achieves impressive results in high-resource conditions, but performance often suffers when the input domain is low-resource. The standard practice of adapting a separate model for each domain of interest does not scale well in practice from both a quality perspective (brittleness under domain shift) as well as a cost perspective (added maintenance and inference complexity). In this paper, we propose a framework for training a single multi-domain neural machine translation model that is able to translate several domains without increasing inference time or memory usage. We show that this model can improve translation on both high- and low-resource domains over strong multi-domain baselines. In addition, our proposed model is effective when domain labels are unknown during training, as well as robust under noisy data conditions.",
}

@article{dabre20survey,
author = {Dabre, Raj and Chu, Chenhui and Kunchukuttan, Anoop},
title = {A Survey of Multilingual Neural Machine Translation},
year = {2020},
issue_date = {October 2020},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {53},
number = {5},
issn = {0360-0300},
url = {https://doi.org/10.1145/3406095},
doi = {10.1145/3406095},
abstract = {We present a survey on multilingual neural machine translation (MNMT), which has gained
a lot of traction in recent years. MNMT has been useful in improving translation quality
as a result of translation knowledge transfer (transfer learning). MNMT is more promising
and interesting than its statistical machine translation counterpart, because end-to-end
modeling and distributed representations open new avenues for research on machine
translation. Many approaches have been proposed to exploit multilingual parallel corpora
for improving translation quality. However, the lack of a comprehensive survey makes
it difficult to determine which approaches are promising and, hence, deserve further
exploration. In this article, we present an in-depth survey of existing literature
on MNMT. We first categorize various approaches based on their central use-case and
then further categorize them based on resource scenarios, underlying modeling principles,
core-issues, and challenges. Wherever possible, we address the strengths and weaknesses
of several techniques by comparing them with each other. We also discuss the future
directions for MNMT. This article is aimed towards both beginners and experts in NMT.
We hope this article will serve as a starting point as well as a source of new ideas
for researchers and engineers interested in MNMT.},
journal = {ACM Comput. Surv.},
month = sep,
articleno = {99},
numpages = {38},
keywords = {zero-shot, multi-source, Neural machine translation, multilingualism, low-resource, survey}
}

@inproceedings{conneau20unsupervised,
    title = "Unsupervised Cross-lingual Representation Learning at Scale",
    author = "Conneau, Alexis  and
      Khandelwal, Kartikay  and
      Goyal, Naman  and
      Chaudhary, Vishrav  and
      Wenzek, Guillaume  and
      Guzm{\'a}n, Francisco  and
      Grave, Edouard  and
      Ott, Myle  and
      Zettlemoyer, Luke  and
      Stoyanov, Veselin",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.747",
    doi = "10.18653/v1/2020.acl-main.747",
    pages = "8440--8451",
    abstract = "This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6{\%} average accuracy on XNLI, +13{\%} average F1 score on MLQA, and +2.4{\%} F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7{\%} in XNLI accuracy for Swahili and 11.4{\%} for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.",
}

@inproceedings{xian20deep,
 author = {Li, Xian and Cooper Stickland, Asa and Tang, Yuqing and Kong, Xiang},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {1736--1746},
 publisher = {Curran Associates, Inc.},
 title = {Deep Transformers with Latent Depth},
 url = {https://proceedings.neurips.cc/paper/2020/file/1325cdae3b6f0f91a1b629307bf2d498-Paper.pdf},
 volume = {33},
 year = {2020}
}


@inproceedings{Sharaf20metalearning,
    title = "Meta-Learning for Few-Shot {NMT} Adaptation",
    author = "Sharaf, Amr and Hassan, Hany  and Daum{\'e} III, Hal",
    booktitle = "Proceedings of the Fourth Workshop on Neural Generation and Translation",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.ngt-1.5",
    doi = "10.18653/v1/2020.ngt-1.5",
    pages = "43--53",
    abstract = "We present META-MT, a meta-learning approach to adapt Neural Machine Translation (NMT) systems in a few-shot setting. META-MT provides a new approach to make NMT models easily adaptable to many target do- mains with the minimal amount of in-domain data. We frame the adaptation of NMT systems as a meta-learning problem, where we learn to adapt to new unseen domains based on simulated offline meta-training domain adaptation tasks. We evaluate the proposed meta-learning strategy on ten domains with general large scale NMT systems. We show that META-MT significantly outperforms classical domain adaptation when very few in- domain examples are available. Our experiments shows that META-MT can outperform classical fine-tuning by up to 2.5 BLEU points after seeing only 4, 000 translated words (300 parallel sentences).",
}
@inproceedings{Philip20monolingual,
    title = "Monolingual Adapters for Zero-Shot Neural Machine Translation",
    author = "Philip, Jerin  and
      Berard, Alexandre  and
      Gall{\'e}, Matthias  and
      Besacier, Laurent",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.361",
    doi = "10.18653/v1/2020.emnlp-main.361",
    pages = "4465--4470",
    abstract = "We propose a novel adapter layer formalism for adapting multilingual models. They are more parameter-efficient than existing adapter layers while obtaining as good or better performance. The layers are specific to one language (as opposed to bilingual adapters) allowing to compose them and generalize to unseen language-pairs. In this zero-shot setting, they obtain a median improvement of +2.77 BLEU points over a strong 20-language multilingual Transformer baseline trained on TED talks.",
}

@inproceedings{Pham20Study,
    title = "A Study of Residual Adapters for Multi-Domain Neural Machine Translation",
    author = "Pham, Minh Quang  and
      Crego, Josep Maria  and
      Yvon, Fran{\c{c}}ois  and
      Senellart, Jean",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.72",
    pages = "617--628",
    abstract = "Domain adaptation is an old and vexing problem for machine translation systems. The most common approach and successful to supervised adaptation is to fine-tune a baseline system with in-domain parallel data. Standard fine-tuning however modifies all the network parameters, which makes this approach computationally costly and prone to overfitting. A recent, lightweight approach, instead augments a baseline model with supplementary (small) adapter layers, keeping the rest of the mode unchanged. This has the additional merit to leave the baseline model intact, and adaptable to multiple domains. In this paper, we conduct a thorough analysis of the adapter model in the context of a multidomain machine translation task. We contrast multiple implementations of this idea on two language pairs. Our main conclusions are that residual adapters provide a fast and cheap method for supervised multi-domain adaptation; our two variants prove as effective as the original adapter model, and open perspective to also make adapted models more robust to label domain errors.",
}

@inproceedings{Aharoni20Unsupervised,
    title = "Unsupervised Domain Clusters in Pretrained Language Models",
    author = "Aharoni, Roee  and
      Goldberg, Yoav",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.692",
    doi = "10.18653/v1/2020.acl-main.692",
    pages = "7747--7763",
    abstract = "The notion of {``}in-domain data{''} in NLP is often over-simplistic and vague, as textual data varies in many nuanced linguistic aspects such as topic, style or level of formality. In addition, domain labels are many times unavailable, making it challenging to build domain-specific systems. We show that massive pre-trained language models implicitly learn sentence representations that cluster by domains without supervision {--} suggesting a simple data-driven definition of domains in textual data. We harness this property and propose domain data selection methods based on such models, which require only a small set of in-domain monolingual data. We evaluate our data selection methods for neural machine translation across five diverse domains, where they outperform an established approach as measured by both BLEU and precision and recall with respect to an oracle selection.",
}

@inproceedings{Jiang20Multi,
    title = "Multi-Domain Neural Machine Translation with Word-Level Adaptive Layer-wise Domain Mixing",
    author = "Jiang, Haoming  and
      Liang, Chen  and
      Wang, Chong  and
      Zhao, Tuo",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.acl-main.165",
    doi = "10.18653/v1/2020.acl-main.165",
    pages = "1823--1834",
    abstract = "Many multi-domain neural machine translation (NMT) models achieve knowledge transfer by enforcing one encoder to learn shared embedding across domains. However, this design lacks adaptation to individual domains. To overcome this limitation, we propose a novel multi-domain NMT model using individual modules for each domain, on which we apply word-level, adaptive and layer-wise domain mixing. We first observe that words in a sentence are often related to multiple domains. Hence, we assume each word has a domain proportion, which indicates its domain preference. Then word representations are obtained by mixing their embedding in individual domains based on their domain proportions. We show this can be achieved by carefully designing multi-head dot-product attention modules for different domains, and eventually taking weighted averages of their parameters by word-level layer-wise domain proportions. Through this, we can achieve effective domain knowledge sharing and capture fine-grained domain-specific knowledge as well. Our experiments show that our proposed model outperforms existing ones in several NMT tasks.",
}

@inproceedings{Pham20Priming,
    title = "Priming Neural Machine Translation",
    author = "Pham, Minh Quang  and
      Xu, Jitao  and
      Crego, Josep  and
      Yvon, Fran{\c{c}}ois  and
      Senellart, Jean",
    booktitle = "Proceedings of the Fifth Conference on Machine Translation",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.wmt-1.63",
    pages = "516--527",
    abstract = "Priming is a well known and studied psychology phenomenon based on the prior presentation of one stimulus (cue) to influence the processing of a response. In this paper, we propose a framework to mimic the process of priming in the context of neural machine translation (NMT). We evaluate the effect of using similar translations as priming cues on the NMT network. We propose a method to inject priming cues into the NMT network and compare our framework to other mechanisms that perform micro-adaptation during inference. Overall, experiments conducted in a multi-domain setting confirm that adding priming cues in the NMT decoder can go a long way towards improving the translation accuracy. Besides, we show the suitability of our framework to gather valuable information for an NMT network from monolingual resources.",
}

@InProceedings{Wang20optimizing,
  title = 	 {Optimizing Data Usage via Differentiable Rewards},
  author =       {Wang, Xinyi and Pham, Hieu and Michel, Paul and Anastasopoulos, Antonios and Carbonell, Jaime and Neubig, Graham},
  booktitle = 	 {Proceedings of the 37th International Conference on Machine Learning},
  pages = 	 {9983--9995},
  year = 	 {2020},
  editor = 	 {Hal Daumé III and Aarti Singh},
  volume = 	 {119},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {13--18 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v119/wang20p/wang20p.pdf},
  url = 	 {
http://proceedings.mlr.press/v119/wang20p.html
},
  abstract = 	 {To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that “adapts” to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.}
}

@inproceedings{Wang20balancing,
    title = "Balancing Training for Multilingual Neural Machine Translation",
    author = "Wang, Xinyi and Tsvetkov, Yulia and Neubig, Graham",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.754",
    doi = "10.18653/v1/2020.acl-main.754",
    pages = "8526--8537",
    abstract = "When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",
}

@inproceedings{Wang20learning-multi,
    title = "Learning a Multi-Domain Curriculum for Neural Machine Translation",
    author = "Wang, Wei and Tian, Ye and Ngiam, Jiquan and Yang, Yinfei and Caswell, Isaac and Parekh, Zarana",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.689",
    doi = "10.18653/v1/2020.acl-main.689",
    pages = "7711--7723",
    abstract = "Most data selection research in machine translation focuses on improving a single domain. We perform data selection for multiple domains at once. This is achieved by carefully introducing instance-level domain-relevance features and automatically constructing a training curriculum to gradually concentrate on multi-domain relevant and noise-reduced data batches. Both the choice of features and the use of curriculum are crucial for balancing and improving all domains, including out-of-domain. In large-scale experiments, the multi-domain curriculum simultaneously reaches or outperforms the individual performance and brings solid gains over no-curriculum training.",
}

@inproceedings{Tan19multilingual,
    title = "Multilingual Neural Machine Translation with Language Clustering",
    author = "Tan, Xu  and
      Chen, Jiale  and
      He, Di  and
      Xia, Yingce  and
      Qin, Tao  and
      Liu, Tie-Yan",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1089",
    doi = "10.18653/v1/D19-1089",
    pages = "963--973",
    abstract = "Multilingual neural machine translation (NMT), which translates multiple languages using a single model, is of great practical importance due to its advantages in simplifying the training process, reducing online maintenance costs, and enhancing low-resource and zero-shot translation. Given there are thousands of languages in the world and some of them are very different, it is extremely burdensome to handle them all in a single model or use a separate model for each language pair. Therefore, given a fixed resource budget, e.g., the number of models, how to determine which languages should be supported by one model is critical to multilingual NMT, which, unfortunately, has been ignored by previous work. In this work, we develop a framework that clusters languages into different groups and trains one multilingual model for each cluster. We study two methods for language clustering: (1) using prior knowledge, where we cluster languages according to language family, and (2) using language embedding, in which we represent each language by an embedding vector and cluster them in the embedding space. In particular, we obtain the embedding vectors of all the languages by training a universal neural machine translation model. Our experiments on 23 languages show that the first clustering method is simple and easy to understand but leading to suboptimal translation accuracy, while the second method sufficiently captures the relationship among languages well and improves the translation accuracy for almost all the languages over baseline methods.",
}

@inproceedings{Zhou20uncertainty,
    title = "Uncertainty-Aware Curriculum Learning for Neural Machine Translation",
    author = "Zhou, Yikai and Yang, Baosong and Wong, Derek F. and Wan, Yu and Chao, Lidia S.",
    booktitle = "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.acl-main.620",
    doi = "10.18653/v1/2020.acl-main.620",
    pages = "6934--6944",
    abstract = "Neural machine translation (NMT) has proven to be facilitated by curriculum learning which presents examples in an easy-to-hard order at different training stages. The keys lie in the assessment of data difficulty and model competence. We propose uncertainty-aware curriculum learning, which is motivated by the intuition that: 1) the higher the uncertainty in a translation pair, the more complex and rarer the information it contains; and 2) the end of the decline in model uncertainty indicates the completeness of current training stage. Specifically, we serve cross-entropy of an example as its data difficulty and exploit the variance of distributions over the weights of the network to present the model uncertainty. Extensive experiments on various translation tasks reveal that our approach outperforms the strong baseline and related methods on both translation quality and convergence speed. Quantitative analyses reveal that the proposed strategy offers NMT the ability to automatically govern its learning schedule.",
}

@InProceedings{Zhang20Pegasus,
  title =        {{PEGASUS}: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author =       {Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter},
  booktitle =    {Proceedings of the 37th International Conference on Machine Learning},
  pages =        {11328--11339},
  year =         {2020},
  editor =       {Hal Daumé III and Aarti Singh},
  volume =       {119},
  series =       {Proceedings of Machine Learning Research},
  month =        {13--18 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v119/zhang20ae/zhang20ae.pdf},
  url =          {
http://proceedings.mlr.press/v119/zhang20ae.html
},
  abstract =     {Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets.}
}

@article{Saunders21Asurvey,
  author    = {Danielle Saunders},
  title     = {Domain Adaptation and Multi-Domain Adaptation for Neural Machine Translation: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2104.06951},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.06951},
  archivePrefix = {arXiv},
  eprint    = {2104.06951},
}

@PhdThesis{Saunders21Domain,
  author = 	 {Danielle Saunders},
  title = 	 {Domain Adaptation for Neural Machine Translation},
  school = 	 {Department of Engineering, University of Cambridge},
  year = 	 2021}

@article{Stergiadis21Multi,
  title={Multi-Domain Adaptation in Neural Machine Translation Through Multidimensional Tagging},
  author={Emmanouil Stergiadis and Satendra Kumar and Fedor Kovalev and Pavel Levin},
  journal={ArXiv},
  year={2021},
  volume={abs/2102.10160}
}

@inproceedings{pfeiffer21adapterfusion,
    title = "{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning",
    author = {Pfeiffer, Jonas  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.39",
    doi = "10.18653/v1/2021.eacl-main.39",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml.",
}

@inproceedings{biao21share,
  author    = {Biao Zhang and
               Ankur Bapna and
               Rico Sennrich and
               Orhan Firat},
  title     = {Share or Not? Learning to Schedule Language-Specific Capacity for
               Multilingual Translation},
  booktitle = {9th International Conference on Learning Representations, {ICLR} 2021,
               Virtual Event, Austria, May 3-7, 2021},
  publisher = {OpenReview.net},
  year      = {2021},
  url       = {https://openreview.net/forum?id=Wj4ODo0uyCF},
  timestamp = {Sun, 25 Jul 2021 11:45:53 +0200},
  biburl    = {https://dblp.org/rec/conf/iclr/ZhangBSF21.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Kumar21learning,
  title={Learning Policies for Multilingual Training of Neural Machine Translation Systems},
  author={Kumar, Gaurav and Koehn, Philipp and Khudanpur, Sanjeev},
  journal   = {CoRR},
  volume    = {abs/2103.06964},
  year      = {2021},
  url       = {https://arxiv.org/abs/2103.06964},
  archivePrefix = {arXiv},
  eprint    = {2103.06964},
  }

@article{Gong21pay,
  title={Pay Better Attention to Attention: Head Selection in Multilingual and Multi-Domain Sequence Modeling},
  author={Hongyu Gong and Yun Tang and J. Pino and Xian Li},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.10840},
  url={https://arxiv.org/abs/2106.10840},
}

@article{Gong21adaptive,
  title={Adaptive Sparse Transformer for Multilingual Translation},
  author={Hongyu Gong and Xian Li and Dmitriy Genzel},
  journal={ArXiv},
  year={2021},
  volume={abs/2104.07358}
}

@article{william21switch,
  author    = {William Fedus and
               Barret Zoph and
               Noam Shazeer},
  title     = {Switch Transformers: Scaling to Trillion Parameter Models with Simple
               and Efficient Sparsity},
  journal   = {CoRR},
  volume    = {abs/2101.03961},
  year      = {2021},
  url       = {https://arxiv.org/abs/2101.03961},
  eprinttype = {arXiv},
  eprint    = {2101.03961},
  timestamp = {Thu, 21 Jan 2021 14:42:30 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2101-03961.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lepikhin21gshard,
title={{\{}GS{\}}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding},
author={Dmitry Lepikhin and HyoukJoong Lee and Yuanzhong Xu and Dehao Chen and Orhan Firat and Yanping Huang and Maxim Krikun and Noam Shazeer and Zhifeng Chen},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=qrwe7XHTmYb}
}

@inproceedings{kong21multilingual,
    title = "Multilingual Neural Machine Translation with Deep Encoder and Multiple Shallow Decoders",
    author = "Kong, Xiang  and
      Renduchintala, Adithya  and
      Cross, James  and
      Tang, Yuqing  and
      Gu, Jiatao  and
      Li, Xian",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.138",
    doi = "10.18653/v1/2021.eacl-main.138",
    pages = "1613--1624",
    abstract = "Recent work in multilingual translation advances translation quality surpassing bilingual baselines using deep transformer models with increased capacity. However, the extra latency and memory costs introduced by this approach may make it unacceptable for efficiency-constrained applications. It has recently been shown for bilingual translation that using a deep encoder and shallow decoder (DESD) can reduce inference latency while maintaining translation quality, so we study similar speed-accuracy trade-offs for multilingual translation. We find that for many-to-one translation we can indeed increase decoder speed without sacrificing quality using this approach, but for one-to-many translation, shallow decoders cause a clear quality drop. To ameliorate this drop, we propose a deep encoder with multiple shallow decoders (DEMSD) where each shallow decoder is responsible for a disjoint subset of target languages. Specifically, the DEMSD model with 2-layer decoders is able to obtain a 1.8x speedup on average compared to a standard transformer model with no drop in translation quality.",
}


@inproceedings{xie21importance,
    title = "Importance-based Neuron Allocation for Multilingual Neural Machine Translation",
    author = "Xie, Wanying  and
      Feng, Yang  and
      Gu, Shuhao  and
      Yu, Dong",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.445",
    doi = "10.18653/v1/2021.acl-long.445",
    pages = "5725--5737",
    abstract = "Multilingual neural machine translation with a single model has drawn much attention due to its capability to deal with multiple languages. However, the current multilingual translation paradigm often makes the model tend to preserve the general knowledge, but ignore the language-specific knowledge. Some previous works try to solve this problem by adding various kinds of language-specific modules to the model, but they suffer from the parameter explosion problem and require specialized manual design. To solve these problems, we propose to divide the model neurons into general and language-specific parts based on their importance across languages. The general part is responsible for preserving the general knowledge and participating in the translation of all the languages, while the language-specific part is responsible for preserving the language-specific knowledge and participating in the translation of some specific languages. Experimental results on several language pairs, covering IWSLT and Europarl corpus datasets, demonstrate the effectiveness and universality of the proposed method.",
}

@inproceedings{he21fast,
    title = "Fast and Accurate Neural Machine Translation with Translation Memory",
    author = "He, Qiuxiang  and
      Huang, Guoping  and
      Cui, Qu  and
      Li, Li  and
      Liu, Lemao",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.246",
    doi = "10.18653/v1/2021.acl-long.246",
    pages = "3170--3180",
    abstract = "It is generally believed that a translation memory (TM) should be beneficial for machine translation tasks. Unfortunately, existing wisdom demonstrates the superiority of TM-based neural machine translation (NMT) only on the TM-specialized translation tasks rather than general tasks, with a non-negligible computational overhead. In this paper, we propose a fast and accurate approach to TM-based NMT within the Transformer framework: the model architecture is simple and employs a single bilingual sentence as its TM, leading to efficient training and inference; and its parameters are effectively optimized through a novel training criterion. Extensive experiments on six TM-specialized tasks show that the proposed approach substantially surpasses several strong baselines that use multiple TMs, in terms of BLEU and running time. In particular, the proposed approach also advances the strong baselines on two general tasks (WMT news Zh-{\textgreater}En and En-{\textgreater}De).",
}


@article{Christopher21efficiently,
  author    = {Christopher Fifty and
               Ehsan Amid and
               Zhe Zhao and
               Tianhe Yu and
               Rohan Anil and
               Chelsea Finn},
  title     = {Efficiently Identifying Task Groupings for Multi-Task Learning},
  journal   = {CoRR},
  volume    = {abs/2109.04617},
  year      = {2021},
  url       = {https://arxiv.org/abs/2109.04617},
  eprinttype = {arXiv},
  eprint    = {2109.04617},
  timestamp = {Tue, 21 Sep 2021 17:46:04 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2109-04617.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}





